{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"kubernetes-training","text":""},{"location":"#versions","title":"Versions","text":"<ol> <li>Kubernetes: v1.25.3</li> <li>kustomize: v4.2.0 (released on 2021-07-02)</li> <li>Helm: v3.11.2</li> <li>Traefik: v2.9.0</li> <li>ArgoCD: v2.8.4</li> <li>Prometheus-Operator: v0.43.1</li> <li>Prometheus: latest</li> <li>Grafana: latest</li> <li>Strimzi: 0.24.0</li> <li>Kind: v0.17.0</li> <li>Ingress Nginx Controller: controller-v1.7.0</li> <li>Conftest: v0.25.0</li> <li>Istio: 1.19.0</li> <li>PostgresOperator: v1.7.1 (released on 2021-11-04)</li> <li>Cert Manager: v1.7.1 (released on 2022-02-05)</li> </ol>"},{"location":"#contents","title":"Contents","text":"<p>Contents are organized based on Cloud Native Trail Map:</p> <ul> <li>https://github.com/cncf/trailmap</li> <li>https://www.cncf.io/blog/2018/03/08/introducing-the-cloud-native-landscape-2-0-interactive-edition/</li> </ul> <p></p>"},{"location":"#1-containerization","title":"1. CONTAINERIZATION","text":"<ol> <li>Containers 101: attach vs. exec - what's the difference?</li> </ol>"},{"location":"#2-cicd","title":"2. CI/CD","text":"<ol> <li>ArgoCD</li> <li>Conftest</li> <li>Kyverno: https://kyverno.io/</li> <li>Polaris: https://www.fairwinds.com/polaris</li> </ol>"},{"location":"#3-orchestration-application-definition","title":"3. ORCHESTRATION &amp; APPLICATION DEFINITION","text":"<ol> <li>Kubernetes<ol> <li> <p>Useful Commands</p> <ul> <li>DNS     <pre><code>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml\nkubectl exec -i -t dnsutils -- nslookup kubernetes.default\n</code></pre></li> <li>Debug with ephemeral containers (alpha in 1.22, beta in 1.23)     <pre><code>kubectl run ephemeral-demo --image=k8s.gcr.io/pause:3.1 --restart=Never\nkubectl debug -it ephemeral-demo --image=busybox --target=ephemeral-demo\n</code></pre></li> <li>Create pod with busyboxy-curl     <pre><code>kubectl run -it --rm=true busybox --image=yauritux/busybox-curl --restart=Never\n</code></pre><ol> <li>Kubernetes Cluster</li> </ol> </li> <li>local cluster: kind, minikube, Docker Desktop</li> <li>kubeadm-local: Set up Kubernetes Cluster with kubeadm (local)</li> <li>Kubernetes The Hard Way: Set up Kubernetes Cluster on GCP (kubernetes-the-hard-way)<ol> <li>Kubernetes Components</li> </ol> </li> <li>kubernetes-scheduler</li> <li>etcd</li> <li>kube-apiserver</li> <li>kube-controller-manager</li> <li>kube-proxy</li> <li>kubelet<ol> <li>Kubernetes Operator</li> </ol> </li> <li>client-go</li> <li>apimachinery</li> <li>controller-runtime<ol> <li>More Practices of Applications on Kubernetes</li> <li>Kubernetes Features</li> </ol> </li> <li>Autoscaler HPA with custom metrics</li> <li>amazon-eks-workshop<ol> <li>Kubernetes Extensions</li> </ol> </li> <li>kubernetes-operator</li> <li>kubernetes-scheduler</li> <li>plugins (todo)<ol> <li>Namespaces</li> </ol> </li> <li>hierarchical namespaces (HNC)<ol> <li>Deloyment Managemet</li> </ol> </li> <li>Knative</li> <li>Skaffold: https://skaffold.dev/ (ToDo)<ol> <li>Middleware (Operator)</li> </ol> </li> <li>strimzi</li> <li>eck<ol> <li>Security</li> </ol> </li> <li>Cert Manager<ol> <li>Machine Learning</li> </ol> </li> <li>kubeflow</li> <li>Helm<ol> <li>Helm vs Kustomize</li> </ol> </li> </ul> </li> </ol> </li> </ol>"},{"location":"#4-observability-analytics","title":"4. OBSERVABILITY &amp; ANALYTICS","text":"<ol> <li>Prometheus<ol> <li>Prometheus Operator</li> </ol> </li> <li>Jaeger: https://www.jaegertracing.io/<ol> <li>Opentelemetry &amp; Jaeger</li> </ol> </li> <li>Opentelemetry (ToDo)</li> <li>fluentd (ToDo)</li> <li>[Thanos (todo)] https://thanos.io/</li> <li>Grafana</li> <li>Grafana Operator</li> <li>Grafana Loki</li> <li>Grafana Tempo</li> </ol>"},{"location":"#5-service-proxy-discovery-mesh","title":"5. SERVICE PROXY, DISCOVERY &amp; MESH","text":"<ol> <li>Istio</li> <li>Envoy</li> <li>CoreDNS (ToDo)</li> <li>Linkerd (ToDo)</li> </ol>"},{"location":"#6-networking-policy-security","title":"6. NETWORKING, POLICY &amp; SECURITY","text":"<ol> <li>Open Policy Agent<ol> <li>gatekeeper</li> <li>conftest</li> </ol> </li> <li>CNI (ToDo)</li> <li>falco (ToDo)</li> <li>Kubernetes Gateway API<ol> <li>Envoy Gateway</li> <li>Istio</li> <li>Kong</li> <li>NGINX Kubernetes Gateway</li> <li>traefik</li> </ol> </li> <li>Ingress<ol> <li>ingress-nginx-controller</li> </ol> </li> </ol>"},{"location":"#7-distributed-database-storage","title":"7. DISTRIBUTED DATABASE &amp; STORAGE","text":"<ol> <li>etcd</li> <li>Vitess: https://github.com/vitessio/vitess (ToDo)</li> <li>Rook: https://rook.io/ (ToDo)</li> <li>TiDB: https://github.com/pingcap/tidb (ToDo)</li> <li>TimescaleDB: https://github.com/timescale/timescaledb-kubernetes (ToDo)</li> <li>Others: Databases<ol> <li>mysql-operator</li> <li>postgres-operator</li> </ol> </li> </ol>"},{"location":"#8-streaming-messaging","title":"8. STREAMING &amp; MESSAGING","text":"<ol> <li>gRPC: https://grpc.io/ (ToDo)</li> <li>NATS: https://nats.io/ (ToDo)</li> <li>cloudevents: https://cloudevents.io/ (ToDo)</li> </ol>"},{"location":"#9-container-registry-runtime","title":"9. CONTAINER REGISTRY &amp; RUNTIME","text":"<ol> <li>containerd: https://containerd.io/ (ToDo)</li> <li>harbor: https://goharbor.io/ (ToDo)</li> <li>cri-o: https://cri-o.io/ (ToDo)</li> </ol>"},{"location":"#10-software-distribution","title":"10. SOFTWARE DISTRIBUTION","text":"<ol> <li>The Update Framework: https://theupdateframework.io/ (ToDo)</li> <li>Notary: https://notaryproject.dev/ (ToDo)</li> </ol>"},{"location":"PRACTICE/","title":"Practice","text":""},{"location":"PRACTICE/#practice-1-install-elasticsearch-kibana-filebeat-with-helm","title":"Practice 1: Install Elasticsearch, Kibana &amp; Filebeat with Helm","text":"<ol> <li> <p>Create namespace</p> <pre><code>kubectl create namespace eck\n</code></pre> </li> <li> <p>Add elastic Helm</p> <pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> </li> <li> <p>Install ES</p> <pre><code>helm install -n eck elasticsearch elastic/elasticsearch -f helm/es-config.yaml\n</code></pre> </li> <li> <p>Install Kibana</p> <pre><code>helm install -n eck kibana elastic/kibana -f helm/kb-config.yaml\n</code></pre> </li> <li> <p>Install filebeat</p> <pre><code>helm install -n eck filebeat elastic/filebeat --version 7.8.1 -f helm/filebeat-config.yaml\n</code></pre> </li> </ol>"},{"location":"PRACTICE/#practice-2-install-kafka-cluster-kafka-connect-with-strimzi","title":"Practice 2: Install Kafka Cluster + Kafka Connect with Strimzi","text":"<ul> <li> <p>Kafka</p> <ol> <li>Update the kafka-connect-twitter with your own API token</li> <li> <p>Apply Kafka</p> <pre><code>kubectl create namespace kafka-strimzi-18\nkubectl apply -k strimzi/overlays/kafka-strimzi-18\n</code></pre> </li> </ol> </li> </ul> <p></p> <pre><code>NAMESPACE          NAME                                                             READY   STATUS    RESTARTS   AGE\neck                elasticsearch-master-0                                           1/1     Running   0          14h\neck                kibana-kibana-55f4bc96f5-7fz65                                   1/1     Running   0          14h\nkafka-strimzi-18   kafka-connect-sink-connect-847cfbf66-gwtkl                       1/1     Running   0          7h27m\nkafka-strimzi-18   kafka-connect-source-connect-57bf7974f7-sz8ww                    1/1     Running   0          7h27m\nkafka-strimzi-18   my-cluster-entity-operator-579cdc77bc-v6rxt                      3/3     Running   5          14h\nkafka-strimzi-18   my-cluster-kafka-0                                               2/2     Running   0          14h\nkafka-strimzi-18   my-cluster-kafka-1                                               2/2     Running   0          14h\nkafka-strimzi-18   my-cluster-kafka-2                                               2/2     Running   2          14h\nkafka-strimzi-18   my-cluster-zookeeper-0                                           1/1     Running   0          14h\nkafka-strimzi-18   strimzi-cluster-operator-6c9d899778-nkd9q                        1/1     Running   0          14h\nkube-system        kube-dns-869d587df7-7whsm                                        3/3     Running   0          14h\nkube-system        kube-dns-869d587df7-z659j                                        3/3     Running   0          14h\nkube-system        kube-dns-autoscaler-645f7d66cf-r9ttj                             1/1     Running   0          14h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-nod-9dff1786-x4wz   1/1     Running   0          14h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-7jsz   1/1     Running   0          93s\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-cnl2   1/1     Running   0          14h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-f6cb   1/1     Running   0          14h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-vw9d   1/1     Running   0          14h\nkube-system        l7-default-backend-678889f899-fvswg                              1/1     Running   0          14h\nkube-system        metrics-server-v0.3.6-7b7d6c7576-msl8x                           2/2     Running   0          14h\n</code></pre>"},{"location":"PRACTICE/#practice-3-install-prometheus-grafana-with-kube-prometheus","title":"Practice 3: Install Prometheus &amp; Grafana with kube-prometheus","text":"<ul> <li> <p>Prometheus &amp; Grafana</p> <pre><code>git clone https://github.com/coreos/kube-prometheus.git &amp;&amp; kube-prometheus\n</code></pre> <pre><code>kubectl apply -f manifests/setup\n</code></pre> <p>wait a few minutes</p> <pre><code>kubectl create -f manifests\n</code></pre> </li> <li> <p>Add strimzi monitoring</p> <pre><code>kubectl apply -f strimzi/monitoring/prometheus-prometheus.yaml,strimzi/monitoring/prometheus-clusterRole.yaml\n</code></pre> </li> <li> <p>Add elasticsearch monitoring</p> </li> </ul> <p></p> <pre><code>kubectl get pod --all-namespaces\nNAMESPACE          NAME                                                             READY   STATUS    RESTARTS   AGE\neck                elasticsearch-master-0                                           1/1     Running   0          3d3h\neck                kibana-kibana-55f4bc96f5-7fz65                                   1/1     Running   0          3d4h\nkafka-strimzi-18   kafka-connect-sink-connect-75db959966-sxqxx                      1/1     Running   0          43m\nkafka-strimzi-18   kafka-connect-source-connect-6bc6d8797c-rr2x2                    1/1     Running   0          42m\nkafka-strimzi-18   my-cluster-entity-operator-579cdc77bc-v6rxt                      3/3     Running   0          3d4h\nkafka-strimzi-18   my-cluster-kafka-0                                               2/2     Running   0          2d13h\nkafka-strimzi-18   my-cluster-kafka-1                                               2/2     Running   0          2d13h\nkafka-strimzi-18   my-cluster-kafka-2                                               2/2     Running   0          2d13h\nkafka-strimzi-18   my-cluster-zookeeper-0                                           1/1     Running   50         2d1h\nkafka-strimzi-18   my-cluster-zookeeper-1                                           1/1     Running   16         2d1h\nkafka-strimzi-18   my-cluster-zookeeper-2                                           1/1     Running   0          2d1h\nkafka-strimzi-18   strimzi-cluster-operator-6c9d899778-nkd9q                        1/1     Running   0          3d4h\nkube-system        kube-dns-869d587df7-7whsm                                        3/3     Running   0          3d4h\nkube-system        kube-dns-869d587df7-z659j                                        3/3     Running   0          3d4h\nkube-system        kube-dns-autoscaler-645f7d66cf-r9ttj                             1/1     Running   0          3d4h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-nod-9dff1786-x4wz   1/1     Running   0          3d4h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-7jsz   1/1     Running   0          2d13h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-cnl2   1/1     Running   0          3d4h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-f6cb   1/1     Running   0          3d4h\nkube-system        kube-proxy-gke-my-gke-cluster-my-gke-cluster-pre-19639e01-vw9d   1/1     Running   0          3d4h\nkube-system        l7-default-backend-678889f899-fvswg                              1/1     Running   0          3d4h\nkube-system        metrics-server-v0.3.6-7b7d6c7576-msl8x                           2/2     Running   0          3d4h\nmonitoring         alertmanager-main-0                                              2/2     Running   0          13h\nmonitoring         alertmanager-main-1                                              2/2     Running   0          13h\nmonitoring         alertmanager-main-2                                              2/2     Running   0          13h\nmonitoring         grafana-58dc7468d7-vnsbh                                         1/1     Running   0          13h\nmonitoring         kube-state-metrics-765c7c7f95-fhkls                              3/3     Running   0          13h\nmonitoring         node-exporter-bjq6x                                              2/2     Running   0          13h\nmonitoring         node-exporter-d7dx8                                              2/2     Running   0          13h\nmonitoring         node-exporter-ddmxd                                              2/2     Running   0          13h\nmonitoring         node-exporter-mj6tx                                              2/2     Running   0          13h\nmonitoring         node-exporter-psf45                                              2/2     Running   0          13h\nmonitoring         prometheus-adapter-5cd5798d96-fkd75                              1/1     Running   0          13h\nmonitoring         prometheus-k8s-0                                                 3/3     Running   1          12h\nmonitoring         prometheus-k8s-1                                                 3/3     Running   1          12h\nmonitoring         prometheus-operator-5f75d76f9f-xtgqz                             1/1     Running   0          2d5h\n</code></pre>"},{"location":"PRACTICE/#practice-4-kafka-exporter-mirrormaker2","title":"Practice 4: Kafka exporter &amp; MirrorMaker2","text":"<ol> <li> <p>Enable the cluster operator to watch the other namespace</p> <pre><code>+  - strimzi-0.18.0/install/cluster-operator/050-Deployment-strimzi-cluster-operator.yaml\n</code></pre> <pre><code>kubectl apply -k strimzi/overlays/kafka-strimzi-18\n</code></pre> </li> <li> <p>Deploy new <code>Kafka</code> cluster and <code>KafkaMirrorMaker2</code> in the other namespace <code>kafka-strimzi-18-staging</code></p> <pre><code>kubectl apply -k strimzi/overlays/kafka-strimzi-18-staging\n</code></pre> </li> <li> <p>Clean up</p> <pre><code>kubectl delete -k strimzi/overlays/kafka-strimzi-18-staging\n</code></pre> </li> </ol> <p></p>"},{"location":"PRACTICE/#practice-5-horizontal-pod-autoscaler-hpa-basic","title":"Practice 5: Horizontal Pod Autoscaler (HPA) (basic)","text":"<ol> <li> <p>Install metrics-server</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> </li> <li> <p>Apply an apache application</p> <pre><code>kubectl apply -f https://k8s.io/examples/application/php-apache.yaml\n</code></pre> </li> <li> <p>Set autoscale by kubectl</p> <pre><code>kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n</code></pre> </li> <li> <p>Increase load -&gt; confirm HPA is working</p> <pre><code>kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\"\n</code></pre> <pre><code>kubectl get hpa\n\nNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache   Deployment/php-apache   76%/50%   1         10        7          4m10s\n</code></pre> </li> </ol>"},{"location":"PRACTICE/#practice-6-hpa-with-custom-metrics-advanced","title":"Practice 6: HPA with custom metrics (advanced)","text":"<p>autoscaler/hpa/custom-metrics</p> <p>Steps:</p> <ol> <li>Prometheus Operator:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml\n</code></pre></li> <li>Prometheus:     <pre><code>kubectl create ns monitoring; kubectl apply -k prometheus-operator -n monitoring\n</code></pre></li> <li>RabbitMQ Operator:     ```     kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml     ````</li> <li>RabbitMQ:     <pre><code>kubectl apply -f autoscaler/hpa/custom-metrics/rabbitmq/rabbitmq-cluster.yaml\nkubectl apply -f autoscaler/hpa/custom-metrics/rabbitmq/pod-monitor-rabbitmq.yaml\n</code></pre></li> <li>RabbitMQ producer:     <pre><code>kubectl apply -f autoscaler/hpa/custom-metrics/rabbitmq-producer-cronjob.yaml\n</code></pre></li> <li>RabbitMQ consumer:     <pre><code>kubectl apply -f autoscaler/hpa/custom-metrics/rabbitmq-consumer-deployment.yaml\n</code></pre></li> <li>Prometheus-Adapter: Extend the Kubernetes custom metrics API with the metrics. (https://github.com/kubernetes-sigs/prometheus-adapter)     <pre><code>cd autoscaler/hpa/custom-metrics/k8s-prom-hpa\ntouch metrics-ca.key metrics-ca.crt metrics-ca-config.json\nmake certs\ncd -\nkubectl create -f autoscaler/hpa/custom-metrics/k8s-prom-hpa/custom-metrics-api\n</code></pre></li> <li>Apply HPA     <pre><code>kubectl apply -f autoscaler/hpa/custom-metrics/rabbitmq-consumer-hpa.yaml\n</code></pre></li> </ol> <p></p>"},{"location":"TESTING/","title":"Testing (WIP)","text":"<p>Using KUTTL to test the behaviors of yamls</p>"},{"location":"TESTING/#prerequisite","title":"Prerequisite","text":"<ul> <li>Docker</li> <li>Kind</li> </ul>"},{"location":"TESTING/#coverage","title":"Coverage","text":"<ol> <li>prometheus-operator</li> </ol>"},{"location":"TESTING/#how-to-add-a-test","title":"How to add a test","text":"<ol> <li>Create a directory in <code>test</code>. e.g. <code>prometheus-operator</code></li> <li>Write your <code>TestStep</code> in the directory.</li> <li>Run in your local     <pre><code>kubectl kuttl test --start-kind=false\n</code></pre></li> </ol>"},{"location":"actions-runner-controller/","title":"Actions Runner Controller","text":"<p>https://github.com/actions-runner-controller/actions-runner-controller</p> <p></p>"},{"location":"argocd/","title":"ArgoCD","text":"<ul> <li>Github: https://github.com/argoproj/argo-cd</li> <li>Docs: https://argo-cd.readthedocs.io/en/stable/</li> <li>Icon: https://cncf-branding.netlify.app/projects/argo/</li> </ul>"},{"location":"argocd/#version","title":"Version","text":"<ul> <li>v2.6.7</li> </ul>"},{"location":"argocd/#install","title":"Install","text":"<pre><code>kubectl create namespace argocd\nkubectl apply -k setup # v1.21 or later\n# kubectl kustomize setup | kubectl apply -f - # before v1.21\n</code></pre> <p>Check all the pods are running</p> <pre><code>kubectl get pod -n argocd\nNAME                                                READY   STATUS    RESTARTS   AGE\nargocd-application-controller-0                     1/1     Running   0          90s\nargocd-applicationset-controller-547665bdfd-rjgvm   1/1     Running   0          94s\nargocd-dex-server-865966bc45-s6fgf                  1/1     Running   0          94s\nargocd-notifications-controller-c47c9f869-2smdk     1/1     Running   0          93s\nargocd-redis-896595fb7-h9pmr                        1/1     Running   0          93s\nargocd-repo-server-678c6cc99c-kkzmd                 1/1     Running   0          93s\nargocd-server-96cdb4df5-mxsq2                       1/1     Running   0          92s\n</code></pre>"},{"location":"argocd/#login","title":"Login","text":"<pre><code>kubectl -n argocd port-forward service/argocd-server 8080:80\n</code></pre> <ul> <li>user: <code>admin</code></li> <li>password: <code>kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath='{.data.password}' | base64 --decode</code> (v1.9.0 or later, there's a secret for initial admin password.)</li> </ul>"},{"location":"argocd/#add-argocd-appproject-application","title":"Add ArgoCD AppProject &amp; Application","text":"<ul> <li>AppProject: <code>dev</code></li> <li> <p>Application: <code>guestbook-kustomize-dev</code>.</p> </li> <li> <p>Deploy application with ArgoCD</p> <pre><code>kubectl apply -f project/dev\n</code></pre> </li> <li> <p>Check in console</p> <pre><code>kubectl get pod -n dev\nNAME                                          READY   STATUS    RESTARTS   AGE\ndev-kustomize-guestbook-ui-7574c75879-6cfnq   1/1     Running   0          95s\n</code></pre> </li> <li> <p>Check on ArgoCD</p> <p></p> </li> </ul>"},{"location":"argocd/#manage-argocd-by-argocd","title":"Manage argocd by argocd","text":"<pre><code>kubectl apply -f project/argocd/project.yaml,project/argocd/app-argocd.yaml\n</code></pre>"},{"location":"argocd/#clean-up","title":"Clean up","text":"<pre><code>kubectl delete -f project/dev\nkubectl delete -k setup\nkubectl delete ns argocd\n</code></pre>"},{"location":"argocd/#argocd-notifications","title":"ArgoCD Notifications","text":"<p>Github: https://github.com/argoproj-labs/argocd-notifications Docs: https://argocd-notifications.readthedocs.io/en/stable/</p>"},{"location":"argocd/#prerequisite","title":"Prerequisite","text":"<p>ArgoCD is installed.</p>"},{"location":"argocd/#version_1","title":"Version","text":"<ul> <li>v1.1.1</li> </ul>"},{"location":"argocd/#manage-by-kustomize","title":"Manage by kustomize","text":""},{"location":"argocd/#basic","title":"Basic","text":"<ol> <li> <p>Install</p> <pre><code>kubectl apply -k setup-notifications/base\n</code></pre> </li> <li> <p>Clean up</p> <pre><code>kubectl delete -k setup-notifications/base\n</code></pre> </li> </ol>"},{"location":"argocd/#manage-by-argocd","title":"Manage by ArgoCD","text":"<pre><code>kubectl apply -f project/argocd/project.yaml,project/argocd/app-argocd-notifications.yaml\n</code></pre>"},{"location":"argocd/#manage-by-helm","title":"Manage by Helm","text":"<ol> <li> <p>Install</p> <pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm install argo/argocd-notifications --generate-name -n argocd -f argocd/setup-notification-with-helm/value.yaml\n</code></pre> <p>failed:</p> <pre><code>Error: YAML parse error on argocd-notifications/templates/configmap.yaml: error converting YAML to JSON: yaml: line 15: did not find expected key\n</code></pre> <p>https://github.com/argoproj/argo-helm/issues/616 \u2192 fixed in https://github.com/argoproj-labs/argocd-notifications/pull/315</p> </li> </ol>"},{"location":"argocd/#configure-slack-notification","title":"Configure Slack Notification","text":"<p>https://argocd-notifications.readthedocs.io/en/stable/services/slack/</p> <ol> <li>Prepare Slack bot and channel.<ol> <li>Create Slack Application with <code>chat:write:bot</code> permission.</li> <li>Get OAuth token.</li> <li>Create Slack channel (e.g. <code>#argocd-notifications</code>).</li> <li>Add your bot to this channel. (<code>/invite @&lt;your-app-name&gt;</code>)</li> </ol> </li> <li>Install either by Kustomize or Helm:<ul> <li> <p>Kustomize:</p> <ol> <li> <p>Prepare Secret <code>argocd-notifications-secret</code> (<code>setup-notifications/overlays/slack/argocd-notification-secret.yaml</code>)     <pre><code>cp setup-notifications/overlays/slack/argocd-notification-secret-sample.yaml setup-notifications/overlays/slack/argocd-notification-secret.yaml\n</code></pre></p> <p>Fill your Slack token.</p> </li> <li> <p>Prepare <code>argocd-notifications-cm</code> (<code>setup-notifications/overlays/slack/argocd-notification-cm.yaml</code>)</p> <ul> <li><code>context</code></li> <li><code>subscriptions</code></li> <li><code>service.slack</code><ul> <li>Optionally, you can set argo icon https://cncf-branding.netlify.app/projects/argo/ in your Slack workspace. (seems not working)</li> </ul> </li> <li>Just writing ConfigMap doesn't work -&gt; Added annotation in Application or Project (Subscribe to notifications by adding the notifications.argoproj.io/subscribe.on-sync-succeeded.slack annotation to the Argo CD application or project)     <pre><code>kubectl patch app &lt;my-app&gt; -n argocd -p '{\"metadata\": {\"annotations\": {\"notifications.argoproj.io/subscribe.on-sync-succeeded.slack\":\"&lt;my-channel&gt;\"}}}' --type merge\n</code></pre></li> </ul> </li> <li>Apply     <pre><code>kubectl apply -k setup-notifications/overlays/slack\n</code></pre><ul> <li>Helm:</li> </ul> </li> </ol> <pre><code>helm install argo/argocd-notifications --generate-name -n argocd -f setup-notification-with-helm/value.yaml --set secret.items.slack-token=&lt;SLACK BOT TOKEN&gt;\n</code></pre> </li> </ul> </li> </ol>"},{"location":"autoscaler/","title":"Autoscaler","text":""},{"location":"autoscaler/#overview","title":"Overview","text":"<ol> <li>Cluster Autoscaler (CA) ca</li> <li>Horizontal Pod Autoscaler (HPA) hpa</li> <li>Virtical Pod Autoscaler (VPA)</li> </ol>"},{"location":"autoscaler/#details","title":"Details","text":""},{"location":"autoscaler/#ca","title":"CA","text":"<p>cluster-autoscaler</p>"},{"location":"autoscaler/#hpa","title":"HPA","text":"<p>horizontal-pod-autoscaler</p>"},{"location":"autoscaler/#vpa","title":"VPA","text":"<p>vertical-pod-autoscaler</p> <ul> <li>Keeping limit proportional to request Automatically set resource limit values based on limit to request ratios specified as part of the container template.</li> </ul> <p>prerequisite</p> <ul> <li>Install metrics server     <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre></li> </ul> <pre><code>git clone https://github.com/kubernetes/autoscaler.git\ncd autoscaler/vertical-pod-autoscaler\n./hack/vpa-up.sh # if you get `unknown option -addext`, ./hack/vpa-up.sh on the [0.8 release branch](https://github.com/kubernetes/autoscaler/tree/vpa-release-0.8)\n</code></pre> <pre><code>brew update\nbrew upgrade openssl\nbrew list openssl\n</code></pre> <pre><code>echo 'export PATH=/usr/local/Cellar/openssl@1.1/1.1.1i/bin:$PATH' &gt;&gt; ~/.zshrc\n</code></pre> <pre><code>which openssl\n/usr/local/Cellar/openssl@1.1/1.1.1i/bin/openssl\nopenssl version\nOpenSSL 1.1.1i  8 Dec 2020\n</code></pre> <pre><code>./hack/vpa-up.sh\n</code></pre> result <pre><code>customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalercheckpoints.autoscaling.k8s.io created\ncustomresourcedefinition.apiextensions.k8s.io/verticalpodautoscalers.autoscaling.k8s.io created\nclusterrole.rbac.authorization.k8s.io/system:metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:vpa-actor created\nclusterrole.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created\nclusterrole.rbac.authorization.k8s.io/system:evictioner created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-reader created\nclusterrolebinding.rbac.authorization.k8s.io/system:vpa-actor created\nclusterrolebinding.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created\nclusterrole.rbac.authorization.k8s.io/system:vpa-target-reader created\nclusterrolebinding.rbac.authorization.k8s.io/system:vpa-target-reader-binding created\nclusterrolebinding.rbac.authorization.k8s.io/system:vpa-evictionter-binding created\nserviceaccount/vpa-admission-controller created\nclusterrole.rbac.authorization.k8s.io/system:vpa-admission-controller created\nclusterrolebinding.rbac.authorization.k8s.io/system:vpa-admission-controller created\nclusterrole.rbac.authorization.k8s.io/system:vpa-status-reader created\nclusterrolebinding.rbac.authorization.k8s.io/system:vpa-status-reader-binding created\nserviceaccount/vpa-updater created\ndeployment.apps/vpa-updater created\nserviceaccount/vpa-recommender created\ndeployment.apps/vpa-recommender created\nGenerating certs for the VPA Admission Controller in /tmp/vpa-certs.\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n.........................................................................................................................................................................................................................................................................................+++++\n....+++++\ne is 65537 (0x010001)\nGenerating RSA private key, 2048 bit long modulus (2 primes)\n...+++++\n....................+++++\ne is 65537 (0x010001)\nSignature ok\nsubject=CN = vpa-webhook.kube-system.svc\nGetting CA Private Key\nUploading certs to the cluster.\nsecret/vpa-tls-certs created\nDeleting /tmp/vpa-certs.\ndeployment.apps/vpa-admission-controller created\nservice/vpa-webhook created\n</code></pre> <pre><code>kubectl get pod -n kube-system | grep vpa\nvpa-admission-controller-664b5997b7-48ck9   1/1     Running   0          78s\nvpa-recommender-5b768c88-nfj6k              1/1     Running   0          78s\nvpa-updater-c9ffff655-6f7sb                 1/1     Running   0          78s\n</code></pre> <p>example</p> <pre><code>kubectl create -f examples/hamster.yaml\n</code></pre> <pre><code>kubectl get pod -o jsonpath='{.items[].spec.containers[].resources}' | jq\n{\n  \"requests\": {\n    \"cpu\": \"100m\",\n    \"memory\": \"50Mi\"\n  }\n}\n</code></pre> <p>Five minutes later, ... nothing happened..</p>"},{"location":"autoscaler/ca/","title":"Cluster Autoscaler","text":""},{"location":"autoscaler/ca/#reference","title":"Reference","text":"<ul> <li>AWS: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</li> </ul>"},{"location":"autoscaler/ca/#prerequisite","title":"Prerequisite","text":"<p>AWS: - Need the following policy:</p> <pre><code>```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:DescribeAutoScalingGroups\",\n                \"autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeLaunchConfigurations\",\n                \"autoscaling:SetDesiredCapacity\",\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\"\n            ],\n            \"Resource\": [\"*\"]\n        }\n    ]\n}\n```\n</code></pre> <ul> <li>Need one of the followings to grant the permission to cluster-autoscaler:<ul> <li>Add the policy to an IAM role and grant the permission to <code>ServiceAccount</code> (EKS)</li> <li>Add the policy to an IAM user and set the access key and secret key in <code>Secret</code></li> </ul> </li> <li>Auto-discovery:<ul> <li>Worker nodes need specific tag: <code>k8s.io/cluster-autoscaler/enabled: true</code></li> <li>args of cluster-autoscaler:  <code>--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/&lt;cluster-name&gt;</code></li> </ul> </li> </ul>"},{"location":"autoscaler/ca/#install-cluster-autoscaler","title":"Install Cluster Autoscaler","text":"<p>AWS:</p> <pre><code>kubectl apply -k autoscaler/ca/aws/\n</code></pre>"},{"location":"autoscaler/ca/#test-app","title":"test app","text":"<pre><code>kubectl apply -f nginx.yaml\nkubectl get deployment/nginx-to-scaleout\n\nNAME                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-to-scaleout   1         1         1            0           8s\n</code></pre> <p>scale out to 10 replicas</p> <pre><code>kubectl scale --replicas=10 deployment/nginx-to-scaleout\n</code></pre> <p>-&gt; automatically add two new nodes</p> <pre><code>kubectl get nodes\nNAME                                            STATUS    ROLES     AGE       VERSION\nip-10-0-0-16.ap-northeast-1.compute.internal    Ready     &lt;none&gt;    2m        v1.12.7\nip-10-0-0-82.ap-northeast-1.compute.internal    Ready     &lt;none&gt;    1h        v1.12.7\nip-10-0-1-177.ap-northeast-1.compute.internal   Ready     &lt;none&gt;    1m        v1.12.7\nip-10-0-1-224.ap-northeast-1.compute.internal   Ready     &lt;none&gt;    1h        v1.12.7\n</code></pre> <p>scale in to 2 replicas</p> <pre><code>kubectl scale --replicas=2 deployment/nginx-to-scaleout\n</code></pre> <p>aboute ten mins later ... -&gt; scaling down</p> <pre><code>I0727 15:41:44.855775       1 scale_down.go:387] ip-10-0-0-16.ap-northeast-1.compute.internal was unneeded for 10m4.589275269s\nI0727 15:41:44.855814       1 scale_down.go:387] ip-10-0-1-177.ap-northeast-1.compute.internal was unneeded for 7m18.53883852s\nI0727 15:41:44.937768       1 scale_down.go:594] Scale-down: removing empty node ip-10-0-0-16.ap-northeast-1.compute.internal\nI0727 15:41:44.938199       1 factory.go:33] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"kube-system\", Name:\"cluster-autoscaler-status\", UID:\"2d9cdefe-b081-11e9-be78-0a78d103117e\", APIVersion:\"v1\", ResourceVersion:\"9275\", FieldPath:\"\"}): type: 'Normal' reason: 'ScaleDownEmpty' Scale-down: removing empty node ip-10-0-0-16.ap-northeast-1.compute.internal\nI0727 15:41:44.952658       1 delete.go:53] Successfully added toBeDeletedTaint on node ip-10-0-0-16.ap-northeast-1.compute.internal\nI0727 15:41:45.162880       1 aws_manager.go:341] Terminating EC2 instance: i-05df3b79e52165a1d\nI0727 15:41:45.163164       1 factory.go:33] Event(v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"ip-10-0-0-16.ap-northeast-1.compute.internal\", UID:\"38a0545b-b083-11e9-be78-0a78d103117e\", APIVersion:\"v1\", ResourceVersion:\"9282\", FieldPath:\"\"}): type: 'Normal' reason: 'ScaleDown' node removed by cluster autoscaler\n</code></pre>"},{"location":"autoscaler/ca/#check-logs","title":"check logs","text":"<pre><code>kubectl logs -f deployment/cluster-autoscaler -n kube-system\n</code></pre>"},{"location":"autoscaler/ca/#node-group-is-not-ready","title":"Node group is not ready","text":"<pre><code>W0727 15:23:39.646482       1 scale_up.go:105] Node group terraform-eks-demo is not ready for scaleup\n</code></pre> <p>max instance is too small or something</p>"},{"location":"autoscaler/hpa/","title":"Horizontal Pod Autoscaler","text":""},{"location":"autoscaler/hpa/#overview","title":"Overview","text":""},{"location":"autoscaler/hpa/#what-hpa-does","title":"What HPA does","text":"<p>Automatically scales the number of Pods in a replication controller, deployment, replica set or stateful set based on observed CPU utilization or with custom metrics.</p>"},{"location":"autoscaler/hpa/#api-object","title":"API Object","text":"<ul> <li>API Group: <code>autoscaling</code></li> <li>CPU autoscaling: <code>autoscaling/v1</code></li> <li>memory &amp; custom metrics: <code>autoscaling/v2beta2</code></li> </ul>"},{"location":"autoscaler/hpa/#kubectl","title":"Kubectl","text":"<pre><code>kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80\n</code></pre>"},{"location":"autoscaler/hpa/#examples","title":"Examples","text":""},{"location":"autoscaler/hpa/#cpu","title":"CPU","text":"<ol> <li> <p>Install <code>metrics-server</code></p> <p>The HorizontalPodAutoscaler normally fetches metrics from a series of aggregated APIs (metrics.k8s.io, custom.metrics.k8s.io, and external.metrics.k8s.io). The metrics.k8s.io API is usually provided by metrics-server</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> </li> <li> <p>Apply an apache application</p> <pre><code>kubectl apply -f https://k8s.io/examples/application/php-apache.yaml\n</code></pre> </li> <li> <p>Set autoscale by kubectl</p> <pre><code>kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n</code></pre> <pre><code>kubectl get hpa\nNAME         REFERENCE               TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache   Deployment/php-apache   &lt;unknown&gt;/50%   1         10        0          9s\n</code></pre> <pre><code>apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\n  namespace: default\nspec:\n  maxReplicas: 10\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: php-apache\n  targetCPUUtilizationPercentage: 50\n</code></pre> </li> <li> <p>Increase the load</p> <pre><code>kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\"\n</code></pre> <pre><code>kubectl get hpa\n\nNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache   Deployment/php-apache   76%/50%   1         10        7          4m10s\n</code></pre> </li> </ol>"},{"location":"autoscaler/hpa/custom-metrics/","title":"HPA with custom metrics","text":""},{"location":"autoscaler/hpa/custom-metrics/#overview","title":"Overview","text":"<ul> <li>RabbitMQ Producer (Java app)</li> <li>RabbitMQ</li> <li>RabbitMQ Consumer (Java app)</li> <li>Prometheus -&gt; http://localhost:30900</li> <li>Grafana -&gt; http://localhost:32111</li> </ul>"},{"location":"autoscaler/hpa/custom-metrics/#1-deploy-prometheus-with-prometheus-operator","title":"1. Deploy Prometheus with prometheus-operator","text":"<p>References: - https://github.com/prometheus-operator/prometheus-operator#quickstart - https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md</p> <p>Steps:</p> <ol> <li> <p>Create prometheus operator</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml\n</code></pre> </li> <li> <p>Prometheus</p> <pre><code>kubectl create ns monitoring\n</code></pre> <pre><code>kubectl apply -k ../../../prometheus-operator -n monitoring\n</code></pre> </li> <li> <p>Check UI at http://localhost:30900</p> <p>You can check targets</p> <p></p> </li> </ol> <p>Monitoring RabbitMQ: https://www.rabbitmq.com/kubernetes/operator/operator-monitoring.html</p> <p>We cannot use <code>ServiceMonitor</code> for RabbitMQ as RabbitMQ service doesn't have prometheus port (15692). We need to use <code>PodMonitor</code> as is recommended in the documentation.</p>"},{"location":"autoscaler/hpa/custom-metrics/#2-deploy-rabbitmq-with-operator","title":"2. Deploy RabbitMQ with operator","text":"<p>https://www.rabbitmq.com/kubernetes/operator/quickstart-operator.html</p> <ol> <li> <p>RabbitMQ Operator</p> <pre><code>kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\n</code></pre> </li> <li> <p>Create a RabbitMQ cluster</p> <pre><code>kubectl apply -f rabbitmq/rabbitmq-cluster.yaml\n</code></pre> </li> <li> <p>Create <code>PodMonitor</code> for RabbitMQ</p> <pre><code>kubectl apply -f rabbitmq/pod-monitor-rabbitmq.yaml\n</code></pre> </li> <li> <p>Get username and password</p> <pre><code>username=\"$(kubectl get secret rabbitmq-default-user -o jsonpath='{.data.username}' | base64 --decode)\"\necho \"username: $username\"\npassword=\"$(kubectl get secret rabbitmq-default-user -o jsonpath='{.data.password}' | base64 --decode)\"\necho \"password: $password\"\n</code></pre> </li> <li> <p>port-forward</p> <pre><code>kubectl port-forward \"service/rabbitmq\" 15672\n</code></pre> <p>Open: http://localhost:15672/ and use the username and password got in the previous step.</p> </li> </ol> <p>Metrics:</p> <p>As of 3.8.0, RabbitMQ ships with built-in Prometheus &amp; Grafana support. Support for Prometheus metric collector ships in the rabbitmq_prometheus plugin. The plugin exposes all RabbitMQ metrics on a dedicated TCP port, in Prometheus text format.</p> <p>Check if <code>rabbitmq_prometheus</code> plugin is enabled.</p> <pre><code>kubectl exec -it rabbitmq-server-0 -- rabbitmq-plugins list | grep prometheus\n[E*] rabbitmq_prometheus               3.8.12\n</code></pre>"},{"location":"autoscaler/hpa/custom-metrics/#3-deploy-rabbitmq-producer-simple-java-app","title":"3. Deploy RabbitMQ producer (simple JAVA app)","text":"<p>Create <code>rabbitmq-producer</code> <code>CronJob</code> (run every five minutes)</p> <pre><code>kubectl apply -f rabbitmq-producer-cronjob.yaml\n</code></pre> <p>If you want to run a job manually, you can run the following command after creating <code>CronJob</code></p> <pre><code>kubectl create job --from=cronjob/rabbitmq-producer rabbitmq-producer-$(date '+%s')\n</code></pre>"},{"location":"autoscaler/hpa/custom-metrics/#4-deploy-rabbitmq-consumer-simple-java-app","title":"4. Deploy RabbitMQ consumer (simple JAVA app)","text":"<p>Create <code>rabbitmq-consumer</code> <code>Deployment</code></p> <pre><code>kubectl apply -f rabbitmq-consumer-deployment.yaml\n</code></pre>"},{"location":"autoscaler/hpa/custom-metrics/#5-deploy-grafana","title":"5. Deploy Grafana","text":"<p>https://devopscube.com/setup-grafana-kubernetes/</p> <pre><code>kubectl apply -k ../../../grafana\n</code></pre> <p>log in to http://localhost:32111 with <code>admin</code> for both username and password</p> <p>Dashboard 10991 is already imported</p> <p></p>"},{"location":"autoscaler/hpa/custom-metrics/#6-create-hpa-with-custom-metrics","title":"6. Create HPA with custom metrics","text":"<ol> <li>Collect metrics from your applications. (Already done by Prometheus)</li> <li> <p>Extend the Kubernetes custom metrics API with the metrics. (https://github.com/kubernetes-sigs/prometheus-adapter)</p> <p>Generate secrets</p> <pre><code>cd k8s-prom-hpa\ntouch metrics-ca.key metrics-ca.crt metrics-ca-config.json\nmake certs\ncd -\n</code></pre> </li> <li> <p>Deploy <code>prometheus-adapter</code></p> <pre><code>kubectl create -f ./k8s-prom-hpa/custom-metrics-api\n</code></pre> <p>Check the custom metrics API</p> <pre><code>kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/rabbitmq_queue_messages_ready\"| jq .\n</code></pre> <p>Custom Metrics API Result <pre><code>kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/rabbitmq_queue_messages_ready\"| jq .\n{\n  \"kind\": \"MetricValueList\",\n  \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\",\n  \"metadata\": {\n    \"selfLink\": \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/rabbitmq_queue_messages_ready\"\n  },\n  \"items\": [\n    {\n      \"describedObject\": {\n        \"kind\": \"Pod\",\n        \"namespace\": \"default\",\n        \"name\": \"rabbitmq-server-0\",\n        \"apiVersion\": \"/v1\"\n      },\n      \"metricName\": \"rabbitmq_queue_messages_ready\",\n      \"timestamp\": \"2021-03-27T12:01:15Z\",\n      \"value\": \"1274\"\n    }\n  ]\n}\n</code></pre> <li> <p>With custom API</p> <pre><code>kubectl apply -f rabbitmq-consumer-hpa.yaml\n</code></pre> <pre><code>kubectl describe hpa rabbitmq-consumer\nName:                                                                               rabbitmq-consumer\nNamespace:                                                                          default\nLabels:                                                                             &lt;none&gt;\nAnnotations:                                                                        &lt;none&gt;\nCreationTimestamp:                                                                  Sat, 27 Mar 2021 21:36:14 +0900\nReference:                                                                          Deployment/rabbitmq-consumer\nMetrics:                                                                            ( current / target )\n  \"rabbitmq_queue_messages_ready\" on Pod/rabbitmq-server-0 (target average value):  442 / 1\nMin replicas:                                                                       1\nMax replicas:                                                                       10\nDeployment pods:                                                                    4 current / 8 desired\nConditions:\n  Type            Status  Reason            Message\n  ----            ------  ------            -------\n  AbleToScale     True    SucceededRescale  the HPA controller was able to update the target scale to 8\n  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from external metric rabbitmq_queue_messages_ready(nil)\n  ScalingLimited  True    ScaleUpLimit      the desired replica count is increasing faster than the maximum scale rate\nEvents:\n  Type    Reason             Age   From                       Message\n  ----    ------             ----  ----                       -------\n  Normal  SuccessfulRescale  20s   horizontal-pod-autoscaler  New size: 8; reason: external metric rabbitmq_queue_messages_ready(nil) above target\n</code></pre> </li>"},{"location":"autoscaler/hpa/custom-metrics/#7-observe-the-behavior","title":"7. Observe the behavior","text":"<ul> <li> <p>Without HPA</p> <p></p> </li> <li> <p>With HPA</p> <p></p> </li> </ul>"},{"location":"autoscaler/hpa/custom-metrics/#8-clean-up","title":"8. Clean up","text":"<pre><code>kubectl delete -f rabbitmq-consumer-hpa.yaml\nkubectl delete -k ../../../grafana\nfor component in rabbitmq rabbitmq-consumer-deployment.yaml rabbitmq-producer-cronjob.yaml; do\n    kubectl delete -f $component\ndone\nkubectl delete -k ../../../prometheus-operator -n monitoring\nkubectl delete -f ./k8s-prom-hpa/custom-metrics-api\nkubectl delete -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\nkubectl delete -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml\nkubectl delete ns monitoring\n</code></pre>"},{"location":"autoscaler/hpa/custom-metrics/#references","title":"References","text":"<ul> <li>Horizontal Pod Autoscaler#support-for-custom-metrics</li> <li>Prometheus ServiceMonitor vs PodMonitor</li> <li>https://qiita.com/Kameneko/items/071c2a064775badd939e     &gt; \u305f\u3060\u3057\u30011\u70b9\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3001\u3053\u308c\u306fPod\u306e\u30e9\u30d9\u30eb\u3067\u306f\u306a\u304f\u3001Service\u2026\u66f4\u306b\u6b63\u3057\u304f\u8a00\u3048\u3070Endpoints\u306e\u30e9\u30d9\u30eb\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</li> <li>https://grafana.com/docs/grafana-cloud/quickstart/prometheus_operator/</li> <li>Troubleshooting ServiceMonitor Changes</li> <li>https://github.com/stefanprodan/k8s-prom-hpa</li> <li>https://github.com/kubernetes-sigs/prometheus-adapter</li> <li>https://github.com/luxas/kubeadm-workshop</li> </ul>"},{"location":"autoscaler/hpa/custom-metrics/#related-topics","title":"Related Topics","text":"<ul> <li> Shutdown RabbitMQ consumer gracefully</li> <li> [Issue] Too fast to scale out.</li> <li> Scale down to zero.</li> <li>Actually Kubernetes supports the scaling to zero only by means of an API call, since the Horizontal Pod Autoscaler does support scaling down to 1 replica only. (In Kubernetes, how can I scale a Deployment to zero when idle.)</li> <li>zero-pod-autoscaler</li> <li>Allow HPA to scale to 0</li> <li>Support scaling HPA to/from zero pods for object/external metrics</li> <li><code>HPAScaleToZero</code> in 1.16 Feature Gates       &gt; HPAScaleToZero: Enables setting minReplicas to 0 for HorizontalPodAutoscaler resources when using custom or external metrics.</li> </ul>"},{"location":"cert-manager/","title":"Cert Manager","text":""},{"location":"cert-manager/#install-cert-manager","title":"Install Cert Manager","text":""},{"location":"cert-manager/#install-with-yaml","title":"Install with yaml","text":"<pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.yaml\n</code></pre>"},{"location":"cert-manager/#install-with-cmctl","title":"Install with <code>cmctl</code>","text":"<ol> <li>Install <code>cmctl</code> following https://cert-manager.io/docs/usage/cmctl/#installation <pre><code>OS=$(go env GOOS); ARCH=$(go env GOARCH); curl -L -o cmctl.tar.gz https://github.com/jetstack/cert-manager/releases/latest/download/cmctl-$OS-$ARCH.tar.gz\ntar xzf cmctl.tar.gz\nsudo mv cmctl /usr/local/bin\n</code></pre> <pre><code>cmctl version\nClient Version: util.Version{GitVersion:\"v1.6.0\", GitCommit:\"49914a057b39c887be0974c4657c095bd7724bc7\", GitTreeState:\"clean\", GoVersion:\"go1.17.1\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nerror: could not detect the cert-manager version: the cert-manager CRDs are not yet installed on the Kubernetes API server\n</code></pre> <pre><code>cmctl help\n</code></pre></li> <li>Install cert manager with <code>cmctl</code> (https://cert-manager.io/docs/installation/cmctl/)     <pre><code>cmctl x install\n</code></pre> output <pre><code>cmctl x install\nCreating the cert-manager CRDs\ncreating 6 resource(s)\nClearing discovery cache\nbeginning wait for 6 resources with timeout of 1m0s\ncreating 1 resource(s)\ncreating 38 resource(s)\nbeginning wait for 38 resources with timeout of 5m0s\nDeployment is not ready: default/cert-manager-cainjector. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-cainjector. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-cainjector. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-cainjector. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-webhook. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-webhook. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-webhook. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-webhook. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-webhook. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-webhook. 0 out of 1 expected pods are ready\nDeployment is not ready: default/cert-manager-webhook. 0 out of 1 expected pods are ready\nStarting delete for \"cert-manager-startupapicheck\" ServiceAccount\nserviceaccounts \"cert-manager-startupapicheck\" not found\ncreating 1 resource(s)\nStarting delete for \"cert-manager-startupapicheck:create-cert\" Role\nroles.rbac.authorization.k8s.io \"cert-manager-startupapicheck:create-cert\" not found\ncreating 1 resource(s)\nStarting delete for \"cert-manager-startupapicheck:create-cert\" RoleBinding\nrolebindings.rbac.authorization.k8s.io \"cert-manager-startupapicheck:create-cert\" not found\ncreating 1 resource(s)\nStarting delete for \"cert-manager-startupapicheck\" Job\njobs.batch \"cert-manager-startupapicheck\" not found\ncreating 1 resource(s)\nWatching for changes to Job cert-manager-startupapicheck with timeout of 5m0s\nAdd/Modify event for cert-manager-startupapicheck: ADDED\ncert-manager-startupapicheck: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\nAdd/Modify event for cert-manager-startupapicheck: MODIFIED\ncert-manager-startupapicheck: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\nAdd/Modify event for cert-manager-startupapicheck: MODIFIED\nStarting delete for \"cert-manager-startupapicheck\" ServiceAccount\nStarting delete for \"cert-manager-startupapicheck:create-cert\" Role\nStarting delete for \"cert-manager-startupapicheck:create-cert\" RoleBinding\nStarting delete for \"cert-manager-startupapicheck\" Job\nNAME: cert-manager\nLAST DEPLOYED: Thu Oct 28 10:29:34 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nDESCRIPTION: Cert-manager was installed using the cert-manager CLI\nNOTES:\ncert-manager v1.6.0 has been deployed successfully!\nIn order to begin issuing certificates, you will need to set up a ClusterIssuer\nor Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).\nMore information on the different types of issuers and how to configure them\ncan be found in our documentation:\nhttps://cert-manager.io/docs/configuration/\nFor information on how to configure cert-manager to automatically provision\nCertificates for Ingress resources, take a look at the `ingress-shim`\ndocumentation:\nhttps://cert-manager.io/docs/usage/ingress/\n</code></pre> <pre><code>kubectl get po\nNAME                                       READY   STATUS    RESTARTS   AGE\ncert-manager-6c576bddcf-jl92p              1/1     Running   0          110s\ncert-manager-cainjector-669c966b86-mx956   1/1     Running   0          110s\ncert-manager-webhook-7d6cf57d55-9qvxk      1/1     Running   0          110s\n</code></pre> Uninstall <pre><code>cmctl x install --dry-run &gt; cert-manager.custom.yaml\nkubectl delete -f cert-manager.custom.yaml\n</code></pre> </li> </ol>"},{"location":"databases/","title":"Databases","text":""},{"location":"databases/mysql-operator/","title":"MySQL Operator","text":"<p>MySQL Version: 8.0.28 mysqlsh version: <code>mysqlsh   Ver 8.0.28 for macos11 on x86_64 - for MySQL 8.0.28 (MySQL Community Server (GPL))</code></p>"},{"location":"databases/mysql-operator/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Install CRDs and operator.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/mysql/mysql-operator/trunk/deploy/deploy-crds.yaml\nkubectl apply -f https://raw.githubusercontent.com/mysql/mysql-operator/trunk/deploy/deploy-operator.yaml\n</code></pre> <pre><code>kubectl get deployments -n mysql-operator\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\nmysql-operator   1/1     1            1           107s\n</code></pre> </li> <li> <p>Create Secret for root user.</p> <pre><code>kubectl create secret generic mypwds \\\n        --from-literal=rootUser=root \\\n        --from-literal=rootHost=% \\\n        --from-literal=rootPassword=password\n</code></pre> </li> <li> <p>Create InnoDBCluster.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/mysql/mysql-operator/trunk/samples/sample-cluster.yaml\n</code></pre> <p>check cluster:</p> <pre><code>kubectl get innodbcluster\n\nNAME        STATUS   ONLINE   INSTANCES   ROUTERS   AGE\nmycluster   ONLINE   3        3           1         2m8s\n</code></pre> <p>Check logs</p> <pre><code>kubectl logs -l name=mysql-operator -n mysql-operator -f\n</code></pre> <p>logs <pre><code>[2022-02-08 21:52:52,983] kopf.objects         [INFO    ] Creation is processed: 1 succeeded; 0 failed.\n[2022-02-08 21:52:53,117] kopf.objects         [INFO    ] POD EVENT : pod=mycluster-2 containers_ready=True deleting=False phase=Running member_info={'memberId': '67632728-8929-11ec-bee4-7e577a1faa95', 'lastTransitionTime': '2022-02-08T21:52:52Z', 'lastProbeTime': '2022-02-08T21:52:52Z', 'groupViewId': '16443571008025241:3', 'status': 'ONLINE', 'version': '8.0.28', 'role': 'SECONDARY', 'joinTime': '2022-02-08T21:52:52Z'} restarts=0 containers=['mysql=ready', 'sidecar=ready'] conditions=['mysql.oracle.com/ready=True', 'mysql.oracle.com/configured=True', 'Initialized=True', 'Ready=False', 'ContainersReady=True', 'PodScheduled=True']\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-0.mycluster-instances.default.svc.cluster.local:3306\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-0.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: Group Replication 'group_name' value: 4c318de6-8929-11ec-95c2-36381ebfe6c7\n2022-02-08 21:52:53: Info: Metadata 'group_name' value: 4c318de6-8929-11ec-95c2-36381ebfe6c7\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-0.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-1.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-2.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n[2022-02-08 21:52:53,261] kopf.objects         [INFO    ] diag instance mycluster-0 --&gt; InstanceDiagStatus.ONLINE quorum=True\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-2.mycluster-instances.default.svc.cluster.local:3306\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-2.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: Group Replication 'group_name' value: 4c318de6-8929-11ec-95c2-36381ebfe6c7\n2022-02-08 21:52:53: Info: Metadata 'group_name' value: 4c318de6-8929-11ec-95c2-36381ebfe6c7\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-0.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-1.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-2.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n[2022-02-08 21:52:53,393] kopf.objects         [INFO    ] diag instance mycluster-2 --&gt; InstanceDiagStatus.ONLINE quorum=True\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-1.mycluster-instances.default.svc.cluster.local:3306\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-1.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: Group Replication 'group_name' value: 4c318de6-8929-11ec-95c2-36381ebfe6c7\n2022-02-08 21:52:53: Info: Metadata 'group_name' value: 4c318de6-8929-11ec-95c2-36381ebfe6c7\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-0.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-1.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n2022-02-08 21:52:53: Info: About to connect to MySQL at: mysql://mysqladmin@mycluster-2.mycluster-instances.default.svc.cluster.local:3306?connect-timeout=5000\n[2022-02-08 21:52:53,509] kopf.objects         [INFO    ] diag instance mycluster-1 --&gt; InstanceDiagStatus.ONLINE quorum=True\n[2022-02-08 21:52:53,510] kopf.objects         [INFO    ] mycluster: all={&lt;MySQLPod mycluster-0&gt;, &lt;MySQLPod mycluster-2&gt;, &lt;MySQLPod mycluster-1&gt;}  members={&lt;MySQLPod mycluster-0&gt;, &lt;MySQLPod mycluster-2&gt;, &lt;MySQLPod mycluster-1&gt;}  online={&lt;MySQLPod mycluster-0&gt;, &lt;MySQLPod mycluster-2&gt;, &lt;MySQLPod mycluster-1&gt;}  offline=set()  unsure=set()\n[2022-02-08 21:52:53,521] kopf.objects         [INFO    ] cluster probe: status=ClusterDiagStatus.ONLINE online=[&lt;MySQLPod mycluster-0&gt;, &lt;MySQLPod mycluster-1&gt;, &lt;MySQLPod mycluster-2&gt;]\n[2022-02-08 21:52:53,524] kopf.objects         [INFO    ] Handler 'on_pod_event' succeeded.\n[2022-02-08 21:52:53,655] kopf.objects         [INFO    ] POD EVENT : pod=mycluster-2 containers_ready=True deleting=False phase=Running member_info={'memberId': '67632728-8929-11ec-bee4-7e577a1faa95', 'lastTransitionTime': '2022-02-08T21:52:52Z', 'lastProbeTime': '2022-02-08T21:52:52Z', 'groupViewId': '16443571008025241:3', 'status': 'ONLINE', 'version': '8.0.28', 'role': 'SECONDARY', 'joinTime': '2022-02-08T21:52:52Z'} restarts=0 containers=['mysql=ready', 'sidecar=ready'] conditions=['mysql.oracle.com/ready=True', 'mysql.oracle.com/configured=True', 'Initialized=True', 'Ready=True', 'ContainersReady=True', 'PodScheduled=True']\n</code></pre> <li> <p>Connect to MySQL InnoDB Cluster.</p> <pre><code>kubectl port-forward service/mycluster mysql\n</code></pre> <p>\u203b If you don't have <code>mysqlsh</code>, you can install from Chapter 2 Installing MySQL Shell.</p> <pre><code>mysqlsh -h127.0.0.1 -P6446 -uroot -p\n</code></pre> <pre><code>\\sql\nshow databases;\n+-------------------------------+\n| Database                      |\n+-------------------------------+\n| information_schema            |\n| mysql                         |\n| mysql_innodb_cluster_metadata |\n| performance_schema            |\n| sys                           |\n+-------------------------------+\n5 rows in set (0.0034 sec)\n</code></pre> </li> <li> <p>Check cluster status</p> <pre><code>mysqlsh -h127.0.0.1 -P6446 -uroot -p --cluster\n</code></pre> <pre><code>MySQL  127.0.0.1:6446 ssl  JS &gt; cluster.status()\n{\n    \"clusterName\": \"mycluster\",\n    \"defaultReplicaSet\": {\n        \"name\": \"default\",\n        \"primary\": \"mycluster-0.mycluster-instances.default.svc.cluster.local:3306\",\n        \"ssl\": \"REQUIRED\",\n        \"status\": \"OK\",\n        \"statusText\": \"Cluster is ONLINE and can tolerate up to ONE failure.\",\n        \"topology\": {\n            \"mycluster-0.mycluster-instances.default.svc.cluster.local:3306\": {\n                \"address\": \"mycluster-0.mycluster-instances.default.svc.cluster.local:3306\",\n                \"memberRole\": \"PRIMARY\",\n                \"memberState\": \"(MISSING)\",\n                \"mode\": \"n/a\",\n                \"readReplicas\": {},\n                \"role\": \"HA\",\n                \"shellConnectError\": \"MySQL Error 2005: Could not open connection to 'mycluster-0.mycluster-instances.default.svc.cluster.local:3306': Unknown MySQL server host 'mycluster-0.mycluster-instances.default.svc.cluster.local' (8)\",\n                \"status\": \"ONLINE\",\n                \"version\": \"8.0.28\"\n            },\n            \"mycluster-1.mycluster-instances.default.svc.cluster.local:3306\": {\n                \"address\": \"mycluster-1.mycluster-instances.default.svc.cluster.local:3306\",\n                \"memberRole\": \"SECONDARY\",\n                \"memberState\": \"(MISSING)\",\n                \"mode\": \"n/a\",\n                \"readReplicas\": {},\n                \"role\": \"HA\",\n                \"shellConnectError\": \"MySQL Error 2005: Could not open connection to 'mycluster-1.mycluster-instances.default.svc.cluster.local:3306': Unknown MySQL server host 'mycluster-1.mycluster-instances.default.svc.cluster.local' (8)\",\n                \"status\": \"ONLINE\",\n                \"version\": \"8.0.28\"\n            },\n            \"mycluster-2.mycluster-instances.default.svc.cluster.local:3306\": {\n                \"address\": \"mycluster-2.mycluster-instances.default.svc.cluster.local:3306\",\n                \"memberRole\": \"SECONDARY\",\n                \"memberState\": \"(MISSING)\",\n                \"mode\": \"n/a\",\n                \"readReplicas\": {},\n                \"role\": \"HA\",\n                \"shellConnectError\": \"MySQL Error 2005: Could not open connection to 'mycluster-2.mycluster-instances.default.svc.cluster.local:3306': Unknown MySQL server host 'mycluster-2.mycluster-instances.default.svc.cluster.local' (8)\",\n                \"status\": \"ONLINE\",\n                \"version\": \"8.0.28\"\n            }\n        },\n        \"topologyMode\": \"Single-Primary\"\n    },\n    \"groupInformationSourceMember\": \"mycluster-0.mycluster-instances.default.svc.cluster.local:3306\"\n}\n</code></pre> </li> <li> <p>Clean up.</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/mysql/mysql-operator/trunk/deploy/deploy-crds.yaml\nkubectl delete -f https://raw.githubusercontent.com/mysql/mysql-operator/trunk/deploy/deploy-operator.yaml\n</code></pre> </li>"},{"location":"databases/mysql-operator/#references","title":"References","text":"<ul> <li>https://speakerdeck.com/oracle4engineer/mysql-technology-cafe-number-13-oracle-mysql-operator-for-kubernetes-purebiyuririsuban-jie-shuo</li> <li>Chapter 2 Installing MySQL Shell</li> <li>3.1 MySQL Shell Commands</li> </ul>"},{"location":"databases/postgres-operator/","title":"Postgres Operator","text":"<p>Version: v1.7.1</p>"},{"location":"databases/postgres-operator/#overview","title":"Overview","text":""},{"location":"databases/postgres-operator/#quickstart","title":"Quickstart","text":"<p>https://github.com/zalando/postgres-operator/blob/master/docs/quickstart.md</p>"},{"location":"databases/postgres-operator/#1-install-postgres-operator","title":"1. Install Postgres Operator","text":"<p>Namespace: <code>default</code></p> <pre><code>kubectl apply -k github.com/zalando/postgres-operator/manifests\n</code></pre> <p>or</p> <pre><code>helm install postgres-operator ./charts/postgres-operator\n</code></pre>"},{"location":"databases/postgres-operator/#2-deploy-the-operator-ui","title":"2. Deploy the Operator UI","text":"<ol> <li> <p>Deploy</p> <pre><code>kubectl apply -k github.com/zalando/postgres-operator/ui/manifests\n</code></pre> <p>or</p> <pre><code>helm install postgres-operator-ui ./charts/postgres-operator-ui\n</code></pre> </li> <li> <p>Check</p> <pre><code>kubectl port-forward svc/postgres-operator-ui 8081:80\n</code></pre> </li> <li> <p>Open http://localhost:8081/</p> <p></p> </li> </ol> <p>Create: - PostgreSQL cluster: <code>acid-minimal-cluster</code> - Secret for roles:     - <code>foo-user.acid-minimal-cluster.credentials.postgresql.acid.zalan.do</code>     - <code>postgres.acid-minimal-cluster.credentials.postgresql.acid.zalan.do</code>     - <code>standby.acid-minimal-cluster.credentials.postgresql.acid.zalan.do</code>     - <code>zalando.acid-minimal-cluster.credentials.postgresql.acid.zalan.do</code></p> <ol> <li> <p>Apply</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/zalando/postgres-operator/master/manifests/minimal-postgres-manifest.yaml\n</code></pre> <p>Roles and Databases initially created: <p>yaml:</p> <pre><code>  users:\n    zalando:  # database owner\n    - superuser\n    - createdb\n    foo_user: []  # role for application foo\n  databases:\n    foo: zalando  # dbname: owner\n  preparedDatabases:\n    bar: {}\n</code></pre> <p>roles:</p> <pre><code>\\du\n                                                     List of roles\n    Role name    |                         Attributes                         |               Member of\n-----------------+------------------------------------------------------------+----------------------------------------\n admin           | Create DB, Cannot login                                    | {foo_user,zalando,bar_owner}\n bar_data_owner  | Cannot login                                               | {bar_data_writer,bar_data_reader}\n bar_data_reader | Cannot login                                               | {}\n bar_data_writer | Cannot login                                               | {bar_data_reader}\n bar_owner       | Cannot login                                               | {bar_writer,bar_data_owner,bar_reader}\n bar_reader      | Cannot login                                               | {}\n bar_writer      | Cannot login                                               | {bar_reader}\n foo_user        |                                                            | {}\n postgres        | Superuser, Create role, Create DB, Replication, Bypass RLS | {}\n robot_zmon      | Cannot login                                               | {}\n standby         | Replication                                                | {}\n zalando         | Superuser, Create DB                                       | {}\n zalandos        | Cannot login                                               | {}\n</code></pre> <p>databases:</p> <pre><code>\\l\n                                  List of databases\n   Name    |   Owner   | Encoding |   Collate   |    Ctype    |   Access privileges\n-----------+-----------+----------+-------------+-------------+-----------------------\n bar       | bar_owner | UTF8     | en_US.utf-8 | en_US.utf-8 |\n foo       | zalando   | UTF8     | en_US.utf-8 | en_US.utf-8 |\n postgres  | postgres  | UTF8     | en_US.utf-8 | en_US.utf-8 |\n template0 | postgres  | UTF8     | en_US.utf-8 | en_US.utf-8 | =c/postgres          +\n           |           |          |             |             | postgres=CTc/postgres\n template1 | postgres  | UTF8     | en_US.utf-8 | en_US.utf-8 | =c/postgres          +\n           |           |          |             |             | postgres=CTc/postgres\n(5 rows)\n</code></pre> <li> <p>Check</p> <pre><code># check the deployed cluster\nkubectl get postgresql\n\n# check created database pods\nkubectl get pods -l application=spilo -L spilo-role\n\n# check created service resources\nkubectl get svc -l application=spilo -L spilo-role\n</code></pre> </li> <li> <p>Connect to Postgres cluster</p> <pre><code>kubectl exec -it acid-minimal-cluster-0 -- psql -Upostgres\npsql (14.0 (Ubuntu 14.0-1.pgdg18.04+1))\nType \"help\" for help.\n\npostgres=#\n</code></pre> </li> <li> <p>Check on UI</p> <p>http://localhost:8081/#/status/default/acid-minimal-cluster</p> <p></p> </li>"},{"location":"databases/postgres-operator/#3-create-a-postgres-cluster","title":"3. Create a Postgres Cluster","text":""},{"location":"databases/postgres-operator/#4-create-a-new-user-john-and-a-new-databas-test_db","title":"4. Create a new user <code>john</code> and a new databas <code>test_db</code>.","text":"<ol> <li> <p>Edit <code>postgresql</code> <code>acid-minimal-cluster</code> either on UI or in yaml.</p> </li> <li> <p><code>Secret</code> is created (<code>{username}.{team}-{clustername}.credentials.postgresql.acid.zalan.do</code>)</p> <pre><code>kubectl get secret john.acid-minimal-cluster.credentials.postgresql.acid.zalan.do\nNAME                                                             TYPE     DATA   AGE\njohn.acid-minimal-cluster.credentials.postgresql.acid.zalan.do   Opaque   2      2m49s\n</code></pre> </li> <li> <p>Log in with <code>john</code>:</p> <pre><code>export PGMASTER=$(kubectl get pods -o jsonpath={.items..metadata.name} -l application=spilo,cluster-name=acid-minimal-cluster,spilo-role=master -n default)\nkubectl port-forward $PGMASTER 6432:5432 -n default\n</code></pre> <pre><code>export PGPASSWORD=$(kubectl get secret john.acid-minimal-cluster.credentials.postgresql.acid.zalan.do -o 'jsonpath={.data.password}' | base64 -d)\nexport PGSSLMODE=require\npsql -U john -h localhost -p 6432 test_db\npsql (14.1, server 14.0 (Ubuntu 14.0-1.pgdg18.04+1))\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\nType \"help\" for help.\n\ntest_db=&gt;\n</code></pre> </li> </ol>"},{"location":"databases/postgres-operator/#4-delete-cluster","title":"4. Delete cluster","text":"<pre><code>kubectl delete -f resources/minimal-postgres-manifest.yaml\n</code></pre>"},{"location":"databases/postgres-operator/#5-remove-operator","title":"5. Remove operator","text":"<pre><code>kubectl apply -k github.com/zalando/postgres-operator/manifests\nkubectl delete -k github.com/zalando/postgres-operator/ui/manifests # ui\n</code></pre>"},{"location":"databases/postgres-operator/#references","title":"References","text":"<ul> <li>https://postgres-operator.readthedocs.io/en/latest/</li> <li>https://recruit.gmo.jp/engineer/jisedai/blog/postgresql12-patroni-cluster/</li> <li>https://postgresconf.org/conferences/2021_Postgres_Conference_Webinars/program/proposals/creating-a-resilient-postgresql-cluster-with-kubegres</li> <li>https://www.youtube.com/watch?v=CftcVhFMGSY</li> <li>https://github.com/zalando/patroni</li> <li>https://github.com/zalando/postgres-operator</li> <li>https://github.com/zalando/spilo</li> </ul>"},{"location":"databases/vitess/","title":"Vitess","text":"<p>Vitess is a database clustering system for horizontal scaling of MySQL through generalized sharding. used by YouTube, Slack, Weave, JD, GitHub, etc.</p> <ul> <li>https://vitess.io/docs/14.0/get-started/operator/</li> <li>https://github.com/planetscale/vitess-operator</li> </ul>"},{"location":"eck/","title":"Elastic Cloud on Kubernetes","text":"<ul> <li>k8s deploy eck</li> <li>deploy elasticsearch</li> <li>Resources</li> </ul>"},{"location":"eck/#install-operator","title":"Install operator","text":"<pre><code>kubectl apply -f https://download.elastic.co/downloads/eck/1.2.0/all-in-one.yaml\ncustomresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created\ncustomresourcedefinition.apiextensions.k8s.io/beats.beat.k8s.elastic.co created\ncustomresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created\ncustomresourcedefinition.apiextensions.k8s.io/enterprisesearches.enterprisesearch.k8s.elastic.co created\ncustomresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created\nnamespace/elastic-system created\nserviceaccount/elastic-operator created\nsecret/elastic-webhook-server-cert created\nclusterrole.rbac.authorization.k8s.io/elastic-operator created\nclusterrole.rbac.authorization.k8s.io/elastic-operator-view created\nclusterrole.rbac.authorization.k8s.io/elastic-operator-edit created\nclusterrolebinding.rbac.authorization.k8s.io/elastic-operator created\nrolebinding.rbac.authorization.k8s.io/elastic-operator created\nservice/elastic-webhook-server created\nstatefulset.apps/elastic-operator created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/elastic-webhook.k8s.elastic.co created\n</code></pre>"},{"location":"eck/#apply-elasticsearch","title":"Apply Elasticsearch","text":"<pre><code>kubectl create ns eck;\nkubectl apply -f eck/elasticsearch.yaml\n</code></pre> <p>Resource requires 2GB -&gt; at least <code>e2-standard-2</code> in GKE</p> <pre><code>kubectl get pod -n eck\nNAME                      READY   STATUS    RESTARTS   AGE\nquickstart-es-default-0   1/1     Running   0          6m20s\n</code></pre> <p>Check connection</p> <pre><code>PASSWORD=$(kubectl get -n eck secret quickstart-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'); echo $PASSWORD\n</code></pre> <pre><code>kubectl -n eck port-forward service/quickstart-es-http 9200\n</code></pre> <pre><code>curl -u \"elastic:$PASSWORD\" -k \"https://localhost:9200\"\n\n{\n  \"name\" : \"quickstart-es-default-0\",\n  \"cluster_name\" : \"quickstart\",\n  \"cluster_uuid\" : \"S5blcUDgQ5u41eBA0TxMVA\",\n  \"version\" : {\n    \"number\" : \"7.8.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"docker\",\n    \"build_hash\" : \"b5ca9c58fb664ca8bf9e4057fc229b3396bf3a89\",\n    \"build_date\" : \"2020-07-21T16:40:44.668009Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"8.5.1\",\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre>"},{"location":"eck/#kibana","title":"Kibana","text":"<pre><code>kubectl apply -f eck/kibana.yaml\n</code></pre>"},{"location":"eck/#install-with-helm-using-this","title":"Install with Helm (Using this)","text":""},{"location":"eck/#elasticsearch","title":"Elasticsearch","text":"<p>https://github.com/elastic/helm-charts/tree/master/elasticsearch</p> <pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> <p>Install with customized values</p> <pre><code>helm show values elastic/elasticsearch &gt; helm/es-config.yaml\nhelm install -n eck elasticsearch elastic/elasticsearch -f helm/es-config.yaml\n</code></pre> <p>Check connection</p> <pre><code>kubectl -n kafka-strimzi-18 exec -it $(kubectl get pod -n kafka-strimzi-18 | grep kafka-connect-sink | awk '{print $1}' | tail -1) -- curl -k \"http://elasticsearch-master-headless.eck:9200\"\n{\n  \"name\" : \"elasticsearch-master-0\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"2Ou_PUP4TCSUoMNHD_rnkA\",\n  \"version\" : {\n    \"number\" : \"7.8.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"docker\",\n    \"build_hash\" : \"b5ca9c58fb664ca8bf9e4057fc229b3396bf3a89\",\n    \"build_date\" : \"2020-07-21T16:40:44.668009Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"8.5.1\",\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre> <p>Upgrade</p> <pre><code>helm upgrade elasticsearch elastic/elasticsearch -n eck -f helm/es-config.yaml\n</code></pre>"},{"location":"eck/#kibana_1","title":"Kibana","text":"<p>https://github.com/elastic/helm-charts/tree/master/kibana</p> <pre><code>helm install kibana elastic/kibana\n</code></pre> <pre><code>helm show values elastic/kibana &gt; helm/kb-config.yaml\nhelm install -n eck kibana elastic/kibana -f helm/kb-config.yaml\n</code></pre> <pre><code>kubectl -n eck port-forward service/kibana-kibana 5601\n</code></pre> <p></p>"},{"location":"eck/#filebeat","title":"Filebeat","text":"<p>https://hub.helm.sh/charts/elastic/filebeat</p> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm show values elastic/filebeat --version 7.8.1 &gt; helm/filebeat-config.yaml\nhelm install -n eck filebeat elastic/filebeat --version 7.8.1 -f helm/filebeat-config.yaml\n</code></pre> <p></p>"},{"location":"eksworkshop/","title":"EKS Workshop","text":"<p>https://eksworkshop.com/</p>"},{"location":"eksworkshop/#praparation","title":"Praparation","text":""},{"location":"eksworkshop/#eksctl","title":"eksctl","text":"<pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n\nsudo mv -v /tmp/eksctl /usr/local/bin\n</code></pre> <pre><code>eksctl version\n0.19.0\n</code></pre>"},{"location":"eksworkshop/#kms","title":"kms","text":"<p>create</p> <pre><code>aws kms create-alias --alias-name alias/eksworkshop --target-key-id $(aws kms create-key --query KeyMetadata.Arn --output text)\n</code></pre> <pre><code>export MASTER_ARN=$(aws kms describe-key --key-id alias/eksworkshop --query KeyMetadata.Arn --output text)\n\n\n[20-05-15 2:38:15] nakamasato at Masatos-MacBook-Pro in ~/Code/MasatoNaka/kubernetes-training/eksworkshop on master \u2718\n\u00b1 echo $MASTER_ARN\narn:aws:kms:ap-northeast-1:135493629466:key/3b5de2cf-b5b9-4d78-8786-b4eb52948204\n</code></pre> <pre><code>export AWS_REGION=ap-northeast-1\n</code></pre>"},{"location":"eksworkshop/#create-eks-cluster","title":"Create EKS cluster","text":"<pre><code>cat &lt;&lt; EOF &gt; eksworkshop.yaml\n---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: eksworkshop-eksctl\n  region: ${AWS_REGION}\n\nmanagedNodeGroups:\n- name: nodegroup\n  desiredCapacity: 3\n  iam:\n    withAddonPolicies:\n      albIngress: true\n\nsecretsEncryption:\n  keyARN: ${MASTER_ARN}\nEOF\n</code></pre> <pre><code>eksctl create cluster -f eksworkshop.yaml\n[\u2139]  eksctl version 0.19.0\n[\u2139]  using region ap-northeast-1\n[\u2139]  setting availability zones to [ap-northeast-1d ap-northeast-1a ap-northeast-1c]\n[\u2139]  subnets for ap-northeast-1d - public:192.168.0.0/19 private:192.168.96.0/19\n[\u2139]  subnets for ap-northeast-1a - public:192.168.32.0/19 private:192.168.128.0/19\n[\u2139]  subnets for ap-northeast-1c - public:192.168.64.0/19 private:192.168.160.0/19\n[\u2139]  using Kubernetes version 1.15\n[\u2139]  creating EKS cluster \"eksworkshop-eksctl\" in \"ap-northeast-1\" region with managed nodes\n[\u2139]  1 nodegroup (nodegroup) was included (based on the include/exclude rules)\n[\u2139]  will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s)\n[\u2139]  will create a CloudFormation stack for cluster itself and 1 managed nodegroup stack(s)\n[\u2139]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=eksworkshop-eksctl'\n[\u2139]  CloudWatch logging will not be enabled for cluster \"eksworkshop-eksctl\" in \"ap-northeast-1\"\n[\u2139]  you can enable it with 'eksctl utils update-cluster-logging --region=ap-northeast-1 --cluster=eksworkshop-eksctl'\n[\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"eksworkshop-eksctl\" in \"ap-northeast-1\"\n[\u2139]  2 sequential tasks: { create cluster control plane \"eksworkshop-eksctl\", create managed nodegroup \"nodegroup\" }\n[\u2139]  building cluster stack \"eksctl-eksworkshop-eksctl-cluster\"\n[\u2139]  deploying stack \"eksctl-eksworkshop-eksctl-cluster\"\n[\u2139]  building managed nodegroup stack \"eksctl-eksworkshop-eksctl-nodegroup-nodegroup\"\n[\u2139]  deploying stack \"eksctl-eksworkshop-eksctl-nodegroup-nodegroup\"\n[\u2139]  waiting for the control plane availability...\n[\u2714]  saved kubeconfig as \"/Users/nakamasato/.kube/config\"\n[\u2139]  no tasks\n[\u2714]  all EKS cluster resources for \"eksworkshop-eksctl\" have been created\n[\u2139]  nodegroup \"nodegroup\" has 3 node(s)\n[\u2139]  node \"ip-192-168-21-173.ap-northeast-1.compute.internal\" is ready\n[\u2139]  node \"ip-192-168-60-193.ap-northeast-1.compute.internal\" is ready\n[\u2139]  node \"ip-192-168-75-58.ap-northeast-1.compute.internal\" is ready\n[\u2139]  waiting for at least 3 node(s) to become ready in \"nodegroup\"\n[\u2139]  nodegroup \"nodegroup\" has 3 node(s)\n[\u2139]  node \"ip-192-168-21-173.ap-northeast-1.compute.internal\" is ready\n[\u2139]  node \"ip-192-168-60-193.ap-northeast-1.compute.internal\" is ready\n[\u2139]  node \"ip-192-168-75-58.ap-northeast-1.compute.internal\" is ready\n[\u2139]  kubectl command should work with \"/Users/nakamasato/.kube/config\", try 'kubectl get nodes'\n[\u2714]  EKS cluster \"eksworkshop-eksctl\" in \"ap-northeast-1\" region is ready\n</code></pre> <p>check</p> <pre><code>kubectl get nodes # if we see our 3 nodes, we know we have authenticated correctly\nNAME                                                STATUS   ROLES    AGE   VERSION\nip-192-168-21-173.ap-northeast-1.compute.internal   Ready    &lt;none&gt;   15m   v1.15.11-eks-af3caf\nip-192-168-60-193.ap-northeast-1.compute.internal   Ready    &lt;none&gt;   15m   v1.15.11-eks-af3caf\nip-192-168-75-58.ap-northeast-1.compute.internal    Ready    &lt;none&gt;   14m   v1.15.11-eks-af3caf\n</code></pre>"},{"location":"eksworkshop/#deploy","title":"Deploy","text":""},{"location":"eksworkshop/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre> <pre><code>kubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true &amp;\n</code></pre> <p>open: http://localhost:8080/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/overview?namespace=default</p> <p>Get token and insert it</p> <pre><code>aws eks get-token --cluster-name eksworkshop-eksctl | jq -r '.status.token'\n</code></pre>"},{"location":"eksworkshop/#example-microservice","title":"Example Microservice","text":"<ul> <li>Backend</li> <li>Frontend</li> </ul>"},{"location":"eksworkshop/#helm","title":"Helm","text":""},{"location":"eksworkshop/#clean-up","title":"Clean-up","text":""},{"location":"eksworkshop/#eks","title":"eks","text":"<pre><code>eksctl delete cluster -f eksworkshop.yaml\n</code></pre>"},{"location":"eksworkshop/#kms_1","title":"kms","text":"<p>delete</p> <pre><code>eksctl delete cluster -f eksworkshop.yaml\n</code></pre>"},{"location":"eksworkshop/intermediate/velero/","title":"Velero","text":"<ul> <li>https://eksworkshop.com/intermediate/280_backup-and-restore/</li> <li>https://velero.io/docs/v1.4/</li> </ul>"},{"location":"eksworkshop/intermediate/velero/#preparation","title":"Preparation","text":"<ul> <li>s3 bucket naka-kubernetes-backup</li> <li>iam user, policy and policy attachment</li> </ul> <pre><code>aws iam create-access-key --user-name velero &gt; velero-access-key.json\n</code></pre> <pre><code>export VELERO_ACCESS_KEY_ID=$(cat velero-access-key.json | jq -r '.AccessKey.AccessKeyId')\nexport VELERO_SECRET_ACCESS_KEY=$(cat velero-access-key.json | jq -r '.AccessKey.SecretAccessKey')\n</code></pre> <pre><code>cat &gt; velero-credentials &lt;&lt;EOF\n[default]\naws_access_key_id=$VELERO_ACCESS_KEY_ID\naws_secret_access_key=$VELERO_SECRET_ACCESS_KEY\nEOF\n</code></pre>"},{"location":"eksworkshop/intermediate/velero/#usage","title":"Usage","text":""},{"location":"eksworkshop/intermediate/velero/#install","title":"Install","text":"<p>Mac: https://velero.io/docs/master/basic-install/</p> <pre><code>brew install velero\n</code></pre> <pre><code>velero version\nClient:\n        Version: v1.4.0\n        Git commit: -\n&lt;error getting server version: the server could not find the requested resource (post serverstatusrequests.velero.io)&gt;\n</code></pre> <pre><code>VELERO_BUCKET=naka-kubernetes-backup\nAWS_REGION=ap-northeast-1\nvelero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.0.1 \\\n    --bucket $VELERO_BUCKET \\\n    --backup-location-config region=$AWS_REGION \\\n    --snapshot-location-config region=$AWS_REGION \\\n    --secret-file ./velero-credentials\nCustomResourceDefinition/backups.velero.io: attempting to create resource\nCustomResourceDefinition/backups.velero.io: created\nCustomResourceDefinition/backupstoragelocations.velero.io: attempting to create resource\nCustomResourceDefinition/backupstoragelocations.velero.io: created\nCustomResourceDefinition/deletebackuprequests.velero.io: attempting to create resource\nCustomResourceDefinition/deletebackuprequests.velero.io: created\nCustomResourceDefinition/downloadrequests.velero.io: attempting to create resource\nCustomResourceDefinition/downloadrequests.velero.io: created\nCustomResourceDefinition/podvolumebackups.velero.io: attempting to create resource\nCustomResourceDefinition/podvolumebackups.velero.io: created\nCustomResourceDefinition/podvolumerestores.velero.io: attempting to create resource\nCustomResourceDefinition/podvolumerestores.velero.io: created\nCustomResourceDefinition/resticrepositories.velero.io: attempting to create resource\nCustomResourceDefinition/resticrepositories.velero.io: created\nCustomResourceDefinition/restores.velero.io: attempting to create resource\nCustomResourceDefinition/restores.velero.io: created\nCustomResourceDefinition/schedules.velero.io: attempting to create resource\nCustomResourceDefinition/schedules.velero.io: created\nCustomResourceDefinition/serverstatusrequests.velero.io: attempting to create resource\nCustomResourceDefinition/serverstatusrequests.velero.io: created\nCustomResourceDefinition/volumesnapshotlocations.velero.io: attempting to create resource\nCustomResourceDefinition/volumesnapshotlocations.velero.io: created\nWaiting for resources to be ready in cluster...\nNamespace/velero: attempting to create resource\nNamespace/velero: created\nClusterRoleBinding/velero: attempting to create resource\nClusterRoleBinding/velero: created\nServiceAccount/velero: attempting to create resource\nServiceAccount/velero: created\nSecret/cloud-credentials: attempting to create resource\nSecret/cloud-credentials: created\nBackupStorageLocation/default: attempting to create resource\nBackupStorageLocation/default: created\nVolumeSnapshotLocation/default: attempting to create resource\nVolumeSnapshotLocation/default: created\nDeployment/velero: attempting to create resource\nDeployment/velero: created\nVelero is installed! \u26f5 Use 'kubectl logs deployment/velero -n velero' to view the status.\n</code></pre> <p>Check resources on kuberentes</p> <pre><code>kubectl get all -n velero\n\nNAME                          READY   STATUS    RESTARTS   AGE\npod/velero-5b596f6b56-4kvhx   1/1     Running   0          43s\n\n\n\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/velero   1/1     1            1           44s\n\nNAME                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/velero-5b596f6b56   1         1         1       44s\n</code></pre>"},{"location":"eksworkshop/intermediate/velero/#backup","title":"backup","text":"<p><code>test-ns</code> as an example in <code>k8s-deploy-test</code> repos</p> <pre><code>kubectl get all -n test-ns\n\nNAME                                READY   STATUS    RESTARTS   AGE\npod/guestbook-ui-8569df798c-plf7b   1/1     Running   0          86m\npod/guestbook-ui-8569df798c-rc4fb   1/1     Running   0          86m\npod/guestbook-ui-8569df798c-twgq7   1/1     Running   0          86m\n\n\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/guestbook-ui   ClusterIP   10.100.89.118   &lt;none&gt;        80/TCP    176m\n\n\nNAME                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/guestbook-ui   3/3     3            3           176m\n\nNAME                                      DESIRED   CURRENT   READY   AGE\nreplicaset.apps/guestbook-ui-8569df798c   3         3         3       176m\n</code></pre> <p>Backup</p> <pre><code>velero backup create test-ns-backup --include-namespaces test-ns\nBackup request \"test-ns-backup\" submitted successfully.\nRun `velero backup describe test-ns-backup` or `velero backup logs test-ns-backup` for more details.\n</code></pre> <p>Check</p> <pre><code>velero backup describe test-ns-backup\nName:         test-ns-backup\nNamespace:    velero\nLabels:       velero.io/storage-location=default\nAnnotations:  velero.io/source-cluster-k8s-gitversion=v1.15.11-eks-af3caf\n              velero.io/source-cluster-k8s-major-version=1\n              velero.io/source-cluster-k8s-minor-version=15+\n\nPhase:  Completed\n\nNamespaces:\n  Included:  test-ns\n  Excluded:  &lt;none&gt;\n\nResources:\n  Included:        *\n  Excluded:        &lt;none&gt;\n  Cluster-scoped:  auto\n\nLabel selector:  &lt;none&gt;\n\nStorage Location:  default\n\nVelero-Native Snapshot PVs:  auto\n\nTTL:  720h0m0s\n\nHooks:  &lt;none&gt;\n\nBackup Format Version:  1\n\nStarted:    2020-06-07 15:38:49 +0900 JST\nCompleted:  2020-06-07 15:38:50 +0900 JST\n\nExpiration:  2020-07-07 15:38:49 +0900 JST\n\nTotal items to be backed up:  32\nItems backed up:              32\n\nVelero-Native Snapshots: &lt;none included&gt;\n</code></pre>"},{"location":"eksworkshop/intermediate/velero/#delete-all-the-resources","title":"Delete all the resources","text":"<pre><code>kubectl delete namespace test-ns\n</code></pre>"},{"location":"eksworkshop/intermediate/velero/#restore","title":"Restore","text":"<pre><code>velero restore create --from-backup test-ns-backup\nRestore request \"test-ns-backup-20200607154244\" submitted successfully.\nRun `velero restore describe test-ns-backup-20200607154244` or `velero restore logs test-ns-backup-20200607154244` for more details.\n</code></pre> <pre><code>velero restore get\n\nNAME                            BACKUP           STATUS      WARNINGS   ERRORS   CREATED                         SELECTOR\ntest-ns-backup-20200607154244   test-ns-backup   Completed   0          0        2020-06-07 15:42:45 +0900 JST   &lt;none&gt;\n</code></pre>"},{"location":"eksworkshop/intermediate/velero/#example-argocd-namespace","title":"Example (Argocd namespace)","text":"<ol> <li> <p>Resources to back up</p> <p></p> </li> <li> <p>Back up     <pre><code>velero backup create argocd-backup --include-namespaces argocd\nBackup request \"argocd-backup\" submitted successfully.\nRun `velero backup describe argocd-backup` or `velero backup logs argocd-backup` for more details.\n</code></pre></p> </li> <li> <p>Delete <code>argocd</code> namespace</p> <pre><code>kubectl delete namespace argocd\nnamespace \"argocd\" deleted\n</code></pre> <p>-&gt; failed to delete completely</p> </li> <li> <p>Restore</p> <pre><code>velero restore create --from-backup argocd-backup\nRestore request \"argocd-backup-20200607155736\" submitted successfully.\nRun `velero restore describe argocd-backup-20200607155736` or `velero restore logs argocd-backup-20200607155736` for more details.\n</code></pre> <p>Couldn't recover ...</p> </li> </ol>"},{"location":"flux/","title":"Flux (WIP)","text":""},{"location":"flux/#installation","title":"Installation","text":"<pre><code>brew install fluxcd/tap/flux\n</code></pre>"},{"location":"grafana/","title":"Grafana","text":"<p>https://github.com/grafana/grafana</p>"},{"location":"grafana/#version","title":"Version","text":"<p>Latest</p>"},{"location":"grafana/#install","title":"Install","text":"<pre><code>kubectl create ns monitoring\nkubectl apply -k .\n</code></pre>"},{"location":"grafana/#dashboards","title":"Dashboards","text":"<p>Pre-configured dashboards are in <code>config/dashboards/dashboard.yaml</code></p> <ul> <li>RabbitMQ: config/dashboards/rabbitmq.json</li> <li>MySQL: config/dashboards/mysql-test.json</li> </ul>"},{"location":"grafana/#data-sources","title":"Data sources","text":"<p>Configured data sources are in <code>config/datasources/datasource.yaml</code></p>"},{"location":"grafana/#prometheus","title":"Prometheus","text":"<ol> <li>Apply Prometheus with prometheus-operator following prometheus-operator</li> <li>Data source is already set to <code>http://prometheus.monitoring.svc:9090</code> on Grafana.</li> </ol>"},{"location":"grafana/#mysql","title":"MySQL","text":"<ol> <li> <p>Create mysql in <code>database</code> namespace     <pre><code>kubectl create ns database\nkubectl apply -k datasources/mysql\n</code></pre></p> </li> <li> <p>The following data in <code>stat</code> table is initialized in entrypoint of MySQL.</p> <pre><code>mysql&gt; select * from stat;\n+-------+---------------------+---------------------+\n| num   | started_at          | updated_at          |\n+-------+---------------------+---------------------+\n| 10000 | 2021-09-01 00:00:00 | 2021-09-01 00:00:20 |\n| 10000 | 2021-09-01 00:00:20 | 2021-09-01 00:00:30 |\n|  5000 | 2021-09-02 00:00:00 | 2021-09-02 00:00:10 |\n|   100 | 2021-09-02 00:00:00 | 2021-09-02 00:00:20 |\n+-------+---------------------+---------------------+\n4 rows in set (0.00 sec)\n</code></pre> </li> <li> <p>Data source for MySQL is configured:</p> <ul> <li>host: <code>mysql.default.svc.cluster.local</code></li> <li>user: <code>grafana</code></li> <li>password: <code>password</code></li> </ul> </li> <li> <p>Dashboard with MySQL data source.</p> <pre><code>SELECT sum(num) / sum(TIME_TO_SEC(TIMEDIFF(updated_at, started_at))) * 86400 AS \"num\" FROM stat\nWHERE started_at &gt; NOW() - INTERVAL 1 MONTH\n</code></pre> <p></p> </li> </ol>"},{"location":"grafana/#cleanup","title":"CleanUp","text":"<p>Delete MySQL for data source:</p> <pre><code>kubectl delete -k datasources/mysql\nkubectl delete ns database\n</code></pre> <p>Delete Grafana:</p> <pre><code>kubectl delete -k .\nkubectl delete ns monitoring\n</code></pre>"},{"location":"grafana/#related","title":"Related","text":"<p>You may also like Loki</p>"},{"location":"grafana-operator/","title":"Grafana Operator (WIP)","text":""},{"location":"grafana-operator/#1-install-operator","title":"1. Install operator","text":"<pre><code>kubectl apply -k github.com/grafana-operator/grafana-operator/deploy/manifests/\n</code></pre>"},{"location":"grafana-operator/#3-create-grafana","title":"3. Create Grafana","text":"<p>Be sure to deploy in the same namespace as the operator (<code>grafana-operator-system</code>).</p> <ol> <li> <p>Create Grafana</p> <p>Option 1 (simple one):</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/grafana-operator/grafana-operator/master/deploy/examples/Grafana.yaml -n grafana-operator-system\n</code></pre> <p>Option 2 (HA with Postgres for session storage)</p> <pre><code>kubectl apply -k ha -n grafana-operator-system\n</code></pre> </li> <li> <p>Check status.     <pre><code>kubectl get grafana example-grafana -n grafana-operator-system -o jsonpath='{.status}'\n{\"message\":\"success\",\"phase\":\"reconciling\",\"previousServiceName\":\"grafana-service\"}\n</code></pre></p> </li> <li> <p>port forward.</p> <pre><code>kubectl port-forward -n grafana-operator-system svc/grafana-service 3000\n</code></pre> </li> <li> <p>Access to UI.</p> <p>http://localhost:3000</p> </li> <li> <p>Log in with <code>admin</code>.</p> <p>Get password.</p> <pre><code>kubectl get secret -n grafana-operator-system grafana-admin-credentials -o jsonpath='{.data.GF_SECURITY_ADMIN_PASSWORD}' | base64 -D\n9GD2tIHo-GrgTQ==%\n\nkubectl get secret -n grafana-operator-system grafana-admin-credentials -o jsonpath='{.data.GF_SECURITY_ADMIN_USER}' | base64 --decode\nadmin%\n</code></pre> </li> </ol>"},{"location":"grafana-operator/#2-datasource","title":"2. Datasource","text":""},{"location":"grafana-operator/#21-prometheus-datasource","title":"2.1. Prometheus Datasource","text":"<ol> <li> <p>Deploy prometheus with Prometheus Operator. Prometheus Datasource expects <code>prometheus</code> service with port 9090.</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml\nkubectl apply -k ../prometheus-operator\n</code></pre> </li> <li> <p>Create Prometheus Datasource     <pre><code>kubectl apply -f datasource-prometheus.yaml -n grafana-operator-system\n</code></pre></p> </li> </ol>"},{"location":"grafana-operator/#3-dashboard","title":"3. Dashboard","text":"<p>simple-dashboard</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/grafana-operator/grafana-operator/master/deploy/examples/dashboards/SimpleDashboard.yaml -n grafana-operator-system\n</code></pre> <p>keycloak-dashboard (data is empty)</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/grafana-operator/grafana-operator/master/deploy/examples/dashboards/KeycloakDashboard.yaml -n grafana-operator-system\n</code></pre> <p>dashboard from grafana (need node exporter)</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/grafana-operator/grafana-operator/7754cd15386ff6da1e3e7b820f8baf53e6dd9356/deploy/examples/dashboards/DashboardFromGrafana.yaml -n grafana-operator-system\n</code></pre>"},{"location":"grafana-operator/#4-cleanup","title":"4. Cleanup","text":"<pre><code>kubectl delete --all grafana,grafanadashboard,grafanadatasource -n grafana-operator-system\nkubectl delete -k github.com/grafana-operator/grafana-operator/deploy/manifests/\n</code></pre>"},{"location":"grafana-operator/#debug","title":"Debug","text":"<ol> <li>Log: <code>kubectl logs -l control-plane=controller-manager -c manager -n grafana-operator-system -f</code></li> </ol>"},{"location":"helm/","title":"Helm","text":""},{"location":"helm/#version","title":"Version","text":"<p>v3.11.2</p>"},{"location":"helm/#install-helm","title":"Install helm","text":"<p>https://helm.sh/docs/intro/install/</p> <pre><code>brew install helm\n</code></pre> <pre><code>helm version --short\nv3.11.2+g912ebc1\n</code></pre>"},{"location":"helm/#update-helm-version","title":"Update helm version","text":"<pre><code>brew upgrade helm\n</code></pre>"},{"location":"helm/#usage","title":"Usage","text":""},{"location":"helm/#1-install","title":"1. Install","text":"<pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> <pre><code>helm install elasticsearch elastic/elasticsearch\nNAME: elasticsearch\nLAST DEPLOYED: Sat Aug  8 17:23:21 2020\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Watch all cluster members come up.\n  $ kubectl get pods --namespace=default -l app=elasticsearch-master -w\n2. Test cluster health using Helm test.\n  $ helm test elasticsearch --cleanup\n</code></pre>"},{"location":"helm/#2-customize","title":"2. Customize","text":"<pre><code>helm show values elastic/elasticsearch &gt; helm/es-config.yaml\nhelm install -n eck elasticsearch elastic/elasticsearch -f helm/es-config.yaml\n</code></pre>"},{"location":"helm/#3-upgrade","title":"3. Upgrade","text":"<pre><code>helm upgrade elasticsearch elastic/elasticsearch -n eck -f helm/es-config.yaml\n</code></pre>"},{"location":"helm/#4-uninstall","title":"4. Uninstall","text":"<pre><code>helm uninstall elasticsearch elastic/elasticsearch\n</code></pre>"},{"location":"helm/#5-create-and-publish-helm-chart","title":"5. Create and publish Helm chart","text":"<p>Prerequisite: create a Helm chart repo (e.g. https://github.com/nakamasato/helm-charts/tree/gh-pages)</p> <ol> <li>Create     There are two options:<ol> <li>Create with <code>helm</code> command: <code>helm create &lt;chart_name&gt;</code></li> <li>Create from exiting yaml files (e.g. yaml generated by operator-sdk or kubebuilder) with <code>helmify</code>: <code>kustomize build | helmify &lt;chart_name&gt;</code></li> </ol> </li> <li>Publish     There are two options:<ol> <li>Push Helm package and index.yaml</li> <li>Push Helm source code, and package it and update index.yaml by GitHub Actions</li> </ol> </li> </ol>"},{"location":"helm/#helm-basic-commands","title":"Helm basic commands","text":"<ul> <li><code>helm ls</code>: Check releases.</li> <li> <p><code>helm template &lt;chart_path or chart&gt;</code>: Test rendering the chart locally (You can check the resulting yaml without applying)</p> <p>You can pass values via 1. flag: <code>helm template chart --set .cloudSecretManagerType=gcp</code> 1. value file: <code>helm template chart -f gcp-helm-values.yaml</code></p> </li> <li> <p><code>helm install &lt;NAME&gt;</code>: Deploy a chart. (Deploy packaged resources to the cluster.)</p> <p>There are five different ways you can express the chart you want to install:</p> <ol> <li>By chart reference: <code>helm install mymaria example/mariadb</code></li> <li>By path to a packaged chart: <code>helm install mynginx ./nginx-1.2.3.tgz</code></li> <li>By path to an unpacked chart directory: <code>helm install mynginx ./nginx</code></li> <li>By absolute URL: helm install mynginx https://example.com/charts/nginx-1.2.3.tgz</li> <li>By chart reference and repo url: <code>helm install --repo https://example.com/charts/ mynginx nginx</code></li> </ol> <p>You can pass values in the same way as <code>helm template</code></p> </li> <li> <p><code>helm upgrade &lt;RELEASE_NAME&gt; &lt;chart&gt;</code></p> <p>Example: <pre><code>helm upgrade mysql-operator-0-1680913123 $HELM_PATH --set cloudSecretManagerType=gcp --set gcpProjectId=$PROJECT_ID\n</code></pre></p> </li> <li> <p><code>helm uninstall &lt;RELEASE_NAME&gt;</code>: Remove a chart. (Remove packaged resources from the cluster.)</p> </li> <li><code>helm status &lt;RELEASE_NAME&gt;</code></li> </ul>"},{"location":"helm/#reference","title":"Reference","text":"<ul> <li>Quick Start</li> <li>helm (github)</li> </ul>"},{"location":"helm/hello-world/","title":"Helm hello world","text":"<ol> <li> <p>Create helm.</p> <pre><code>helm create helloworld-chart\n</code></pre> <p>By default, - <code>Deployment</code>, <code>HPA</code>, <code>Ingress</code>, <code>Service</code> and <code>ServiceAccount</code> are generated under <code>templates</code>. - <code>values.yaml</code> has <code>replicaCount</code>, <code>image</code>, and other configuration fields for the resources.</p> </li> <li> <p>(Optional) Change <code>values.yaml</code> (you can change image name, tag or any fields)</p> </li> <li> <p>Run <code>helm package</code> command and it'll generate <code>helloworld-chart-0.1.0.tgz</code>.</p> <pre><code>helm package helloworld-chart\nSuccessfully packaged chart and saved it to: /Users/masato-naka/repos/nakamasato/kubernetes-training/helm/hello-world/helloworld-chart-0.1.0.tgz\n</code></pre> </li> <li> <p>Apply helm <code>hello-world</code>.</p> <pre><code>helm install helloworld-chart-0.1.0.tgz --generate-name\nNAME: helloworld-chart-0-1589247182\nLAST DEPLOYED: Tue May 12 10:33:03 2020\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=helloworld-chart,app.kubernetes.io/instance=helloworld-chart-0-1589247182\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace default port-forward $POD_NAME 8080:80\n</code></pre> </li> <li> <p>Check <code>helm list</code> (status should be <code>deployed</code>).</p> <pre><code>helm list\nNAME                            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                        APP VERSION\nhelloworld-chart-0-1621414497   default         1               2021-05-19 17:54:59.111853 +0900 JST    deployed        helloworld-chart-0.1.0       1.16.0\n</code></pre> </li> <li> <p>Check the created resources.</p> <ol> <li> <p><code>Deployment</code></p> <pre><code>kubectl get deployment -n default\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\nhelloworld-chart-0-1589247182   0/1     1            0           16s\n</code></pre> </li> <li> <p><code>Service</code></p> <pre><code>kubectl get svc\nNAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nhelloworld-chart-0-1621414497   ClusterIP   10.97.169.140   &lt;none&gt;        80/TCP    7m58s\nkubernetes                      ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP   12d\n</code></pre> </li> <li> <p><code>ServiceAccount</code></p> <pre><code>kubectl get sa\nNAME                            SECRETS   AGE\ndefault                         1         12d\nhelloworld-chart-0-1621414497   1         8m59s\n</code></pre> </li> <li> <p><code>Ingress</code> and <code>HPA</code> are not deployed because of <code>enable: false</code> in <code>values.yaml</code>.</p> </li> </ol> </li> <li> <p>Uninstall the chart.</p> <pre><code>helm uninstall helloworld-chart-0-1589247182\nrelease \"helloworld-chart-0-1589247182\" uninstalled\n</code></pre> </li> </ol>"},{"location":"helm/hello-world/#reference","title":"Reference","text":"<ul> <li>https://artifacthub.io/packages/search?kind=0</li> <li>https://helm.sh/docs/intro/quickstart/</li> <li>https://github.com/helm/helm</li> </ul>"},{"location":"helm-vs-kustomize/","title":"Helm vs Kustomize","text":""},{"location":"helm-vs-kustomize/#helm","title":"Helm","text":""},{"location":"helm-vs-kustomize/#steps-to-create-a-chart","title":"Steps to create a chart","text":"<ol> <li> <p>Create directory for practice <code>helm-example</code> and move into the new directory.</p> <pre><code>mkdir helm-example &amp;&amp; cd helm-example\n</code></pre> </li> <li> <p>Create helm. -&gt; would generate a folder <code>helm-example</code> and files in it.</p> <pre><code>helm create helm-example\n</code></pre> </li> <li> <p>Go into the directory <code>helm-example</code> (a directory for chart) and check the generated files.</p> <pre><code>cd helm-example\n</code></pre> <pre><code>tree\n.\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 NOTES.txt\n\u2502   \u251c\u2500\u2500 _helpers.tpl\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 hpa.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u2514\u2500\u2500 tests\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n\n3 directories, 10 files\n</code></pre> </li> <li> <p>Update templates to meet the requirements.</p> <ol> <li>Remove unnecessary templates.</li> </ol> <pre><code>rm templates/hpa.yaml templates/ingress.yaml templates/serviceaccount.yaml\n</code></pre> <ol> <li> <p>Remove unnecessary values from values.yaml.</p> </li> <li> <p>Write your templates.</p> </li> <li> <p>You can use the following built-in objects.</p> <ul> <li>Built-in Objects<ul> <li><code>Release</code>: This object describes the release itself.<ul> <li><code>Release.Name</code></li> <li><code>Release.Namespace</code> ...</li> </ul> </li> <li><code>Values</code>: Values passed into the template from the values.yaml file.</li> <li><code>Chart</code>: The contents of the Chart.yaml file.<ul> <li><code>Chart.Name</code></li> <li><code>Chart.Version</code></li> </ul> </li> <li>Others: <code>Files</code>, <code>Capabilities</code>, <code>Template</code></li> </ul> </li> </ul> </li> <li> <p><code>Values</code>: write <code>values.yaml</code> and pass them into template yaml with <code>{{ Values.xxx.yyy }}</code></p> </li> <li>Template functions: <code>{{ quote .Values.favorite.drink }}</code> or pipelines: <code>{{ .Values.favorite.drink | quote }}</code></li> </ol> </li> <li> <p>Check with <code>--dry-run</code>.</p> <pre><code>cd ..\nhelm install helm-example --debug --dry-run ./helm-example\n</code></pre> </li> <li> <p>Lint</p> <pre><code>helm lint helm-example\n</code></pre> </li> <li> <p>Install.</p> <pre><code>helm install helm-example --debug ./helm-example\n</code></pre> </li> <li> <p>Check <code>helm</code></p> <pre><code>helm ls\nNAME            NAMESPACE       REVISION        UPDATED                                 STATUS   CHART                    APP VERSION\nhelm-example    default         1               2021-05-24 07:22:45.809015 +0900 JST    deployed helm-example-0.1.0       1.16.0\n</code></pre> </li> <li> <p>Check pod</p> <p><pre><code>kubectl get po\nNAME                            READY   STATUS    RESTARTS   AGE\nhelm-example-5d796c89c7-v4pvw   2/2     Running   0          80m\n</code></pre> 1. Test</p> <pre><code>helm test helm-example\n</code></pre> </li> <li> <p>Package a chart.</p> <pre><code>helm package helm-example\n</code></pre> </li> <li> <p>Publish the chart. (Create a Github repo for Helm chart repository. https://github.com/nakamasato/helm-charts)</p> <ol> <li> <p>Options1: push package and index to the chart repo.</p> <pre><code>helm repo index ./ --url https://nakamasato.github.io/helm-charts\n</code></pre> <p>This would generate <code>index.yaml</code>. Push the <code>index.yaml</code> and <code>helm-example-0.1.0.tgz</code> to the chart repo.</p> </li> <li> <p>Option2: push chart source code + actions/helm-chart-releaser (ref: https://helm.sh/docs/howto/chart_releaser_action/) &lt;- nakamasato/helm-charts uses this.</p> </li> </ol> </li> <li> <p>Add the repo that is created above.</p> <pre><code>helm repo add nakamasato https://nakamasato.github.io/helm-charts\nhelm repo update # update the repository info\n</code></pre> <p>Search for your chart.</p> <pre><code>helm search repo naka\nNAME                    CHART VERSION   APP VERSION     DESCRIPTION\nnakamasato/helm-example 0.1.0           v0.0.1          Simple API application.\n</code></pre> </li> <li> <p>Install your helm chart.</p> <pre><code>helm install example-from-my-repo nakamasato/helm-example\nNAME: example-from-my-repo\nLAST DEPLOYED: Tue May 25 09:07:24 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get the application URL by running these commands:\nexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=helm-example,app.kubernetes.io/instance=example-from-my-repo\" -o jsonpath=\"{.items[0].metadata.name}\")\nexport CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\necho \"Visit http://127.0.0.1:8080 to use your application\"\nkubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT\n</code></pre> </li> </ol>"},{"location":"helm-vs-kustomize/#kustomize","title":"Kustomize","text":""},{"location":"helm-vs-kustomize/#steps-to-create-yaml-for-multiple-envs","title":"Steps to create yaml for multiple envs","text":"<ol> <li> <p>Make a directory for two envs (<code>dev</code>, <code>prod</code> or any necessary envs)</p> <pre><code>mkdir -p kustomize-example/{base,overlays/dev,overlays/prod} &amp;&amp; cd kustomize-example\n</code></pre> </li> <li> <p>Check structure.</p> <pre><code>tree\n.\n\u251c\u2500\u2500 base\n\u2514\u2500\u2500 overlays\n    \u251c\u2500\u2500 dev\n    \u2514\u2500\u2500 prod\n\n4 directories, 0 files\n</code></pre> </li> <li> <p>Add necessary resources to <code>base</code> folder.</p> <p>Tips: Generate <code>yaml</code> with <code>kubectl</code> with <code>--dry-run=client -o yaml</code></p> <p>Examples:</p> <pre><code>kubectl create deployment kustomize-example --image nginx --replicas=1 --dry-run=client --output yaml &gt; kustomize-example/base/deployment.yaml # need manual modification\n</code></pre> <pre><code>kubectl create service clusterip kustomize-example --tcp=80:80 --dry-run=client --output yaml &gt; kustomize-example/base/service.yaml\n</code></pre> <pre><code>kubectl create configmap kustomize-example-uwsgi --from-literal=MYSQL_HOST=mysql.database.svc.cluster.local --from-literal=MYSQL_USER=user --from-literal=MYSQL_PORT=3306 --from-literal=MYSQL_DATABASE=test --dry-run=client -o yaml &gt; kustomize-example/base/configmap.yaml\n</code></pre> <pre><code>kubectl create secret generic kustomize-example-uwsgi --from-literal=MYSQL_PASSWORD=password --dry-run=client -o yaml &gt; kustomize-example/base/secret.yaml\n</code></pre> </li> <li> <p>Check if <code>base</code> is valid.</p> <pre><code>kubectl apply -k kustomize-example/base --dry-run=client\n</code></pre> </li> <li> <p>Create <code>Namespace</code> <code>kustomize-dev</code> and <code>kustomize-prod</code>.</p> <pre><code>kubectl create ns kustomize-dev --dry-run=client -o yaml &gt; kustomize-example/ns-kustomize-dev.yaml\nkubectl create ns kustomize-prod --dry-run=client -o yaml &gt; kustomize-example/ns-kustomize-prod.yaml\nkubectl apply -f kustomize-example/ns-kustomize-dev.yaml,kustomize-example/ns-kustomize-prod.yaml\n</code></pre> </li> <li> <p>Create overlays.</p> <ol> <li> <p>Make each overlay same as <code>base</code>.</p> <ul> <li> <p><code>kustomize-example/overlays/dev/kustomization.yaml</code>:</p> <p><pre><code>namespace: kustomize-dev\nbases:\n  - ../../base\n</code></pre>         - <code>kustomize-example/overlays/prod/kustomization.yaml</code>:</p> <pre><code>namespace: kustomize-prod\nbases:\n  - ../../base\n</code></pre> </li> <li> <p>Check</p> <pre><code>kubectl diff -k kustomize-example/overlays/dev\nkubectl diff -k kustomize-example/overlays/prod\n</code></pre> </li> </ul> </li> <li> <p>Create files to overwrite <code>base</code>.</p> <p>Example: - Add resource request/limit to prod. - Increase replicas for prod.</p> <ol> <li> <p>Add <code>patches</code> to <code>kustomize-example/overlays/prod/kustomization.yaml</code>.</p> <p>Example:</p> <pre><code>+ patches:\n+  - deployment.yaml\n</code></pre> </li> <li> <p>Update resources to override <code>base</code></p> <p>Example: <code>kustomize-example/overlays/prod/deployemnt.yaml</code></p> <pre><code>+        resources:\n+          requests:\n+            cpu: \"100m\"\n+            memory: \"256Mi\"\n+          limits:\n+            cpu: \"1000m\"\n+            memory: \"256Mi\"\n</code></pre> </li> </ol> </li> </ol> </li> <li> <p>Apply overlays (<code>prod</code> in this case.).</p> <pre><code>kubectl diff -k kustomize-example/overlays/prod\n</code></pre> <pre><code>@@ -123,7 +141,7 @@\nuid: 8a415db8-48c3-4a5b-831a-b70dd9adbf4c\nspec:\nprogressDeadlineSeconds: 600\n-  replicas: 1\n+  replicas: 2\nrevisionHistoryLimit: 10\nselector:\n    matchLabels:\n@@ -143,7 +161,13 @@\n    - image: nginx\n        imagePullPolicy: Always\n        name: nginx\n-        resources: {}\n+        resources:\n+          limits:\n+            cpu: \"1\"\n+            memory: 256Mi\n+          requests:\n+            cpu: 100m\n+            memory: 256Mi\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n@@ -158,7 +182,13 @@\n        image: nakamasato/flask-test\n        imagePullPolicy: Always\n        name: uwsgi\n-        resources: {}\n+        resources:\n+          limits:\n+            cpu: \"1\"\n+            memory: 256Mi\n+          requests:\n+            cpu: 100m\n+            memory: 256Mi\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n    dnsPolicy: ClusterFirst\n</code></pre> <pre><code>kubectl apply -k kustomize-example/overlays/prod\n</code></pre> </li> </ol>"},{"location":"helm-vs-kustomize/#example-1-web-app-with-mysql","title":"Example 1 (web app with mysql)","text":"<ol> <li> <p>Deploy dependencies.</p> <pre><code>kubectl create ns database; kubectl apply -k dependencies/mysql\n</code></pre> </li> <li> <p>Set up with <code>kustomize</code></p> <ol> <li> <p>Create <code>Namespace</code>s.</p> <pre><code>kubectl apply -f kustomize-example/ns-kustomize-dev.yaml,kustomize-example/ns-kustomize-prod.yaml\n</code></pre> </li> <li> <p>Deploy <code>kustomize-example</code>.</p> <pre><code>kubectl apply -k kustomize-example/overlays/dev\nkubectl apply -k kustomize-example/overlays/prod\n</code></pre> </li> <li> <p>Port-forward the service.</p> <pre><code>kubectl port-forward svc/kustomize-example 8080:80 -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Check the application functionality.</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" -d '{\"name\": \"naka\", \"email\": \"naka@example.com\"}' localhost:8080/users{\"id\":2,\"name\":\"naka\"}\n</code></pre> </li> </ol> </li> <li> <p>Set up with <code>helm</code></p> <ol> <li> <p>Install Helm chart.     <pre><code>helm install helm-example nakamasato/helm-example -n helm-dev --create-namespace\nhelm install helm-example nakamasato/helm-example -n helm-prod --create-namespace\n</code></pre></p> </li> <li> <p>Port-forward the service.</p> <pre><code>kubectl port-forward svc/helm-example 8080:80 -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Check <code>GET</code></p> <pre><code>curl localhost:8080/users/1\n</code></pre> </li> </ol> </li> <li> <p>Update image.</p> <ol> <li> <p><code>helm</code></p> <ul> <li><code>helm upgrade --set nginx.image.tag=1.15.2 helm-example nakamasato/helm-example -n helm-dev</code></li> </ul> <p>or</p> <ul> <li>Prepare <code>values-prod.yaml</code></li> <li> <p>Apply</p> <pre><code>helm upgrade -f values-prod.yaml helm-example nakamasato/helm-example -n helm-prod\n</code></pre> </li> </ul> </li> <li> <p><code>kustomize</code></p> <ul> <li> <p>Add the following code to <code>kustomize-example/overlays/prod/kustomization.yaml</code></p> <p><pre><code>images:\n  - name: nginx\n    newName: nginx\n    newTag: 1.15.2\n</code></pre>         - Apply</p> <pre><code>kubectl apply -k kustomize-example/overlays/prod\n</code></pre> </li> </ul> </li> </ol> </li> </ol>"},{"location":"helm-vs-kustomize/#argocd","title":"ArgoCD","text":"<p>Version: v2.0.3</p> <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.0.3/manifests/install.yaml\n</code></pre> <ol> <li> <p>Deploy using Kustomize</p> <ol> <li> <p>Namespace</p> <pre><code>kubectl apply -f kustomize-example/ns-kustomize-dev.yaml,kustomize-example/ns-kustomize-prod.yaml\n</code></pre> </li> <li> <p>Apply <code>ArgoProject</code>, dev and prod <code>Application</code>.</p> <pre><code>kubectl apply -f argocd/kustomize\n</code></pre> </li> </ol> </li> <li> <p>Deploy using Helm</p> <ol> <li> <p>Apply <code>ArgoProject</code>, dev and prod <code>Application</code>.</p> <pre><code>kubectl apply -f argocd/helm\n</code></pre> </li> </ol> </li> </ol> <p></p>"},{"location":"helm-vs-kustomize/#references","title":"References","text":"<ul> <li>https://helm.sh/docs/chart_template_guide/builtin_objects/</li> </ul>"},{"location":"ingress-nginx-controller/","title":"Ingress Nginx Controller","text":"<p>https://kubernetes.github.io/ingress-nginx/deploy</p> <p></p>"},{"location":"ingress-nginx-controller/#prerequisite","title":"Prerequisite","text":"<p>If you use <code>kind</code>, you need to create with the following configuration</p> <pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n</code></pre> <p>ref: https://kind.sigs.k8s.io/docs/user/ingress/#ingress-nginx</p>"},{"location":"ingress-nginx-controller/#get-started","title":"Get started","text":"<ul> <li> <p><code>namespace</code>: <code>ingress-nginx</code> (will be created)</p> </li> <li> <p>apply</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.7.0/deploy/static/provider/cloud/deploy.yaml\n</code></pre> </li> <li> <p>check the pods</p> <pre><code>kubectl get pod -n ingress-nginx\nNAME                                        READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create-j2mvc        0/1     Completed   0          62s\ningress-nginx-admission-patch-vpfdn         0/1     Completed   2          62s\ningress-nginx-controller-68649d49b8-8xld6   1/1     Running     0          62s\n</code></pre> </li> <li> <p>check nginx ingress controller's version</p> <pre><code>kubectl exec -it $(kubectl get po -n ingress-nginx | grep ingress-nginx-controller | awk '{print $1}') -n ingress-nginx -- /nginx-ingress-controller --version\n-------------------------------------------------------------------------------\nNGINX Ingress controller\n  Release:       v1.7.0\n  Build:         72ff21ed9e26cb969052c753633049ba8a87ecf9\n  Repository:    https://github.com/kubernetes/ingress-nginx\n  nginx version: nginx/1.21.6\n\n-------------------------------------------------------------------------------\n</code></pre> </li> </ul>"},{"location":"ingress-nginx-controller/#practice-with-an-app","title":"Practice with an app","text":"<p>Kubernetes ingress nginx contoller</p> <ol> <li>Deploy <code>Deployment</code> <pre><code>kubectl create deployment demo --image=httpd --port=80\n</code></pre></li> <li>Create <code>Service</code> <pre><code>kubectl expose deployment demo\n</code></pre></li> <li> <p>Create <code>Ingress</code> <pre><code>kubectl create ingress demo-localhost --class=nginx --rule=\"demo.localdev.me/*=demo:80\"\n</code></pre></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  creationTimestamp: null\n  name: demo-localhost\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: demo.localdev.me\n    http:\n      paths:\n      - backend:\n          service:\n            name: demo\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\nstatus:\n  loadBalancer: {}\n</code></pre> </li> <li> <p>Port-forward ingress     <pre><code>kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80\n</code></pre></p> </li> <li> <p>Curl http://demo.localdev.me:8080</p> <pre><code>curl http://demo.localdev.me:8080\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n</code></pre> </li> </ol>"},{"location":"ingress-nginx-controller/#changelogs","title":"Changelogs","text":"<ul> <li>1.4.0: Deprecated Kubernetes versions 1.20-1.21, Added support for, 1.25, currently supported versions v1.22, v1.23, v1.24, v1.25</li> <li>1.3.0:</li> <li>This release removes support for Kubernetes v1.19.0</li> <li>This release adds support for Kubernetes v1.24.0</li> <li>1.0.0: networking.k8s.io/v1beta is being dropped</li> <li>0.40.0: Following the Ingress extensions/v1beta1 deprecation, <code>networking.k8s.io/v1beta1</code> or <code>networking.k8s.io/v1</code> (Kubernetes v1.19 or higher)</li> <li>0.25.0: Support new <code>networking.k8s.io/v1beta1</code> package (for Kubernetes cluster &gt; v1.14.0)</li> </ul>"},{"location":"istio/","title":"Istio","text":""},{"location":"istio/#1-overview","title":"1. Overview","text":"<p>An Istio service mesh is logically split into a\u00a0data plane\u00a0and a\u00a0control plane.</p> <ul> <li>The\u00a0data plane\u00a0is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices. They also collect and report telemetry on all mesh traffic.</li> <li>The\u00a0control plane\u00a0manages and configures the proxies to route traffic.</li> </ul> <p></p> <p>A service mesh is a dedicated infrastructure layer that you can add to your applications. It allows you to transparently add capabilities like observability, traffic management, and security, without adding them to your own code. The term \u201cservice mesh\u201d describes both the type of software you use to implement this pattern, and the security or network domain that is created when you use that software.</p> <p>Istio uses Envoy, AN OPEN SOURCE EDGE AND SERVICE PROXY, DESIGNED FOR CLOUD-NATIVE APPLICATIONS, proxy as its data plane.</p>"},{"location":"istio/#2-summary","title":"2. Summary","text":"<p>CRDs and their roles</p> <ol> <li> <p>VirtualService: along with destination rules, the key building blocks of Istio\u2019s traffic routing functionality (e.g. routing to a specific version based on the request header, A/B testing, Canary release). <code>hosts</code>, <code>http</code>(match and <code>route</code>)     example <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n    - bookinfo.com # can be ip address, DNS, kubernetes service short name\n  http:\n  - match:\n    - uri:\n        prefix: /reviews\n    route:\n    - destination:\n        host: reviews\n  - match:\n    - uri:\n        prefix: /ratings\n    route:\n    - destination:\n        host: ratings\n</code></pre> <p>example with weight (A/B testing and canary rollouts) <pre><code>spec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n      weight: 75\n    - destination:\n        host: reviews\n        subset: v2\n      weight: 25\n</code></pre> <li> <p>DestinationRule: use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated. specify named service <code>subsets</code> (Ref Destination Rule)</p> <p>example <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: my-destination-rule\nspec:\n  host: my-svc\n  trafficPolicy:\n    loadBalancer:\n      simple: RANDOM\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n    trafficPolicy:\n      loadBalancer:\n        simple: ROUND_ROBIN\n  - name: v3\n    labels:\n      version: v3\n</code></pre> <li> <p>Gateway (Istio): manage inbound and outbound traffic for your mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads. Istio provides some preconfigured gateway proxy deployments (<code>istio-ingressgateway</code> and <code>istio-egressgateway</code>). You also need to bind the gateway to a virtual service. example <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: ext-host-gwy\nspec:\n  selector:\n    app: my-gateway-controller\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    hosts:\n    - ext-host.example.com\n    tls:\n      mode: SIMPLE\n      credentialName: ext-host-cert\n</code></pre> <p>specify routing</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: virtual-svc\nspec:\n  hosts:\n  - ext-host.example.com\n  gateways:\n  - ext-host-gwy\n</code></pre> <li> <p>ServiceEntry: Configuring service entries allows you to manage traffic for services running outside of the mesh. (ref: Service Entry)</p> </li> <li>Sidecar</li> <li><code>Gateway</code> (Kubernetes Gateway API): To overcome Ingress's shortcomings with a standard Kubernetes API (beta). You can consider migration of ingress traffic from Kubernetes Ignress or Gateway/VirtualService to the new Gateway API. (e.g. Istio Implementation of the Gateway API) Ref: Getting started with the Kubernetes Gateway API     Configure with Gateway in <code>gateway.networking.k8s.io/v1beta1</code> and <code>HTTPRoute</code></li>"},{"location":"istio/#3-getting-started","title":"3. Getting Started","text":""},{"location":"istio/#31-prepare-kubernetes-cluster","title":"3.1. Prepare Kubernetes Cluster","text":"<p>If you test on your local cluster, pleasee use docker-desktop, minikube, or kind.</p> <ol> <li> <p>kind: Istio Gateway might not work</p> <pre><code>kind create cluster --config=../local-cluster/kind/cluster-with-port-mapping.yaml\n</code></pre> </li> <li> <p>minikube: Confirmed everything works <pre><code>minikube start\n</code></pre></p> </li> <li>Docker Desktop</li> </ol>"},{"location":"istio/#32-install-istio","title":"3.2. Install Istio","text":"<ol> <li> <p>Install <code>istioctl</code> (you can skip this step if you already installed <code>istioctl</code>)</p> <pre><code>ISTIO_VERSION=1.20.0\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=$ISTIO_VERSION sh -\nexport PATH=\"$PATH:/$PWD/istio-${ISTIO_VERSION}/bin\"\n</code></pre> <p>Check istioctl version</p> <pre><code>istioctl version\nno ready Istio pods in \"istio-system\"\n1.20.0\n</code></pre> </li> <li> <p>Install istio</p> <pre><code>istioctl install --set profile=demo -y\n</code></pre> <p>Result <pre><code>\u2714 Istio core installed\n\u2714 Istiod installed\n\u2714 Ingress gateways installed\n\u2714 Egress gateways installed\n\u2714 Installation complete\nMaking this installation the default for injection and validation.\n</code></pre> <p><code>istio-egressgateway</code>, <code>istio-ingressgateway</code>, and <code>istiod</code> are deployed in <code>istio-system</code> namespace:</p> <pre><code>kubectl get po -n istio-system\nNAME                                   READY   STATUS    RESTARTS   AGE\nistio-egressgateway-6c4796c98-4q45f    1/1     Running   0          2m48s\nistio-ingressgateway-d94b4444b-v4tbq   1/1     Running   0          2m48s\nistiod-85669db8fd-5lz4s                1/1     Running   0          2m58s\n</code></pre>"},{"location":"istio/#33-add-istio-injectionenabled-to-the-target-namespace","title":"3.3. Add <code>istio-injection=enabled</code> to the target Namespace","text":"<ol> <li> <p>Add a namespace label <code>istio-injection=enabled</code> to <code>default</code> Namespace to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later:</p> <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre> <p>Check labels</p> <pre><code>kubectl get ns default --show-labels\nNAME      STATUS   AGE     LABELS\ndefault   Active   4m57s   istio-injection=enabled,kubernetes.io/metadata.name=default\n</code></pre> </li> </ol>"},{"location":"istio/#34-deploy-the-sample-application","title":"3.4. Deploy the sample application","text":"<ol> <li> <p>Deploy sample app</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>Deployed resources:</p> <ol> <li><code>Deployment</code>: <code>details-v1</code>, <code>ratings-v1</code>, <code>reviews-v1</code>, <code>reviews-v2</code>, <code>reviews-v3</code>, <code>productpage-v1</code></li> <li><code>Service</code>: <code>details</code>, <code>ratings</code>, <code>reviews</code>, <code>productpage</code></li> <li><code>ServiceAccount</code>: <code>bookinfo-details</code>, <code>bookinfo-ratings</code>, <code>bookinfo-reviews</code>, <code>bookinfo-productpage</code></li> </ol> <pre><code>service/details created\nserviceaccount/bookinfo-details created\ndeployment.apps/details-v1 created\nservice/ratings created\nserviceaccount/bookinfo-ratings created\ndeployment.apps/ratings-v1 created\nservice/reviews created\nserviceaccount/bookinfo-reviews created\ndeployment.apps/reviews-v1 created\ndeployment.apps/reviews-v2 created\ndeployment.apps/reviews-v3 created\nservice/productpage created\nserviceaccount/bookinfo-productpage created\ndeployment.apps/productpage-v1 created\n</code></pre> <p>Envoy sider is added to all pods (2 containers are running in each pod).</p> <pre><code>kubectl get po\nNAME                              READY   STATUS    RESTARTS   AGE\ndetails-v1-79f774bdb9-ctf75       2/2     Running   0          28s\nproductpage-v1-6b746f74dc-7zgpg   2/2     Running   0          28s\nratings-v1-b6994bb9-rw74b         2/2     Running   0          28s\nreviews-v1-545db77b95-t6gkl       2/2     Running   0          28s\nreviews-v2-7bf8c9648f-n9tmq       2/2     Running   0          28s\nreviews-v3-84779c7bbc-tmzlr       2/2     Running   0          28s\n</code></pre> <p>If you deploy to another namespace without <code>istio-injection=enabled</code> label, Envoy sidecar container will not be injected.</p> </li> <li> <p>Verify app is running.</p> <pre><code>kubectl exec \"$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n\n&lt;title&gt;Simple Bookstore App&lt;/title&gt;\n</code></pre> </li> </ol>"},{"location":"istio/#35-open-the-app-to-outside-traffic-gateway-virtualservice","title":"3.5. Open the app to outside traffic (Gateway &amp; VirtualService)","text":"<ol> <li> <p>Istio Gateway (<code>Gateway</code> and <code>VirtualService</code> (<code>networking.istio.io/v1alpha3</code>))</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/networking/bookinfo-gateway.yaml\n</code></pre> <p>yaml details <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  # The selector matches the ingress gateway pod labels.\n  # If you installed Istio using Helm following the standard documentation, this would be \"istio=ingress\"\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - bookinfo-gateway\n  http:\n  - match:\n    - uri:\n        exact: /productpage\n    - uri:\n        prefix: /static\n    - uri:\n        exact: /login\n    - uri:\n        exact: /logout\n    - uri:\n        prefix: /api/v1/products\n    route:\n    - destination:\n        host: productpage\n        port:\n          number: 9080\n</code></pre> <p>Notes: If you deploy <code>Gateway</code> and <code>VirtualService</code> in different namespaces, you need to specify the gateway in <code>VirtualService</code> with namespace as a prefix.</p> <pre><code>gateways:\n- gateway/bookinfo-gateway # &lt;namespace of gateway&gt;/&lt;gateway name&gt;\n</code></pre> <p>Alternatively, <code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/gateway-api/bookinfo-gateway.yaml</code> to install (<code>Gateway</code> and <code>HTTPRoute</code> in <code>gateway.networking.k8s.io/v1beta1</code>)</p> <li> <p>Check     <pre><code>istioctl analyze\n\u2714 No validation issues found when analyzing namespace: default.\n</code></pre></p> </li> <li> <p>Check ingress gateway</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                                      AGE\nistio-ingressgateway   LoadBalancer   10.103.34.38   localhost     15021:31476/TCP,80:31411/TCP,443:32714/TCP,31400:30467/TCP,15443:30550/TCP   44m\n</code></pre> <p>You might see <code>EXTERNAL-IP</code> is <code>&lt;pending&gt;</code>. You need to run <code>minikube tunnel</code></p> </li> <li> <p>Set ingress ip and ports:</p> <p>Most platforms:</p> <pre><code>export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nexport INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}')\nexport SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"https\")].port}')\n</code></pre> <p>Docker Desktop or Kind:</p> <pre><code>export INGRESS_HOST=127.0.0.1\n</code></pre> <pre><code>export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT\n</code></pre> <p>Check</p> <pre><code>echo \"$GATEWAY_URL\"\n127.0.0.1:80\n</code></pre> </li> <li> <p>Open http://127.0.0.1:80/productpage on your browser:</p> <p></p> <p>TODO: You might not be able to open it when <code>EXTERNAL-IP</code> is <code>&lt;pending&gt;</code> (this happens when using <code>kind</code>).</p> </li>"},{"location":"istio/#36-define-the-service-versions","title":"3.6. Define the service versions","text":"<p>Before you can use Istio to control the Bookinfo version routing, you need to define the available versions.</p> <p>Create <code>DestinationRule</code> for each service <code>productpage</code>, <code>reviews</code>, <code>ratings</code> and <code>details</code>.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/networking/destination-rule-all.yaml\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n  - name: v3\n    labels:\n      version: v3\n</code></pre>"},{"location":"istio/#37-request-routing","title":"3.7. Request Routing","text":"<p>Istio includes beta support for the Kubernetes Gateway API</p>"},{"location":"istio/#371-optional-install-necessary-crds-only-for-gateway-api-not-needed-for-istio-apis","title":"3.7.1. (Optional) Install necessary CRDs (only for <code>Gateway API</code> not needed for Istio APIs)","text":"<pre><code>kubectl get crd gateways.gateway.networking.k8s.io &amp;&gt; /dev/null || \\\n  { kubectl kustomize \"github.com/kubernetes-sigs/gateway-api/config/crd?ref=v0.8.0-rc1\" | kubectl apply -f -; }\n</code></pre> <p>The following custom resource definitions will be created:</p> <ol> <li><code>GatewayClass</code></li> <li><code>Gateway</code></li> <li><code>HttpRoute</code></li> <li><code>ReferenceGrant</code></li> </ol> <p>For more details, please check https://github.com/kubernetes-sigs/gateway-api</p>"},{"location":"istio/#372-route-to-version-1-istio-apis","title":"3.7.2. Route to version 1 (Istio APIs)","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/networking/virtual-service-all-v1.yaml\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n</code></pre>"},{"location":"istio/#373-route-based-on-user-identity-istio-apis","title":"3.7.3. Route based on user identity (Istio APIs)","text":"<p>Istio also supports routing based on strongly authenticated JWT on ingress gateway, refer to the JWT claim based routing for more details.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n    - reviews\n  http:\n  - match:\n    - headers:\n        end-user:\n          exact: jason\n    route:\n    - destination:\n        host: reviews\n        subset: v2\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n</code></pre> <p>Login to <code>jason</code>:</p> <p></p> <p>What's done?</p> <p>In this task, you used Istio to send 100% of the traffic to the v1 version of each of the Bookinfo services. You then set a rule to selectively send traffic to version v2 of the reviews service based on a custom end-user header added to the request by the productpage service.</p>"},{"location":"istio/#38-view-the-dashboard","title":"3.8. View the dashboard","text":"<ol> <li> <p>Install kiali dashboard</p> <pre><code>for f in https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/addons/{grafana,jaeger,kiali,loki,prometheus}.yaml; do kubectl apply -f $f; done\nkubectl rollout status deployment/kiali -n istio-system\n</code></pre> </li> <li> <p>Open dashboard</p> <pre><code>istioctl dashboard kiali\n</code></pre> <p>The traffic is visualized in the graph.</p> <p></p> </li> </ol>"},{"location":"istio/#4-cleanup","title":"4. Cleanup","text":"<pre><code>istioctl uninstall --purge\n</code></pre> <pre><code>for f in https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/addons/{grafana,jaeger,kiali,loki,prometheus}.yaml; do kubectl delete -f $f; done # delete kilia\nkubectl delete -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/networking/bookinfo-gateway.yaml # delete gateway\nkubectl delete -f kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/platform/kube/bookinfo.yaml # delete application\nistioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f - # delete istio\nistioctl tag remove default\n</code></pre> <pre><code>kubectl delete namespace istio-system\nkubectl label namespace default istio-injection-\n</code></pre>"},{"location":"istio/#5-more","title":"5. More","text":"<ol> <li>Ingress</li> <li>Ingress Gateway</li> </ol>"},{"location":"istio/#5-faq","title":"5. FAQ","text":"<ol> <li>Istio APIs vs Gateway APIs</li> </ol>"},{"location":"istio/#7-experiment","title":"7. Experiment","text":""},{"location":"istio/#71-deploy-gateway-and-virtualservice-in-different-namespaces","title":"7.1. Deploy Gateway and VirtualService in different namespaces","text":"<p>Delete gateway and virtual service:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/istio/istio/release-${ISTIO_VERSION%.*}/samples/bookinfo/networking/bookinfo-gateway.yaml\n</code></pre> <p>Create a namespace for gateway</p> <pre><code>kubectl create ns gateway\n</code></pre> <p>Create <code>productpage-v2</code></p> <pre><code>kubectl deploy -f productpage-v2.yaml\n</code></pre> <p>Gateway and multiple VirtualServices</p> <p><code>bookinfo-gateway.yaml</code>: <code>Gateway</code> and <code>VirtualService</code> (one for Gateway)</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\n  namespace: gateway\nspec:\n  # The selector matches the ingress gateway pod labels.\n  # If you installed Istio using Helm following the standard documentation, this would be \"istio=ingress\"\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 8080\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\n  namespace: gateway\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - bookinfo-gateway # &lt;namespace of gateway&gt;/&lt;gateway name&gt;\n  http:\n  - match:\n    - uri:\n        exact: /productpage\n    - uri:\n        prefix: /static\n    route:\n    - destination:\n        host: productpage.default.svc.cluster.local\n        port:\n          number: 9080\n  - match:\n    - uri:\n        exact: /login\n    - uri:\n        exact: /logout\n    - uri:\n        prefix: /api/v1/products\n    route:\n    - destination:\n        host: productpage-v2.default.svc.cluster.local\n        port:\n          number: 9080\nEOF\n</code></pre> <p>You can check <code>/api/v1/products</code> is routed to <code>productpage-v2</code>, while <code>/productpage</code> is routed to <code>productpage-v1</code>.</p> <p>Clean up the resources:</p> <pre><code>kubectl delete gateway -n gateway bookinfo-gateway\nkubectl delete virtualservice -n gateway bookinfo\n</code></pre>"},{"location":"istio/#72-gateway-with-multiple-virtualservices","title":"7.2. Gateway with Multiple VirtualServices","text":"<p>If you have mulitple <code>VirtualService</code> for the same host, the one corresponding to <code>Gateway</code> will be used. here</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\n  namespace: gateway\nspec:\n  # The selector matches the ingress gateway pod labels.\n  # If you installed Istio using Helm following the standard documentation, this would be \"istio=ingress\"\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 8080\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\n  namespace: gateway\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - bookinfo-gateway # &lt;namespace of gateway&gt;/&lt;gateway name&gt;\n  http:\n  - route:\n    - destination:\n        host: productpage.default.svc.cluster.local\n        port:\n          number: 9080\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: productpage\nspec:\n  hosts:\n  - productpage\n  http:\n  - match:\n    - uri:\n        exact: /productpage\n    - uri:\n        prefix: /static\n    route:\n    - destination:\n        host: productpage\n        port:\n          number: 9080\n  - match:\n    - uri:\n        exact: /login\n    - uri:\n        exact: /logout\n    - uri:\n        prefix: /api/v1/products\n    route:\n    - destination:\n        host: productpage-v2\n        port:\n          number: 9080\nEOF\n</code></pre> <p>Result:</p> <ul> <li>All the requests via gateway are routed to <code>productpage</code> service.<ul> <li>http://localhost/productpage</li> <li>http://localhost/api/v1/products</li> </ul> </li> <li> <p>If you access <code>productpage</code> service directly, the routing is based on <code>productpage</code> VirtualService. (it seems that all the requests are routed to <code>productpage</code> not <code>productpage-v2</code>?)</p> <ul> <li>http://productpage.default.svc.cluster.local/productpage</li> <li>http://productpage.default.svc.cluster.local/api/v1/products</li> </ul> <pre><code>for p in productpage api/v1/products; do kubectl run -i --tty --rm debug --image=curlimages/curl:latest --restart=Never --overrides='{\"apiVersion\": \"v1\",\"metadata\": {\"annotations\": {\"sidecar.istio.io/inject\": \"false\"}}}' -- http://productpage.default.svc.cluster.local:9080/$p; done\n</code></pre> </li> </ul>"},{"location":"istio/#6-ref","title":"6. Ref","text":"<ol> <li>How to install kind and istio ingress controller</li> <li>Official<ol> <li>Istio operator code overview</li> <li>Istio Operator</li> </ol> </li> <li>ConflictingMeshGatewayVirtualServiceHosts: We can use <code>exportTo</code> to specify the target namespace.</li> <li>Route rules have no effect on ingress gateway requests The ingress requests are using the gateway host (e.g., myapp.com) which will activate the rules in the myapp VirtualService that routes to any endpoint of the helloworld service. Only internal requests with the host helloworld.default.svc.cluster.local will use the helloworld VirtualService which directs traffic exclusively to subset v1.</li> <li>Split large virtual services and destination rules into multiple resources</li> <li>#45332 Istio virtual service multiple services with the same host</li> <li>Introducing istiod: simplifying the control plane</li> <li>Relationship between the CLI and controller: The CLI and controller share the same API and codebase for generating manifests from the API. You can think of the controller as the CLI command istioctl install running in a loop in a pod in the cluster and using the config from the in-cluster IstioOperator custom resource (CR).</li> </ol>"},{"location":"istio/implementation/","title":"Istio Implementation","text":""},{"location":"istio/implementation/#1-must-read","title":"1. Must-read","text":"<ol> <li>Architecture of Istiod</li> </ol>"},{"location":"istio/implementation/#2-getting-started","title":"2. Getting Started","text":""},{"location":"istio/implementation/#21-components","title":"2.1. Components","text":"<p>Need to understand the Istio components</p> <ol> <li>Envoy: sidecar proxies for microservices to handle ingress/egress traffic between services in the cluster and from a service to external services.</li> <li>Istiod: Istio control plane, which provides service discovery, configuration and certificate management &lt;- This is a modular monolith<ol> <li>Pilot: Responsible for configuring the proxies at runtime</li> <li>Citadel: Responsible for certificate issuance and rotation.</li> <li>Gallery: Responsible for validating, ingesting, aggregating, transforming, and distributing config within Istio</li> </ol> </li> <li>Operator: The component provides user friendly options to operate the Istio service mesh. The role is management of istio components (<code>istiod</code>, <code>istio-ingressgateway</code>, and <code>istio-egressgateway</code>) with <code>IstioOperator</code> Custom Resource, which is also same role of <code>istioctl</code> (Istio Operator Inatall)</li> </ol> <p>Memo</p> <ol> <li> <p>Istiod from microservices to a single binary istiod in 2020: &lt;- Very interesting story!</p> <p>Istio\u2019s control plane is, itself, a modern, cloud-native application. Thus, it was built from the start as a set of microservices. Individual Istio components like service discovery (Pilot), configuration (Galley), certificate generation (Citadel) and extensibility (Mixer) were all written and deployed as separate microservices.</p> <p>Having established that many of the common benefits of microservices didn\u2019t apply to the Istio control plane, we decided to unify them into a single binary: istiod (the \u2019d\u2019 is for daemon). 1. Pilot seems to represent istiod: https://github.com/istio/istio/blob/master/architecture/networking/pilot.md &lt;- You must read this to know the details about Istiod 1. Ingress: In addition to <code>VirtualService</code> and <code>Gateway</code>, Istio supports the <code>Ingress</code> core resource type. Like CRDs, the <code>Ingress</code> controller implements <code>ConfigStore</code>, but a bit differently. <code>Ingress</code> resources are converted on the fly to <code>VirtualService</code> and <code>Gateway</code>, read more 1. Gateway API: <code>Gateway</code> (referring to the Kubernetes API, not the same-named Istio type) works very similarly to Ingress. The <code>Gateway</code> controller also coverts Gateway API types into <code>VirtualService</code> and <code>Gateway</code>, implementing the <code>ConfigStore</code> interface.</p> </li> </ol>"},{"location":"istio/implementation/#22-repository","title":"2.2. Repository","text":"<p>Better to know the structure of the repository! Please read https://github.com/istio/istio/tree/1.19.0#repositories</p>"},{"location":"istio/implementation/#3-installation","title":"3. Installation","text":""},{"location":"istio/implementation/#31-install-with-istio-operator","title":"3.1. Install with Istio Operator","text":"<p>Install Istio Operator</p> <pre><code>istioctl operator init\n</code></pre> <p><code>istio-operator</code> Deployment will be deployed in <code>istio-operator</code> Namespace.</p> <p>Install Istio with <code>IstioOperator</code> CR</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  namespace: istio-system\n  name: example-istiocontrolplane\nspec:\n  profile: demo\nEOF\n</code></pre>"},{"location":"istio/implementation/#32-install-with-istioctl","title":"3.2. Install with <code>istioctl</code>","text":"<pre><code>istioctl profile list\nistioctl profile dump demo # demo is one of the profiles\n</code></pre> <pre><code>apiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  components:\n    base:\n      enabled: true\n    cni:\n      enabled: false\n    egressGateways:\n    - enabled: true\n      k8s:\n        resources:\n          requests:\n            cpu: 10m\n            memory: 40Mi\n      name: istio-egressgateway\n    ingressGateways:\n    - enabled: true\n      k8s:\n        resources:\n          requests:\n            cpu: 10m\n            memory: 40Mi\n        service:\n          ports:\n          - name: status-port\n            port: 15021\n            targetPort: 15021\n          - name: http2\n            port: 80\n            targetPort: 8080\n          - name: https\n            port: 443\n            targetPort: 8443\n          - name: tcp\n            port: 31400\n            targetPort: 31400\n          - name: tls\n            port: 15443\n            targetPort: 15443\n      name: istio-ingressgateway\n    istiodRemote:\n      enabled: false\n    pilot:\n      enabled: true\n      k8s:\n        env:\n        - name: PILOT_TRACE_SAMPLING\n          value: \"100\"\n        resources:\n          requests:\n            cpu: 10m\n            memory: 100Mi\n  hub: docker.io/istio\n  meshConfig:\n    accessLogFile: /dev/stdout\n    defaultConfig:\n      proxyMetadata: {}\n    enablePrometheusMerge: true\n    extensionProviders:\n    - envoyOtelAls:\n        port: 4317\n        service: opentelemetry-collector.istio-system.svc.cluster.local\n      name: otel\n    - name: skywalking\n      skywalking:\n        port: 11800\n        service: tracing.istio-system.svc.cluster.local\n    - name: otel-tracing\n      opentelemetry:\n        port: 4317\n        service: opentelemetry-collector.otel-collector.svc.cluster.local\n  profile: demo\n  tag: 1.19.0\n  values:\n    base:\n      enableCRDTemplates: false\n      validationURL: \"\"\n    defaultRevision: \"\"\n    gateways:\n      istio-egressgateway:\n        autoscaleEnabled: false\n        env: {}\n        name: istio-egressgateway\n        secretVolumes:\n        - mountPath: /etc/istio/egressgateway-certs\n          name: egressgateway-certs\n          secretName: istio-egressgateway-certs\n        - mountPath: /etc/istio/egressgateway-ca-certs\n          name: egressgateway-ca-certs\n          secretName: istio-egressgateway-ca-certs\n        type: ClusterIP\n      istio-ingressgateway:\n        autoscaleEnabled: false\n        env: {}\n        name: istio-ingressgateway\n        secretVolumes:\n        - mountPath: /etc/istio/ingressgateway-certs\n          name: ingressgateway-certs\n          secretName: istio-ingressgateway-certs\n        - mountPath: /etc/istio/ingressgateway-ca-certs\n          name: ingressgateway-ca-certs\n          secretName: istio-ingressgateway-ca-certs\n        type: LoadBalancer\n    global:\n      configValidation: true\n      defaultNodeSelector: {}\n      defaultPodDisruptionBudget:\n        enabled: true\n      defaultResources:\n        requests:\n          cpu: 10m\n      imagePullPolicy: \"\"\n      imagePullSecrets: []\n      istioNamespace: istio-system\n      istiod:\n        enableAnalysis: false\n      jwtPolicy: third-party-jwt\n      logAsJson: false\n      logging:\n        level: default:info\n      meshNetworks: {}\n      mountMtlsCerts: false\n      multiCluster:\n        clusterName: \"\"\n        enabled: false\n      network: \"\"\n      omitSidecarInjectorConfigMap: false\n      oneNamespace: false\n      operatorManageWebhooks: false\n      pilotCertProvider: istiod\n      priorityClassName: \"\"\n      proxy:\n        autoInject: enabled\n        clusterDomain: cluster.local\n        componentLogLevel: misc:error\n        enableCoreDump: false\n        excludeIPRanges: \"\"\n        excludeInboundPorts: \"\"\n        excludeOutboundPorts: \"\"\n        image: proxyv2\n        includeIPRanges: '*'\n        logLevel: warning\n        privileged: false\n        readinessFailureThreshold: 30\n        readinessInitialDelaySeconds: 1\n        readinessPeriodSeconds: 2\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 1024Mi\n          requests:\n            cpu: 10m\n            memory: 40Mi\n        statusPort: 15020\n        tracer: zipkin\n      proxy_init:\n        image: proxyv2\n      sds:\n        token:\n          aud: istio-ca\n      sts:\n        servicePort: 0\n      tracer:\n        datadog: {}\n        lightstep: {}\n        stackdriver: {}\n        zipkin: {}\n      useMCP: false\n    istiodRemote:\n      injectionURL: \"\"\n    pilot:\n      autoscaleEnabled: false\n      autoscaleMax: 5\n      autoscaleMin: 1\n      configMap: true\n      cpu:\n        targetAverageUtilization: 80\n      env: {}\n      image: pilot\n      keepaliveMaxServerConnectionAge: 30m\n      nodeSelector: {}\n      podLabels: {}\n      replicaCount: 1\n      traceSampling: 1\n    telemetry:\n      enabled: true\n      v2:\n        enabled: true\n        metadataExchange:\n          wasmEnabled: false\n        prometheus:\n          enabled: true\n          wasmEnabled: false\n        stackdriver:\n          configOverride: {}\n          enabled: false\n          logging: false\n          monitoring: false\n          topology: false\n</code></pre> <p>What <code>istioctl install</code> (you can install with <code>-p demo</code> to specify a profile) does is to create an <code>IstioOperator</code> Custom Resource and Istio Controller does the actual work.</p> <p>Relationship between the CLI and controller:</p> <p>The CLI and controller share the same API and codebase for generating manifests from the API. You can think of the controller as the CLI command istioctl install running in a loop in a pod in the cluster and using the config from the in-cluster IstioOperator custom resource (CR).</p>"},{"location":"istio/implementation/#4-code-reading","title":"4. Code Reading","text":""},{"location":"istio/implementation/#41-pilot-discovery-istiod","title":"4.1. pilot-discovery (istiod)","text":"<ol> <li>pilot/cmd/pilot-discovery/main.go<ol> <li>pilot/cmd/pilot-discovery/app/cmd.go <pre><code>discoveryServer, err := bootstrap.NewServer(serverArgs)\n</code></pre></li> </ol> </li> <li>bootstrap.NewServer<ol> <li>aggregate.NewController</li> <li>Initialize <code>Server</code> -&gt; <code>s</code></li> <li>xds.NewDiscoveryServer</li> <li>initServers</li> <li>initKubeClient</li> <li>initMeshConfiguration</li> <li>initMeshNetworks</li> <li>initMeshHandlers</li> <li>s.environment.InitNetworksManager</li> <li>maybeCreateCA</li> <li>initControllers</li> <li>s.XDSServer.InitGenerators</li> <li>s.initWorkloadTrustBundle</li> <li>s.initIstiodCerts</li> <li>s.initSecureDiscoveryService</li> <li>s.initSecureWebhookServer</li> <li>s.initSidecarInjector</li> <li>s.initConfigValidation</li> <li>s.initRegistryEventHandlers</li> <li>s.initDiscoveryService</li> </ol> </li> </ol>"},{"location":"istio/implementation/#42-ingress","title":"4.2. Ingress","text":"<ol> <li> <p>pilot/pkg/config/kube/ingress/controller.go</p> <ol> <li>Ingress\u3001Gateway\u3001VirtualService\u3092\u7ba1\u7406\u3057\u3066\u308b\u3051\u3069\u3001pilot\u3068\u306f\uff1f -&gt; Istiod\u5185\u306eComponent\u3067Traffic Management\u306a\u3069\u3092\u62c5\u5f53\u3059\u308b\u4e3b\u8981\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8</li> </ol> <p><pre><code>// In 1.0, the Gateway is defined in the namespace where the actual controller runs, and needs to be managed by\n// user.\n// The gateway is named by appending \"-istio-autogenerated-k8s-ingress\" to the name of the ingress.\n//\n// Currently the gateway namespace is hardcoded to istio-system (model.IstioIngressNamespace)\n//\n// VirtualServices are also auto-generated in the model.IstioIngressNamespace.\n//\n// The sync of Ingress objects to IP is done by status.go\n// the 'ingress service' name is used to get the IP of the Service\n// If ingress service is empty, it falls back to NodeExternalIP list, selected using the labels.\n// This is using 'namespace' of pilot - but seems to be broken (never worked), since it uses Pilot's pod labels\n// instead of the ingress labels.\n\n// Follows mesh.IngressControllerMode setting to enable - OFF|STRICT|DEFAULT.\n// STRICT requires \"kubernetes.io/ingress.class\" == mesh.IngressClass\n// DEFAULT allows Ingress without explicit class.\n\n// In 1.1:\n// - K8S_INGRESS_NS - namespace of the Gateway that will act as ingress.\n// - labels of the gateway set to \"app=ingressgateway\" for node_port, service set to 'ingressgateway' (matching default install)\n//   If we need more flexibility - we can add it (but likely we'll deprecate ingress support first)\n// -\n</code></pre> 1. pilot/pkg/config/kube/gateway/controller.g: This is for Gateway API</p> </li> </ol>"},{"location":"istio/implementation/#local-development","title":"Local Development","text":"<ol> <li>https://github.com/istio/istio/wiki/Preparing-for-Development</li> <li>https://github.com/istio/istio/wiki/Using-the-Code-Base<ol> <li>Clone     <pre><code>mkdir -p $GOPATH/src/istio.io/istio\ngit clone https://github.com/istio/istio $GOPATH/src/istio.io/istio\ncd $GOPATH/src/istio.io/istio\n</code></pre></li> <li> <p>Set env var</p> <pre><code>USER=nakamasato # in my case\n</code></pre> <pre><code># This defines the docker hub to use when running integration tests and building docker images\n# eg: HUB=\"docker.io/istio\", HUB=\"gcr.io/istio-testing\"\nexport HUB=\"docker.io/$USER\"\n\n# This defines the docker tag to use when running integration tests and\n# building docker images to be your user id. You may also set this variable\n# this to any other legitimate docker tag.\nexport TAG=$USER\n\n# This defines a shortcut to change directories to $HOME/istio.io\nexport ISTIO=$GOPATH/src/istio.io/istio\n</code></pre> </li> </ol> </li> </ol>"},{"location":"istio/implementation/#memo","title":"Memo","text":""},{"location":"istio/implementation/#overview","title":"Overview","text":"<ol> <li><code>istio-ingressgateway</code>, <code>istio-egressgateway</code><ol> <li>Helm\u306f\u3053\u3053</li> <li>image\u306f<code>proxyv2</code>\u304c\u4f7f\u308f\u308c\u3066\u308b</li> <li><code>proxyv2</code> \u306fpilot-agent\u306b\u3082\u4f7f\u308f\u308c\u3066\u3044\u308b\u304b\u3089\u3069\u3046\u3044\u3046\u9055\u3044\u304b\u304c\u4e0d\u660e</li> </ol> </li> <li><code>pilot-agent</code> sidecar<ol> <li><code>proxyv2</code></li> <li>Dockerfile.proxyv2: Entrypoint: <code>\"/usr/local/bin/pilot-agent\"</code></li> <li>Inject\u3055\u308c\u308bSidecar\u306e\u3053\u3068\u304c <code>pilot-agent</code>\u3068\u547c\u3070\u308c\u3066\u3044\u308b\u3063\u307d\u3044</li> <li>The code for sidecar injection: https://github.com/istio/istio/blob/1.19.0/pkg/kube/inject/inject.go</li> <li>main.go: https://github.com/istio/istio/blob/1.19.0/pilot/cmd/pilot-agent/main.go</li> </ol> </li> <li><code>pilot-discovery</code> controller &lt;- <code>istiod</code> Deployment<ol> <li>Docker: https://github.com/istio/istio/blob/1.19.0/pilot/docker/Dockerfile.pilot</li> <li><code>main.go</code>: Entrypoint of istiod server<ol> <li>pilot/cmd/pilot-discovery/main.go</li> <li>pilot/cmd/pilot-discovery/app/cmd.go <pre><code>discoveryServer, err := bootstrap.NewServer(serverArgs)\n</code></pre></li> </ol> </li> <li>From Logs, https://github.com/istio/istio/blob/1.19.0/pilot/pkg/bootstrap/server.go is the istiod server initialization.<ol> <li>bootstrap.NewServer<ol> <li>initializing istiod admin server: initServers</li> <li><code>initIstiodAdminServer</code></li> <li><code>initControllers</code></li> <li>initializing secure discovery service: initSecureDiscoveryService</li> <li>initializing secure webhook server for istiod webhooks: initSecureWebhookServer</li> <li>initializing sidecar injector: initSidecarInjector</li> <li>initializing config validator: initConfigValidation</li> <li>initializing registry event handlers: initRegistryEventHandlers</li> <li>starting discovery service: initDiscoveryService</li> </ol> </li> </ol> </li> </ol> </li> </ol>"},{"location":"istio/implementation/#faq","title":"FAQ","text":"<ol> <li> <p>How <code>sidecar</code> container is injected -&gt; https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection     &gt; Sidecars can be automatically added to applicable Kubernetes pods using a mutating webhook admission controller provided by Istio.</p> <p>Implementation: https://github.com/istio/istio/blob/1.19.0/pkg/kube/inject/webhook.go</p> </li> </ol>"},{"location":"istio/implementation/#ref","title":"Ref","text":"<ol> <li>Istio operator code overview</li> <li>Istio Operator</li> </ol>"},{"location":"istio/ingress/","title":"Kubernetes Ingress","text":"<ol> <li>In this page, we'll use <code>Istio APIs</code> not <code>Gateway API</code>.</li> <li>For local Kubernetes, I use Kubernetes in Docker Desktop as we can directly use <code>LoadBalancer</code> type <code>Service</code>.</li> <li>For <code>Ingress</code> resource, The <code>spec.ingressClassName</code> is required to tell the Istio gateway controller that it should handle this Ingress, otherwise it will be ignored.</li> </ol>"},{"location":"istio/ingress/#1-setup","title":"1. Setup","text":"<pre><code>istioctl install --set profile=demo -y\n</code></pre> <pre><code>export INGRESS_NAME=istio-ingressgateway\nexport INGRESS_NS=istio-system\n</code></pre> <pre><code>kubectl get svc \"$INGRESS_NAME\" -n \"$INGRESS_NS\"\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                                      AGE\nistio-ingressgateway   LoadBalancer   10.96.30.107   localhost     15021:30627/TCP,80:30740/TCP,443:31354/TCP,31400:31161/TCP,15443:30423/TCP   33s\n</code></pre> <pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.19/samples/httpbin/httpbin.yaml\n</code></pre> <pre><code>kubectl port-forward svc/httpbin 8000:8000\n</code></pre> <p>http://localhost:8000</p> <p></p>"},{"location":"istio/ingress/#2-configure-ingress-using-a-gateway","title":"2. Configure ingress using a Gateway","text":"<p>Gateway:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: httpbin-gateway\nspec:\n  # The selector matches the ingress gateway pod labels.\n  # If you installed Istio using Helm following the standard documentation, this would be \"istio=ingress\"\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"httpbin.example.com\"\nEOF\n</code></pre> <p>VirtualService:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  hosts:\n  - \"httpbin.example.com\"\n  gateways:\n  - httpbin-gateway\n  http:\n  - match:\n    - uri:\n        prefix: /status\n    - uri:\n        prefix: /delay\n    route:\n    - destination:\n        port:\n          number: 8000\n        host: httpbin\nEOF\n</code></pre> <pre><code>kubectl get gw\nNAME              AGE\nhttpbin-gateway   2m7s\n</code></pre> <pre><code>kubectl get vs\nNAME      GATEWAYS              HOSTS                     AGE\nhttpbin   [\"httpbin-gateway\"]   [\"httpbin.example.com\"]   104s\n</code></pre>"},{"location":"istio/ingress/#3-determine-ingress-ip-and-ports","title":"3. Determine Ingress IP and ports","text":"<pre><code>kubectl get svc \"$INGRESS_NAME\" -n \"$INGRESS_NS\"\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                                      AGE\nistio-ingressgateway   LoadBalancer   10.96.30.107   localhost     15021:30627/TCP,80:30740/TCP,443:31354/TCP,31400:31161/TCP,15443:30423/TCP   2m13s\n</code></pre> <pre><code>export INGRESS_HOST=$(kubectl -n \"$INGRESS_NS\" get service \"$INGRESS_NAME\" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\nexport INGRESS_PORT=$(kubectl -n \"$INGRESS_NS\" get service \"$INGRESS_NAME\" -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}')\n</code></pre>"},{"location":"istio/ingress/#4-access-ingress-service","title":"4. Access ingress service","text":"<pre><code>curl -s -I -HHost:httpbin.example.com \"http://$INGRESS_HOST:$INGRESS_PORT/status/200\"\nHTTP/1.1 200 OK\nserver: istio-envoy\ndate: Mon, 25 Sep 2023 08:41:17 GMT\ncontent-type: text/html; charset=utf-8\naccess-control-allow-origin: *\naccess-control-allow-credentials: true\ncontent-length: 0\nx-envoy-upstream-service-time: 53\n</code></pre>"},{"location":"istio/ingress/#5-configuring-ingress-using-an-ingress-resource","title":"5. Configuring ingress using an Ingress resource","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress\nspec:\n  ingressClassName: istio\n  rules:\n  - host: httpbin.example.com\n    http:\n      paths:\n      - path: /status\n        pathType: Prefix\n        backend:\n          service:\n            name: httpbin\n            port:\n              number: 8000\nEOF\n</code></pre> <p>The <code>spec.ingressClassName</code> is required to tell the Istio gateway controller that it should handle this Ingress, otherwise it will be ignored.</p> <pre><code>curl -s -I -HHost:httpbin.example.com \"http://$INGRESS_HOST:$INGRESS_PORT/status/200\"\n</code></pre>"},{"location":"istio/ingress/#ref","title":"Ref","text":"<ol> <li>(ingress controller)<ol> <li>In order for the Ingress resource to work, the cluster must have an ingress controller running.</li> <li>Istio Ingress is an Istio based ingress controller.</li> </ol> </li> </ol>"},{"location":"knative/","title":"Knative","text":"<p>abstracting away the complex details and enabling developers to focus on what matters</p> <p>Enterprise-grade Serverless on your own terms. Kubernetes-based platform to deploy and manage modern serverless workloads.</p>"},{"location":"knative/#components","title":"Components","text":"<ol> <li>Serving</li> <li>Eventing</li> </ol>"},{"location":"knative/#concept","title":"Concept","text":"<p>Serving Resources</p> <ol> <li>Service: The <code>service.serving.knative.dev</code> resource automatically manages the whole lifecycle of your workload. It controls the creation of other objects to ensure that your app has a route, a configuration, and a new revision for each update of the service. Service can be defined to always route traffic to the latest revision or to a pinned revision.</li> <li>Route: The <code>route.serving.knative.dev</code> resource maps a network endpoint to one or more revisions. You can manage the traffic in several ways, including fractional traffic and named routes.</li> <li>Configuration: The <code>configuration.serving.knative.dev</code> resource maintains the desired state for your deployment. It provides a clean separation between code and configuration and follows the Twelve-Factor App methodology. Modifying a configuration creates a new revision.</li> <li>Revision: The <code>revision.serving.knative.dev</code> resource is a point-in-time snapshot of the code and configuration for each modification made to the workload. Revisions are immutable objects and can be retained for as long as useful. Knative Serving Revisions can be automatically scaled up and down according to incoming traffic. See Configuring the Autoscaler for more information.</li> </ol> <p></p>"},{"location":"knative/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Install Knative CLI <code>kn</code>.</p> <p>For Mac:</p> <pre><code>brew install kn\n</code></pre> </li> <li> <p>Install the Knative \"Quickstart\" plugin. (<code>kn</code> CLI's plugin.)</p> <p>\u203b Somehow brew fails. <pre><code>brew install knative-sandbox/kn-plugins/quickstart\n</code></pre> <p>-&gt; Follow this</p> <ol> <li>Download the binary</li> <li>Move it to <code>/usr/local/bin</code> <pre><code>mv ~/Downloads/kn-quickstart-darwin-amd64 /usr/local/bin/kn-quickstart\nchmod +x /usr/local/bin/kn-quickstart\n</code></pre></li> <li>Check plugin.     <pre><code>kn plugin list\n- kn-quickstart : /usr/local/bin/kn-quickstart\n</code></pre></li> </ol> <pre><code>kn quickstart kind\n</code></pre> <p>pods <pre><code>kubectl get po -A\nNAMESPACE            NAME                                            READY   STATUS    RESTARTS   AGE\nknative-eventing     eventing-controller-58875c5478-bhhcd            1/1     Running   0          56s\nknative-eventing     eventing-webhook-5968f79978-cswb4               1/1     Running   0          56s\nknative-eventing     imc-controller-86cd7b7857-ndx69                 1/1     Running   0          41s\nknative-eventing     imc-dispatcher-7fcb4b5d8c-rtjkz                 1/1     Running   0          41s\nknative-eventing     mt-broker-controller-8d979648f-nv4w4            1/1     Running   0          27s\nknative-eventing     mt-broker-filter-574dc4457f-x4lch               1/1     Running   0          27s\nknative-eventing     mt-broker-ingress-5ddd6f8b5d-llj4g              1/1     Running   0          27s\nknative-serving      activator-85bd4ddcbb-sdgkc                      1/1     Running   0          2m14s\nknative-serving      autoscaler-84fcdc5449-52rzj                     1/1     Running   0          2m14s\nknative-serving      controller-6fd5bb86df-zctsm                     1/1     Running   0          2m14s\nknative-serving      domain-mapping-74d5d688bd-k9stl                 1/1     Running   0          2m14s\nknative-serving      domainmapping-webhook-8484d5fd8b-6jb9n          1/1     Running   0          2m14s\nknative-serving      net-kourier-controller-66bc9d6697-czjbr         1/1     Running   0          97s\nknative-serving      webhook-97c648588-rwb6n                         1/1     Running   0          2m13s\nkourier-system       3scale-kourier-gateway-58856c6cc7-czpnz         1/1     Running   0          97s\nkube-system          coredns-78fcd69978-qdg6m                        1/1     Running   0          2m28s\nkube-system          coredns-78fcd69978-qj7xh                        1/1     Running   0          2m28s\nkube-system          etcd-knative-control-plane                      1/1     Running   0          2m43s\nkube-system          kindnet-h85z6                                   1/1     Running   0          2m28s\nkube-system          kube-apiserver-knative-control-plane            1/1     Running   0          2m42s\nkube-system          kube-controller-manager-knative-control-plane   1/1     Running   0          2m43s\nkube-system          kube-proxy-b9p56                                1/1     Running   0          2m28s\nkube-system          kube-scheduler-knative-control-plane            1/1     Running   0          2m43s\nlocal-path-storage   local-path-provisioner-85494db59d-99nt7         1/1     Running   0          2m28s\n</code></pre> <li> <p>Apply hello.yaml     <pre><code>kubectl apply -f hello.yaml\n</code></pre></p> </li> <li> <p>Check</p> <p>Knative service:</p> <pre><code>kn service list\n</code></pre> <pre><code>NAME    URL                                LATEST   AGE   CONDITIONS   READY     REASON\nhello   http://hello.default.example.com            36s   0 OK / 3     Unknown   RevisionMissing : Configuration \"hello\" is waiting for a Revision to become ready.\n</code></pre> <pre><code>kn service list\nNAME    URL                                LATEST        AGE   CONDITIONS   READY   REASON\nhello   http://hello.default.example.com   hello-world   37m   3 OK / 3     True\n</code></pre> <p>Check <code>Hello World</code>.</p> <pre><code>curl http://hello.default.127.0.0.1.sslip.io\n\nHello World!\n</code></pre> </li> <li> <p>Scaling to Zero</p> <p>It may take up to 2 minutes for your Pods to scale down. Pinging your service again will reset this timer.</p> <pre><code>kubectl get pod -l serving.knative.dev/service=hello\nNo resources found in default namespace.\n</code></pre> </li> <li> <p>Basics of Traffic Splitting</p> <ol> <li> <p>Creating a new Revision</p> <pre><code>kn service update hello \\\n--env TARGET=Knative \\\n--revision-name=knative\n</code></pre> <pre><code>Updating Service 'hello' in namespace 'default':\n\n  0.024s The Configuration is still working to reflect the latest desired specification.\n  3.181s Traffic is not yet migrated to the latest revision.\n  3.210s Ingress has not yet been reconciled.\n  3.242s Waiting for load balancer to be ready\n  3.419s Ready to serve.\n\nService 'hello' updated to latest revision 'hello-knative' is available at URL:\nhttp://hello.default.127.0.0.1.sslip.io\n</code></pre> </li> <li> <p>Check</p> <pre><code>curl http://hello.default.127.0.0.1.sslip.io\nHello Knative!\n</code></pre> </li> <li> <p>Splitting Traffic</p> <pre><code>kn revisions list\n\nNAME            SERVICE   TRAFFIC   TAGS   GENERATION   AGE     CONDITIONS   READY           REASON\nhello-knative   hello     100%             2            2m12s   3 OK / 4     True\nhello-world     hello                      1            120m    3 OK / 4     True\n</code></pre> <pre><code>kn service update hello \\\n--traffic hello-world=50 \\\n--traffic @latest=50\n</code></pre> <pre><code>kn revisions list\n\nNAME            SERVICE   TRAFFIC   TAGS   GENERATION   AGE    CONDITIONS   READY   REASON\nhello-knative   hello     50%              2            3m1s   3 OK / 4     True\nhello-world     hello     50%              1            121m   3 OK / 4     True\n</code></pre> </li> <li> <p>Check</p> <pre><code>\u00b1 curl http://hello.default.127.0.0.1.sslip.io\nHello Knative!\n\n\u00b1 curl http://hello.default.127.0.0.1.sslip.io\nHello Knative!\n\n\u00b1 curl http://hello.default.127.0.0.1.sslip.io\nHello Knative!\n\n\u00b1 curl http://hello.default.127.0.0.1.sslip.io\nHello World!\n</code></pre> </li> </ol> </li> <li> <p>Clean up</p> <pre><code>kn service delete hello\n</code></pre> </li> <li> <p>Clean up cluster</p> <pre><code>kind delete clusters knative\n</code></pre> </li>"},{"location":"kubeadm-local/","title":"Set up Kubernetes cluster with Kubeadm","text":""},{"location":"kubeadm-local/#prerequisite","title":"Prerequisite","text":"<ul> <li>OS X</li> <li>Virtualbox<ul> <li>https://www.virtualbox.org/wiki/Downloads</li> </ul> </li> <li>Vagrant<ul> <li>https://www.vagrantup.com/downloads</li> </ul> </li> </ul>"},{"location":"kubeadm-local/#references","title":"References","text":"<ul> <li>https://github.com/kodekloudhub/certified-kubernetes-administrator-course</li> <li>https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</li> </ul>"},{"location":"kubeadm-local/#steps","title":"Steps","text":"<ol> <li> <p>Clone repo</p> <pre><code>git clone git@github.com:kodekloudhub/certified-kubernetes-administrator-course.git\n</code></pre> </li> <li> <p>Prepare vagrant</p> <ol> <li> <p>Confirm everything is <code>not created</code></p> <pre><code>vagrant status\nCurrent machine states:\n\nkubemaster                not created (virtualbox)\nkubenode01                not created (virtualbox)\nkubenode02                not created (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> </li> <li> <p><code>vagrant up</code></p> <p>If you have a problem of virtualbox, you need to allow <code>Oracle</code> in <code>Preferences &gt; Security &amp; Privacy</code> https://qiita.com/akane_kato/items/c103332729e3d0ac39e6.</p> </li> <li> <p>Check</p> <p>Login to master</p> <pre><code>vagrant status\nvagrant ssh kubemaster\n</code></pre> </li> </ol> </li> <li> <p>Set up all nodes</p> <p>run the following commands in all nodes</p> <ol> <li> <p>Bridge network</p> <pre><code>sudo modprobe br_netfilter\n</code></pre> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\nbr_netfilter\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsudo sysctl --system\n</code></pre> </li> <li> <p>Install container runtime (Docker)</p> <p>https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y \\\napt-transport-https ca-certificates curl software-properties-common gnupg2\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key --keyring /etc/apt/trusted.gpg.d/docker.gpg add -\nsudo add-apt-repository \\\n\"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) \\\nstable\"\n</code></pre> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y \\\ncontainerd.io=1.2.13-2 \\\ndocker-ce=5:19.03.11~3-0~ubuntu-$(lsb_release -cs) \\\ndocker-ce-cli=5:19.03.11~3-0~ubuntu-$(lsb_release -cs)\nsudo mkdir /etc/docker\n</code></pre> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json\n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"],\n\"log-driver\": \"json-file\",\n\"log-opts\": {\n    \"max-size\": \"100m\"\n},\n\"storage-driver\": \"overlay2\"\n}\nEOF\nsudo mkdir -p /etc/systemd/system/docker.service.d\n</code></pre> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n</code></pre> </li> <li> <p>Install kubeadm, kubelet, and kubectl</p> <p>https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\ncat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre> </li> </ol> </li> <li> <p>Create a cluster with kubeadm (on master node)</p> <p>https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/</p> <ol> <li> <p>Get the ip address of master node</p> <pre><code>vagrant@kubemaster:~$ ifconfig enp0s8\nenp0s8: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.56.2  netmask 255.255.255.0  broadcast 192.168.56.255\n        inet6 fe80::a00:27ff:fe6e:f8b9  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether 08:00:27:6e:f8:b9  txqueuelen 1000  (Ethernet)\n        RX packets 1  bytes 86 (86.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 18  bytes 1356 (1.3 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n</code></pre> </li> <li> <p><code>kubeadm init</code></p> <pre><code>sudo -i\nkubeadm init --pod-network-cidr 10.244.0.0/16 --apiserver-advertise-address=192.168.56.2\n</code></pre> <pre><code>[init] Using Kubernetes version: v1.20.4\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubemaster kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.56.2]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [kubemaster localhost] and IPs [192.168.56.2 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [kubemaster localhost] and IPs [192.168.56.2 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[kubelet-check] Initial timeout of 40s passed.\n[apiclient] All control plane components are healthy after 84.502944 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.20\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node kubemaster as control-plane by adding the labels \"node-role.kubernetes.io/master=''\" and \"node-role.kubernetes.io/control-plane='' (deprecated)\"\n[mark-control-plane] Marking the node kubemaster as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[bootstrap-token] Using token: 5cptpm.66n574gszk32ginj\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.56.2:6443 --token 5cptpm.66n574gszk32ginj \\\n    --discovery-token-ca-cert-hash sha256:f202b95d32fdfcad22a7b83b2a040b7b7451a0b92c8b96cdb14a47d3cea1dd31\n</code></pre> </li> <li> <p>Prepare kubeconfig</p> <pre><code>logout\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>check</p> <pre><code>kubectl get nodes\nNAME         STATUS     ROLES                  AGE   VERSION\nkubemaster   NotReady   control-plane,master   79m   v1.20.4\n</code></pre> </li> <li> <p>Deploy a pod network (weave-net)</p> <p>https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network</p> <p><code>weave-net</code>: https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#install</p> <pre><code>kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\n</code></pre> <p>Check</p> <pre><code>kubectl get node\nNAME         STATUS   ROLES                  AGE   VERSION\nkubemaster   Ready    control-plane,master   84m   v1.20.4\n</code></pre> </li> </ol> </li> <li> <p>Let <code>node01</code> and <code>node02</code> join the cluster</p> <ol> <li>Login to <code>node01</code> and <code>node02</code></li> <li>Become root user by <code>sudo -i</code></li> <li> <p>Join the cluster     <pre><code>kubeadm join 192.168.56.2:6443 --token 5cptpm.66n574gszk32ginj \\\n    --discovery-token-ca-cert-hash sha256:f202b95d32fdfcad22a7b83b2a040b7b7451a0b92c8b96cdb14a47d3cea1dd31\n</code></pre></p> <pre><code>root@kubenode01:~#         kubeadm join 192.168.56.2:6443 --token 5cptpm.66n574gszk32ginj \\\n&gt;             --discovery-token-ca-cert-hash sha256:f202b95d32fdfcad22a7b83b2a040b7b7451a0b92c8b96cdb14a47d3cea1dd31\n[preflight] Running pre-flight checks\n[preflight] Reading configuration from the cluster...\n[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Starting the kubelet\n[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\n\nThis node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun 'kubectl get nodes' on the control-plane to see this node join the cluster.\n</code></pre> </li> <li> <p>Check on master node</p> <pre><code>vagrant@kubemaster:~$ kubectl get node\nNAME         STATUS   ROLES                  AGE   VERSION\nkubemaster   Ready    control-plane,master   86m   v1.20.4\nkubenode01   Ready    &lt;none&gt;                 61s   v1.20.4\nkubenode02   Ready    &lt;none&gt;                 61s   v1.20.4\n</code></pre> </li> </ol> </li> <li> <p>Confirm the cluster is working</p> <pre><code>kubectl run nginx --image nginx\npod/nginx created\n</code></pre> <pre><code>kubectl get po -o wide\nNAME    READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES\nnginx   1/1     Running   0          90s   10.36.0.1   kubenode02   &lt;none&gt;           &lt;none&gt;\n</code></pre> <pre><code>kubectl delete pod nginx\npod \"nginx\" deleted\n</code></pre> </li> <li> <p>Clean up vagrant</p> <pre><code>vagrant destroy\n    kubenode02: Are you sure you want to destroy the 'kubenode02' VM? [y/N] y\n==&gt; kubenode02: Forcing shutdown of VM...\n==&gt; kubenode02: Destroying VM and associated drives...\n    kubenode01: Are you sure you want to destroy the 'kubenode01' VM? [y/N] y\n==&gt; kubenode01: Forcing shutdown of VM...\n==&gt; kubenode01: Destroying VM and associated drives...\n    kubemaster: Are you sure you want to destroy the 'kubemaster' VM? [y/N] y\n==&gt; kubemaster: Forcing shutdown of VM...\n==&gt; kubemaster: Destroying VM and associated drives...\n</code></pre> <pre><code>vagrant status\nCurrent machine states:\n\nkubemaster                not created (virtualbox)\nkubenode01                not created (virtualbox)\nkubenode02                not created (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> </li> </ol>"},{"location":"kubernetes-components/","title":"Kubernetes Components","text":"<p>Ref: https://kubernetes.io/docs/concepts/overview/components</p> <ul> <li>etcd</li> <li>kubernetes-scheduler</li> <li>kube-apiserver</li> <li>cloud-controller-manager</li> <li>kube-controller-manager</li> <li>kube-proxy</li> <li>kubelet</li> <li>kubectl</li> </ul>"},{"location":"kubernetes-components/#build-kubernetes","title":"Build Kubernetes","text":"<pre><code>git clone https://github.com/kubernetes/kubernetes\ncd kubernetes\n</code></pre> <p>With <code>Golang</code>: <pre><code>make\n</code></pre></p>"},{"location":"kubernetes-components/#tips","title":"Tips","text":""},{"location":"kubernetes-components/#change-log-level-of-kubernetes-component-kind","title":"Change log level of Kubernetes component (kind)","text":"<ol> <li>Create <code>kind</code> cluster.     <pre><code>kind create cluster\n</code></pre></li> <li> <p>Create role and rolebinding and attach it to the <code>default</code> service account:</p> <p><pre><code>cat &lt;&lt; EOT | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: edit-debug-flags-v\nrules:\n  # only for kubelet\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/proxy\n  verbs:\n  - update\n  # enough for other component\n- nonResourceURLs:\n  - /debug/flags/v\n  verbs:\n  - put\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: edit-debug-flags-v\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit-debug-flags-v\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\nEOT\n</code></pre> 1. Generate token <pre><code>TOKEN=$(kubectl create token default)\n</code></pre> 1. Change Log Level 1. API Server     <pre><code>APISERVER=$(kubectl config view -o jsonpath=\"{.clusters[?(@.name==\\\"kind-kind\\\")].cluster.server}\")\ncurl -s -X PUT -d '5' $APISERVER/debug/flags/v --header \"Authorization: Bearer $TOKEN\" -k\n</code></pre> 1. kube-scheduler     <pre><code>kubectl -n kube-system port-forward kube-scheduler-kind-control-plane 10259:10259\n</code></pre></p> <pre><code>```\ncurl -s -X PUT -d '5' https://localhost:10259/debug/flags/v --header \"Authorization: Bearer $TOKEN\" -k\n```\n</code></pre> <ol> <li> <p>kubelet     <code>docker exec kind-control-plane curl -s -X PUT -d '5' https://localhost:10250/debug/flags/v --header \"Authorization: Bearer $TOKEN\" -k</code></p> <p>You might see the following warning:</p> <pre><code>Warning: resource configmaps/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\n</code></pre> </li> <li> <p>kube-proxy</p> <p>set <code>enableProfiling</code> to true</p> <pre><code>kubectl -n kube-system get configmap kube-proxy -o yaml  | sed -e 's/enableProfiling: false/enableProfiling: true/' | kubectl apply -f -\n</code></pre> <p>restart kube-proxy</p> <pre><code>kubectl -n kube-system rollout restart daemonset/kube-proxy\n</code></pre> <p>port-forward:</p> <pre><code>kubectl port-forward -n kube-system $(kubectl get pod -n kube-system -l k8s-app=kube-proxy -o jsonpath='{.items[0].metadata.name}') 10249:10249\n</code></pre> <pre><code>curl -s -XPUT -d '5' http://localhost:10249/debug/flags/v\n</code></pre> </li> <li> <p>kube-controller-manager (enabled by kubernetes/kubernetes#104571) 10257</p> <pre><code>kubectl -n kube-system port-forward kube-controller-manager-kind-control-plane 10257:10257\n</code></pre> <pre><code>curl -s -X PUT -d '5' https://localhost:10257/debug/flags/v --header \"Authorization: Bearer $TOKEN\" -k\n</code></pre> </li> </ol> </li> </ol>"},{"location":"kubernetes-components/#references","title":"References","text":"<ol> <li>99% to 99.9% SLO: High Performance Kubernetes Control Plane at Pinterest</li> <li>Kubernetes\u306e\u4e3b\u8981\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u30ed\u30b0\u30ec\u30d9\u30eb\u3092\u52d5\u7684\u306b\u5909\u66f4\u3059\u308b + Kubernetes v1.24\u3067ServiceAccount\u306e\u30c8\u30fc\u30af\u30f3\u3092\u751f\u6210\u30fb\u53d6\u5f97\u3059\u308b</li> </ol>"},{"location":"kubernetes-components/cloud-controller-manager/","title":"Cloud Controller Manager","text":"<p>https://github.com/kubernetes/cloud-provider/tree/master/controllers</p> <ol> <li>node_controller</li> <li>nodelifecycle_controller</li> <li>route_controller</li> <li>service_controller</li> </ol> <p>References:</p> <ul> <li><code>cloud-controller-manager</code> moved to <code>k8s.io/cloud-provider</code> in PR#95748 to resolve Issue#29</li> <li>https://bells17.medium.com/kubernetes-ccm-d4d3c71ba523</li> </ul>"},{"location":"kubernetes-components/etcd/","title":"etcd","text":""},{"location":"kubernetes-components/etcd/#install","title":"Install","text":"<pre><code>brew install etcd\n</code></pre> <pre><code>etcd --version\netcd Version: 3.5.7\nGit SHA: 215b53cf3\nGo Version: go1.19.5\nGo OS/Arch: darwin/arm64\n</code></pre>"},{"location":"kubernetes-components/etcd/#quickstart","title":"Quickstart","text":"<ol> <li> <p>Start etcd.</p> <p><pre><code>etcd\n</code></pre> 1. Set a key <code>greeting</code> and a value <code>Hello, etcd</code>. <pre><code>etcdctl put greeting \"Hello, etcd\"\nOK\n</code></pre> 1. Get the value for the key <code>greeting</code> <pre><code>etcdctl get greeting\ngreeting\nHello, etcd\n</code></pre></p> </li> </ol>"},{"location":"kubernetes-components/etcd/#kubernetes-objects-in-etcd","title":"Kubernetes objects in etcd","text":"<p>Kubernetes objects are stored under the <code>/registry</code> path</p> <p>You can see all the keys under <code>/registry</code> with the following command:</p> <pre><code>etcdctl get /registry/ --prefix --keys-only\n</code></pre> <pre><code>/registry/apiregistration.k8s.io/apiservices/v1.\n\n/registry/apiregistration.k8s.io/apiservices/v1.admissionregistration.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.apiextensions.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.apps\n\n/registry/apiregistration.k8s.io/apiservices/v1.authentication.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.authorization.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.autoscaling\n\n/registry/apiregistration.k8s.io/apiservices/v1.batch\n\n/registry/apiregistration.k8s.io/apiservices/v1.certificates.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.coordination.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.discovery.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.events.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.networking.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.node.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.policy\n\n/registry/apiregistration.k8s.io/apiservices/v1.rbac.authorization.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.scheduling.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1.storage.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1beta1.storage.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1beta2.flowcontrol.apiserver.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v1beta3.flowcontrol.apiserver.k8s.io\n\n/registry/apiregistration.k8s.io/apiservices/v2.autoscaling\n\n/registry/configmaps/kube-system/extension-apiserver-authentication\n\n/registry/endpointslices/default/kubernetes\n\n/registry/flowschemas/catch-all\n\n/registry/flowschemas/endpoint-controller\n\n/registry/flowschemas/exempt\n\n/registry/flowschemas/global-default\n\n/registry/flowschemas/kube-controller-manager\n\n/registry/flowschemas/kube-scheduler\n\n/registry/flowschemas/kube-system-service-accounts\n\n/registry/flowschemas/probes\n\n/registry/flowschemas/service-accounts\n\n/registry/flowschemas/system-leader-election\n\n/registry/flowschemas/system-node-high\n\n/registry/flowschemas/system-nodes\n\n/registry/flowschemas/workload-leader-election\n\n/registry/leases/kube-system/kube-apiserver-c7xw4gapfj47r73yd6xe7yiqea\n\n/registry/masterleases/192.168.10.33\n\n/registry/namespaces/default\n\n/registry/namespaces/kube-node-lease\n\n/registry/namespaces/kube-public\n\n/registry/namespaces/kube-system\n\n/registry/pods/default/nginx\n\n/registry/priorityclasses/system-cluster-critical\n\n/registry/priorityclasses/system-node-critical\n\n/registry/prioritylevelconfigurations/catch-all\n\n/registry/prioritylevelconfigurations/exempt\n\n/registry/prioritylevelconfigurations/global-default\n\n/registry/prioritylevelconfigurations/leader-election\n\n/registry/prioritylevelconfigurations/node-high\n\n/registry/prioritylevelconfigurations/system\n\n/registry/prioritylevelconfigurations/workload-high\n\n/registry/prioritylevelconfigurations/workload-low\n\n/registry/ranges/serviceips\n\n/registry/ranges/servicenodeports\n\n/registry/serviceaccounts/default/default\n\n/registry/services/endpoints/default/kubernetes\n\n/registry/services/specs/default/kubernetes\n</code></pre> <p>You can also check a specific object e.g. nginx Pod</p> <pre><code>etcdctl get /registry/pods/default/nginx\n</code></pre> <pre><code>/registry/pods/default/nginx\nk8s\n\nv1Pod\ufffd\n\ufffd\nnginxdefault\"*$a77f3131-9ce0-4319-a7c2-ea859df720212\ufffd\ufffd\ufffd\ufffdZ\n\nrunnginx\ufffd\ufffd\n\nkubectl-runUpdatev\ufffd\ufffd\ufffd\ufffdFieldsV1:\ufffd\n\ufffd{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:run\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"nginx\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:enableServiceLinks\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}B\ufffd\n\ufffd\nkube-api-access-2f85qk\ufffdh\n\"\n\n\ufffdtoken\n(&amp;\n\nkube-root-ca.crt\nca.crtca.crt\n)'\n%\n        namespace\nv1metadata.namespace\ufffd\ufffd\nnginxnginx*BJL\nkube-api-access-2f85q-/var/run/secrets/kubernetes.io/serviceaccount\"2j/dev/termination-logrAlways\ufffd\ufffd\ufffd\ufffdFileAlways 2\n                                                        ClusterFirstBdefaultJdefaultRX`hr\ufffd\ufffd\ufffddefault-scheduler\ufffd6\nnode.kubernetes.io/not-readyExists\"     NoExecute(\ufffd\ufffd8\nnode.kubernetes.io/unreachableExists\"   NoExecute(\ufffd\ufffd\ufffd\ufffd\ufffdPreemptLowerPriority\nPending\"*2J\nBestEffortZ\"\n</code></pre> <p>However, the data is encoded. You can decode with https://github.com/jpbetz/auger</p> <pre><code>git clone https://github.com/jpbetz/auger &amp;&amp; cd auger\ngo build -o anger main.go\n</code></pre> <pre><code>etcdctl get /registry/pods/default/nginx | ~/repos/jpbetz/auger/anger decode\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: \"2023-03-25T00:21:26Z\"\n  labels:\n    run: nginx\n  name: nginx\n  namespace: default\n  uid: a77f3131-9ce0-4319-a7c2-ea859df72021\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: Always\n    name: nginx\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-2f85q\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - name: kube-api-access-2f85q\n    projected:\n      defaultMode: 420\n      sources:\n      - {}\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\nstatus:\n  phase: Pending\n  qosClass: BestEffort\n</code></pre> <p>For more details about encoding in Kubernetes, you can read Kubernetes Protobuf encoding</p>"},{"location":"kubernetes-components/etcd/#reference","title":"Reference","text":"<ul> <li>https://etcd.io</li> <li>https://technekey.com/check-whats-inside-the-etcd-database-in-kubernetes/</li> </ul>"},{"location":"kubernetes-components/kube-apiserver/","title":"kube-apiserver","text":""},{"location":"kubernetes-components/kube-apiserver/#overview","title":"Overview","text":"<p>Components:</p> <ol> <li>API Extensions Server: Create HTTP handlers for CRD.</li> <li>API Server: Manage core API and core Kubernetes components.</li> <li>Aggregator Layer: Proxy the requests sent to the registered extended resource to the extension API server that runs in a Pod in the same cluster.</li> </ol>"},{"location":"kubernetes-components/kube-apiserver/#kube-apiserver_1","title":"kube-apiserver","text":""},{"location":"kubernetes-components/kube-apiserver/#auditing","title":"Auditing","text":"<p>https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/</p> example <pre><code>apiVersion: audit.k8s.io/v1 # This is required.\nkind: Policy\n# Don't generate audit events for all requests in RequestReceived stage.\nomitStages:\n  - \"RequestReceived\"\nrules:\n  # Log pod changes at RequestResponse level\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      # Resource \"pods\" doesn't match requests to any subresource of pods,\n      # which is consistent with the RBAC policy.\n      resources: [\"pods\"]\n  # Log \"pods/log\", \"pods/status\" at Metadata level\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"pods/log\", \"pods/status\"]\n\n  # Don't log requests to a configmap called \"controller-leader\"\n  - level: None\n    resources:\n    - group: \"\"\n      resources: [\"configmaps\"]\n      resourceNames: [\"controller-leader\"]\n\n  # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services\n  - level: None\n    users: [\"system:kube-proxy\"]\n    verbs: [\"watch\"]\n    resources:\n    - group: \"\" # core API group\n      resources: [\"endpoints\", \"services\"]\n\n  # Don't log authenticated requests to certain non-resource URL paths.\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\" # Wildcard matching.\n    - \"/version\"\n\n  # Log the request body of configmap changes in kube-system.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n      resources: [\"configmaps\"]\n    # This rule only applies to resources in the \"kube-system\" namespace.\n    # The empty string \"\" can be used to select non-namespaced resources.\n    namespaces: [\"kube-system\"]\n\n  # Log configmap and secret changes in all other namespaces at the Metadata level.\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n\n  # Log all other resources in core and extensions at the Request level.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n    - group: \"extensions\" # Version of group should NOT be included.\n\n  # A catch-all rule to log all other requests at the Metadata level.\n  - level: Metadata\n    # Long-running requests like watches that fall under this rule will not\n    # generate an audit event in RequestReceived.\n    omitStages:\n      - \"RequestReceived\"\n</code></pre> <pre><code>--audit-policy-file=/etc/kubernetes/audit-policy.yaml \\\n--audit-log-path=/var/log/kubernetes/audit/audit.log\n</code></pre>"},{"location":"kubernetes-components/kube-apiserver/#run-kube-apiserver-in-local","title":"Run kube-apiserver in local","text":""},{"location":"kubernetes-components/kube-apiserver/#prerequisite","title":"Prerequisite","text":"<ol> <li> <p>Bash version 4 or later     Mac: <code>brew install bash</code></p> <p>version <pre><code>bash --version\nGNU bash, version 3.2.57(1)-release (arm64-apple-darwin21)\nCopyright (C) 2007 Free Software Foundation, Inc.\n</code></pre> <li> <p>Openssl: <code>LibreSSL</code> is also ok. (<code>brew install openssl</code> &lt;- this should also work.)</p> <p>version <pre><code>openssl version\nOpenSSL 3.1.0 14 Mar 2023 (Library: OpenSSL 3.1.0 14 Mar 2023)\n</code></pre> <li> <p>etcd: <code>brew install etcd</code></p> <p>version <pre><code>etcd --version\netcd Version: 3.5.7\nGit SHA: 215b53cf3\nGo Version: go1.19.5\nGo OS/Arch: darwin/arm64\n</code></pre>"},{"location":"kubernetes-components/kube-apiserver/#steps","title":"Steps","text":"<ol> <li>Build Kubernetes binary (ref: Build Kubernetes).<ol> <li>Clone Kubernetes repo.     <pre><code>git clone https://github.com/kubernetes/kubernetes\n</code></pre></li> <li>Build the version you want to use.     <pre><code>git checkout release-1.26 # you can choose any version\nmake\n</code></pre></li> </ol> </li> <li> <p>Run <code>etcd</code>. (ref: etcd)</p> <pre><code>etcd\n</code></pre> </li> <li> <p>Create certificates.     <pre><code>./generate_certificate.sh\n</code></pre></p> <p>manual steps <ol> <li> <p>Create certificates for <code>service-account</code>.</p> <pre><code>openssl genrsa -out service-account-key.pem 4096\nopenssl req -new -x509 -days 365 -key service-account-key.pem -subj \"/CN=test\" -sha256 -out service-account.pem\n</code></pre> </li> <li> <p>Create certificate for <code>apiserver</code>.</p> <ol> <li>Generate a <code>ca.key</code> with 2048bit:     <pre><code>openssl genrsa -out ca.key 2048\n</code></pre></li> <li>According to the <code>ca.key</code> generate a <code>ca.crt</code> (use -days to set the certificate effective time):     <pre><code>openssl req -x509 -new -nodes -key ca.key -subj \"/CN=127.0.0.1\" -days 10000 -out ca.crt\n</code></pre></li> <li><code>server.key</code> <pre><code>openssl genrsa -out server.key 2048\n</code></pre></li> <li><code>csr.conf</code></li> <li>generate certificate signing request (<code>server.csr</code>)     <pre><code>openssl req -new -key server.key -out server.csr -config csr.conf\n</code></pre></li> <li>generate server certificate <code>server.crt</code> using <code>ca.key</code>, <code>ca.crt</code> and <code>server.csr</code>.     <pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\\n-CAcreateserial -out server.crt -days 10000 \\\n-extensions v3_ext -extfile csr.conf\n</code></pre></li> </ol> </li> </ol> <p>For more details, please check Generate Certificates Manually</p> <li> <p>Run the built binary.</p> <p>Set the path:</p> <pre><code>PATH_TO_KUBERNETES_DIR=~/repos/kubernetes/kubernetes\n</code></pre> <p>Check the API server's version:</p> <pre><code>${PATH_TO_KUBERNETES_DIR}/_output/bin/kube-apiserver --version\n</code></pre> <pre><code>Kubernetes v1.26.3-11+9043dd888deae0\n</code></pre> <p>Start API server:</p> <pre><code>${PATH_TO_KUBERNETES_DIR}/_output/bin/kube-apiserver --etcd-servers http://localhost:2379 \\\n--service-account-key-file=service-account-key.pem \\\n--service-account-signing-key-file=service-account-key.pem \\\n--service-account-issuer=api \\\n--tls-cert-file=server.crt \\\n--tls-private-key-file=server.key \\\n--client-ca-file=ca.crt\n</code></pre> </li> <li> <p>Configure <code>kubeconfig</code>. (You can skip this step by running <code>./generate_certificate.sh</code>)</p> <p>(I'm too lazy to generate crt and key for kubectl. So used the same one as server here.)</p> <pre><code>kubectl config set-cluster local-apiserver \\\n--certificate-authority=ca.crt \\\n--embed-certs=true \\\n--server=https://127.0.0.1:6443 \\\n--kubeconfig=kubeconfig\n\nkubectl config set-credentials admin \\\n--client-certificate=server.crt \\\n--client-key=server.key \\\n--embed-certs=true \\\n--kubeconfig=kubeconfig\n\nkubectl config set-context default \\\n--cluster=local-apiserver \\\n--user=admin \\\n--kubeconfig=kubeconfig\n\nkubectl config use-context default --kubeconfig=kubeconfig\n</code></pre> </li> <li> <p>Check component status. (only <code>etcd</code> is healthy.)     <pre><code>kubectl get componentstatuses --kubeconfig kubeconfig\n\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS      MESSAGE                                                                                        ERROR\nscheduler            Unhealthy   Get \"https://127.0.0.1:10259/healthz\": dial tcp 127.0.0.1:10259: connect: connection refused\ncontroller-manager   Unhealthy   Get \"https://127.0.0.1:10257/healthz\": dial tcp 127.0.0.1:10257: connect: connection refused\netcd-0               Healthy     {\"health\":\"true\",\"reason\":\"\"}\n</code></pre></p> </li> <li>Create service account <code>default</code> <pre><code>kubectl create sa default --kubeconfig kubeconfig\n</code></pre></li> <li> <p>Create a Pod     <pre><code>kubectl run nginx --image nginx --kubeconfig kubeconfig\npod/nginx created\n</code></pre></p> <p>A new pod is created but will always remain Pending, as we don't have kubelet to start a container. <pre><code>kubectl get pod --kubeconfig kubeconfig\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   0/1     Pending   0          41s\n</code></pre></p> </li> <li> <p>Read the data from etcd</p> <pre><code>etcdctl get /registry/pods/default/nginx\n</code></pre> <pre><code>etcdctl get /registry/pods/default/nginx\n/registry/pods/default/nginx\nk8s\n\nv1Pod\ufffd\n\ufffd\nnginxdefault\"*$a77f3131-9ce0-4319-a7c2-ea859df720212\ufffd\ufffd\ufffd\ufffdZ\n\nrunnginx\ufffd\ufffd\n\nkubectl-runUpdatev\ufffd\ufffd\ufffd\ufffdFieldsV1:\ufffd\n\ufffd{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:run\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"nginx\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:enableServiceLinks\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}B\ufffd\n\ufffd\nkube-api-access-2f85qk\ufffdh\n\"\n\n\ufffdtoken\n(&amp;\n\nkube-root-ca.crt\nca.crtca.crt\n)'\n%\n        namespace\nv1metadata.namespace\ufffd\ufffd\nnginxnginx*BJL\nkube-api-access-2f85q-/var/run/secrets/kubernetes.io/serviceaccount\"2j/dev/termination-logrAlways\ufffd\ufffd\ufffd\ufffdFileAlways 2\n                                                        ClusterFirstBdefaultJdefaultRX`hr\ufffd\ufffd\ufffddefault-scheduler\ufffd6\nnode.kubernetes.io/not-readyExists\"     NoExecute(\ufffd\ufffd8\nnode.kubernetes.io/unreachableExists\"   NoExecute(\ufffd\ufffd\ufffd\ufffd\ufffdPreemptLowerPriority\nPending\"*2J\nBestEffortZ\"\n</code></pre> <p>You can decode with https://github.com/jpbetz/auger.</p> <p>Clone and build</p> <pre><code>AUGER_DIR=~/repos/jpbetz/auger\nmkdir -p $AUGER_DIR\ngit clone https://github.com/jpbetz/auger $AUGER_DIR &amp;&amp; cd $AUGER_DIR\ngo build -o anger main.go\n</code></pre> <pre><code>etcdctl get /registry/pods/default/nginx | $AUGER_DIR/anger decode\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: \"2023-03-25T00:21:26Z\"\n  labels:\n    run: nginx\n  name: nginx\n  namespace: default\n  uid: a77f3131-9ce0-4319-a7c2-ea859df72021\nspec:\n  containers:\n  - image: nginx\n    imagePullPolicy: Always\n    name: nginx\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-2f85q\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - name: kube-api-access-2f85q\n    projected:\n      defaultMode: 420\n      sources:\n      - {}\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\nstatus:\n  phase: Pending\n  qosClass: BestEffort\n</code></pre> </li> <li> <p>Cleanup     <pre><code>kubectl delete pod nginx --kubeconfig kubeconfig\n</code></pre></p> </li>"},{"location":"kubernetes-components/kube-apiserver/#errors","title":"Errors","text":"<ol> <li> <p>Error1: mkdir /var/run/kubernetes: permission denied</p> <pre><code>E0302 06:40:09.767084   37385 run.go:74] \"command failed\" err=\"error creating self-signed certificates: mkdir /var/run/kubernetes: permission denied\"\n</code></pre> <p>Run <pre><code>sudo mkdir /var/run/kubernetes\nchown -R `whoami` /var/run/kubernetes\n</code></pre></p> </li> <li> <p>Error2: service-account-issuer is a required flag, --service-account-signing-key-file and --service-account-issuer are required flags</p> <pre><code>E0302 07:14:46.234431   79468 run.go:74] \"command failed\" err=\"[service-account-issuer is a required flag, --service-account-signing-key-file and --service-account-issuer are required flags]\"\n</code></pre> <p><code>BoundServiceAccountTokenVolume</code> is now GA from 1.22. Need to pass <code>--service-account-signing-key-file</code> and <code>--service-account-issuer</code>.</p> </li> </ol>"},{"location":"kubernetes-components/kube-apiserver/#apiextensions-apiserver","title":"apiextensions-apiserver","text":"<p>It provides an API for registering <code>CustomResourceDefinitions</code>.</p> <p>When creating CRD: 1. Store CRD resource. 1. Validate the CRD with several controllers. 1. CRD handler automatically creates HTTP handler for the CRD.</p> <p>When deleting CRD: 1. Wait until <code>finalizingController</code> deletes all the custom resources.</p> <ul> <li>NewCustomResourceDefinitionHandler is called in CompletedConfig.New</li> <li>CompletedConfig.New<ol> <li>Prepare genericServer with completedConfig.New.</li> <li>Initialize <code>CustomResourceDefinitions</code> with <code>GenericAPIServer</code>.</li> <li>Initialize <code>apiGroupInfo</code> with genericapiserver.NewDefaultAPIGroupInfo.</li> <li>Install API group with <code>s.GenericAPIServer.InstallAPIGroup</code>.</li> <li>Initialize clientset for CRD with <code>crdClient, err := clientset.NewForConfig(s.GenericAPIServer.LoopbackClientConfig)</code></li> <li>Initialize and set informer with <code>s.Informers = externalinformers.NewSharedInformerFactory(crdClient, 5*time.Minute)</code></li> <li>Prepare handlers<ol> <li>delegateHandler</li> <li>versionDiscoveryHandler</li> <li>groupDiscoveryHandler</li> </ol> </li> <li>Initialize <code>EstablishingController</code>.</li> <li>Initialize <code>crdHandler</code> by <code>NewCustomResourceDefinitionHandler</code> with <code>versionDiscoveryHandler</code>, <code>groupDiscoveryHandler</code>, informer, <code>delegateHandler</code>, <code>establishingController</code>, etc.</li> <li>Set HTTP handler for GenericAPIServer with <code>crdHandler</code>.     <pre><code>s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(\"/apis\", crdHandler)\ns.GenericAPIServer.Handler.NonGoRestfulMux.HandlePrefix(\"/apis/\", crdHandler)\n</code></pre></li> <li>Initialize controllers.<ul> <li>discoveryController</li> <li>namingController</li> <li>nonStructuralSchemaController</li> <li>apiApprovalController</li> <li>finalizingController</li> <li>openapicontroller</li> </ul> </li> <li>Set <code>AddPostStartHookOrDie</code> for <code>GenericAPIServer</code> to start informer.</li> <li>Set <code>AddPostStartHookOrDie</code> for <code>GenericAPIServer</code> to start controllers.</li> <li>Set <code>AddPostStartHookOrDie</code> for <code>GenericAPIServer</code> to wait until CRD informer is synced.</li> </ol> </li> </ul>"},{"location":"kubernetes-components/kube-apiserver/#functions","title":"Functions","text":""},{"location":"kubernetes-components/kube-apiserver/#delete","title":"Delete","text":"<p>Return value:</p> <ol> <li>runtime.Object</li> <li>bool</li> <li>error</li> </ol> <p>Steps:</p> <ol> <li>Get key</li> <li>Get obj from the storage</li> <li>BeforeDelete: responsible for setting <code>deletionTimestamp</code>.<ol> <li>The return value is<ol> <li><code>graceful</code> (bool)</li> <li><code>gracefulPending</code> (bool)</li> <li><code>err</code> (error)</li> </ol> </li> <li>Case1: if not deleting gracefully -&gt; <code>false</code>, <code>false</code>, <code>nil</code></li> <li>Case2: Update DeletionTimestamp &amp; DeletionGracePeriodSeconds if necessary</li> <li>Case3: if <code>gracefulStrategy.CheckGracefulDelete</code> is false -&gt; <code>false</code>, <code>false</code>, <code>nil</code></li> </ol> </li> <li>(If <code>pendingGraceful</code> is true, finalizeDelete. &lt;- this function doesn't modify object.)</li> <li> <p>deletionFinalizersForGarbageCollection <pre><code>shouldUpdateFinalizers, _ := deletionFinalizersForGarbageCollection(ctx, e, accessor, options)\n</code></pre></p> <ol> <li><code>deletionFinalizersForGarbageCollection</code>: Remove <code>orphan</code> and <code>foreground</code> finalizers if <code>shouldOrphanDependents</code> and <code>shouldDeleteDependents</code> are not true respectively. if finalizers are updated, return <code>false</code>, otherwise <code>true</code>.</li> </ol> </li> <li> <p>Update <code>deleteImmediately</code>:</p> <ol> <li>if there's pending finalizers -&gt; <code>false</code><ol> <li>markAsDeleting sets the obj's DeletionGracePeriodSeconds to 0, and sets the DeletionTimestamp to \"now\"</li> </ol> </li> <li>if GracePeriodSeconds &gt; 0 -&gt; <code>false</code></li> <li>not pendingGraceful and not graceful -&gt; <code>true</code></li> </ol> <p><pre><code>if graceful || pendingFinalizers || shouldUpdateFinalizers {\n    err, ignoreNotFound, deleteImmediately, out, lastExisting = e.updateForGracefulDeletionAndFinalizers(ctx, name, key, options, preconditions, deleteValidation, obj)\n    // Update the preconditions.ResourceVersion if set since we updated the object.\n    if err == nil &amp;&amp; deleteImmediately &amp;&amp; preconditions.ResourceVersion != nil {\n        accessor, err = meta.Accessor(out)\n        if err != nil {\n            return out, false, apierrors.NewInternalError(err)\n        }\n        resourceVersion := accessor.GetResourceVersion()\n        preconditions.ResourceVersion = &amp;resourceVersion\n    }\n}\n</code></pre> 1. <code>updateForGracefulDeletionAndFinalizers</code> 1. If <code>deleteImmediately</code> is false or if there's err, return. (not delete immediately) 1. If Dry-run, return. 1. Finally, Delete the obj from the storage. <pre><code>err := e.Storage.Delete(ctx, key, out, &amp;preconditions, storage.ValidateObjectFunc(deleteValidation), dryrun.IsDryRun(options.DryRun), nil);\n</code></pre> 1. <code>finalizeDelete</code></p> </li> </ol>"},{"location":"kubernetes-components/kube-apiserver/#references","title":"References","text":"<ul> <li>Feature Gates</li> <li>kube-apiserver fails init. receive \"--service-account-signing-key-file and --service-account-issuer are required flag\" #626</li> <li>Kubernetes The Hard Way On VirtualBox 6\u65e5\u76ee</li> <li>Certificates</li> <li>Generating Kubernetes Configuration Files for Authentication</li> <li>OpenSSL\u30b3\u30de\u30f3\u30c9\u306e\u5099\u5fd8\u9332</li> </ul>"},{"location":"kubernetes-components/kube-controller-manager/","title":"kube-controller-manager","text":""},{"location":"kubernetes-components/kube-controller-manager/#1-kube-controller-manager","title":"1. Kube Controller Manager","text":"<p>kube-controller-manager runs built-in controllers.</p> <p>Entrypoint:</p> <ol> <li>controller-manager.go starts kube-controller-manager with only three lines.     <pre><code>func main() {\n    command := app.NewControllerManagerCommand()\n    code := cli.Run(command)\n    os.Exit(code)\n}\n</code></pre></li> <li>NewControllerManagerCommand returns a cobra.Command</li> <li>NewControllerInitializers defines a set of controllers to start in the controller manager. Specifically, the following controllers are registered with the <code>startXXXController</code> with type <code>InitFunc</code> <pre><code>type InitFunc func(ctx context.Context, controllerCtx ControllerContext) (controller controller.Interface, enabled bool, err error)\n</code></pre> <pre><code>register(\"endpoint\", startEndpointController)\nregister(\"endpointslice\", startEndpointSliceController)\nregister(\"endpointslicemirroring\", startEndpointSliceMirroringController)\nregister(\"replicationcontroller\", startReplicationController)\nregister(\"podgc\", startPodGCController)\nregister(\"resourcequota\", startResourceQuotaController)\nregister(\"namespace\", startNamespaceController)\nregister(\"serviceaccount\", startServiceAccountController)\nregister(\"garbagecollector\", startGarbageCollectorController)\nregister(\"daemonset\", startDaemonSetController)\nregister(\"job\", startJobController)\nregister(\"deployment\", startDeploymentController)\nregister(\"replicaset\", startReplicaSetController)\nregister(\"horizontalpodautoscaling\", startHPAController)\nregister(\"disruption\", startDisruptionController)\nregister(\"statefulset\", startStatefulSetController)\nregister(\"cronjob\", startCronJobController)\nregister(\"csrsigning\", startCSRSigningController)\nregister(\"csrapproving\", startCSRApprovingController)\nregister(\"csrcleaner\", startCSRCleanerController)\nregister(\"ttl\", startTTLController)\nregister(\"bootstrapsigner\", startBootstrapSignerController)\nregister(\"tokencleaner\", startTokenCleanerController)\nregister(\"nodeipam\", startNodeIpamController)\nregister(\"nodelifecycle\", startNodeLifecycleController)\n</code></pre></li> <li>The core function Run calls StartControllers to start the controllers specified by <code>controllers</code> (<code>map[string]InitFunc{}</code> defined in the previous step)</li> </ol> <p>Each Controller:</p> <ol> <li><code>InitFunc</code> of each controller is defined in kube-controller-manager/app/core.go. e.g. startGarbageCollectorController <pre><code>garbageCollector, err := garbagecollector.NewGarbageCollector(\n    gcClientset,\n    metadataClient,\n    controllerContext.RESTMapper,\n    ignoredResources,\n    controllerContext.ObjectOrMetadataInformerFactory,\n    controllerContext.InformersStarted,\n)\n</code></pre></li> <li>Each controller is defined under pkg/controller. e.g. pkg/controller/garbagecollector/garbagecollector.go <pre><code>type GarbageCollector struct {\n    restMapper     meta.ResettableRESTMapper\n    metadataClient metadata.Interface\n    // garbage collector attempts to delete the items in attemptToDelete queue when the time is ripe.\n    attemptToDelete workqueue.RateLimitingInterface\n    // garbage collector attempts to orphan the dependents of the items in the attemptToOrphan queue, then deletes the items.\n    attemptToOrphan        workqueue.RateLimitingInterface\n    dependencyGraphBuilder *GraphBuilder\n    // GC caches the owners that do not exist according to the API server.\n    absentOwnerCache *ReferenceCache\n\n    kubeClient       clientset.Interface\n    eventBroadcaster record.EventBroadcaster\n\n    workerLock sync.RWMutex\n}\n</code></pre></li> </ol>"},{"location":"kubernetes-components/kube-controller-manager/#2-controller-overview","title":"2. Controller Overview","text":"<p>Components:</p> <ul> <li>Reflector:</li> <li>Delta FIFO queue:</li> <li> <p>Informer: Monitor Object's event and EventHandler is called for each event (usually add item to WorkQueue in event handlers in a controller).</p> <ul> <li>sharedIndexInformer: Usually created for a specific resource (e.g. <code>deploymentInformer</code>) with <code>NewSharedIndexInformer</code>, which creates a new instance for the listwatcher.<ol> <li>Indexer: indexed local cache. Indexer extends Store with multiple indices and restricts each accumulator to simply hold the current object.</li> <li><code>Controller</code> that pulls objects/notifications using the ListerWatcher and pushes them into a <code>DeltaFIFO</code>.</li> <li><code>sharedProcessor</code> responsible for relaying those notifications to each of the informer's clients. &lt;- <code>EventHandler</code> is set to <code>processorListener</code>, which is stored in <code>listeners</code> of a <code>sharedProcessor</code>.</li> <li> <p><code>listerWatcher</code> for the target resource. e.g. for deploymentInformer</p> <pre><code>&amp;cache.ListWatch{\n    ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {\n        if tweakListOptions != nil {\n            tweakListOptions(&amp;options)\n        }\n        return client.AppsV1().Deployments(namespace).List(context.TODO(), options)\n    },\n    WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n        if tweakListOptions != nil {\n            tweakListOptions(&amp;options)\n        }\n        return client.AppsV1().Deployments(namespace).Watch(context.TODO(), options)\n    },\n},\n</code></pre> </li> </ol> </li> </ul> </li> <li> <p>Lister: Retrieve object from in-memory-cache.</p> </li> <li>WorkQueue: Store item for which Reconcile loop is executed.</li> <li>Scheme: Scheme defines methods for serializing and deserializing API objects, a type registry for converting group, version, and kind information to and from Go schemas, and mappings between Go schemas of different versions. (ref: scheme.go)</li> <li>processNextWorkItem: Process an item in WorkQueue.</li> <li>syncHandler: Reconcile loop called by <code>processNextWorkItem</code> function (Function name can be different).</li> </ul>"},{"location":"kubernetes-components/kube-controller-manager/#3-built-in-controllers","title":"3. Built-in Controllers","text":""},{"location":"kubernetes-components/kube-controller-manager/#31-endpointscontroller","title":"3.1. EndpointsController","text":"<p>[Kubernetes] Endpoints\u304c\u3088\u304f\u308f\u304b\u3063\u3066\u306a\u3044\u306e\u3067EndpointsController\u3092\u8aad\u3093\u3067\u307f\u305f</p>"},{"location":"kubernetes-components/kube-controller-manager/#32-garbagecollector","title":"3.2. GarbageCollector","text":"![](garbagecollector.drawio.svg)"},{"location":"kubernetes-components/kube-controller-manager/#321-components","title":"3.2.1. Components","text":""},{"location":"kubernetes-components/kube-controller-manager/#3211-garbagecollector","title":"3.2.1.1. GarbageCollector:","text":"<pre><code>type GarbageCollector struct {\n    restMapper     meta.ResettableRESTMapper\n    metadataClient metadata.Interface\n    // garbage collector attempts to delete the items in attemptToDelete queue when the time is ripe.\n    attemptToDelete workqueue.RateLimitingInterface\n    // garbage collector attempts to orphan the dependents of the items in the attemptToOrphan queue, then deletes the items.\n    attemptToOrphan        workqueue.RateLimitingInterface\n    dependencyGraphBuilder *GraphBuilder\n    // GC caches the owners that do not exist according to the API server.\n    absentOwnerCache *ReferenceCache\n\n    kubeClient       clientset.Interface\n    eventBroadcaster record.EventBroadcaster\n\n    workerLock sync.RWMutex\n}\n</code></pre> <ol> <li>RestMapper: map resources to kind, and map kind and version to interfaces</li> </ol>"},{"location":"kubernetes-components/kube-controller-manager/#3212-graphbuilder","title":"3.2.1.2. GraphBuilder","text":"<p>builds a graph caching the dependencies among objects.</p> <pre><code>type GraphBuilder struct {\n    restMapper meta.RESTMapper\n\n    // each monitor list/watches a resource, the results are funneled to the\n    // dependencyGraphBuilder\n    monitors    monitors\n    monitorLock sync.RWMutex\n    // informersStarted is closed after after all of the controllers have been initialized and are running.\n    // After that it is safe to start them here, before that it is not.\n    informersStarted &lt;-chan struct{}\n\n    // stopCh drives shutdown. When a receive from it unblocks, monitors will shut down.\n    // This channel is also protected by monitorLock.\n    stopCh &lt;-chan struct{}\n\n    // running tracks whether Run() has been called.\n    // it is protected by monitorLock.\n    running bool\n\n    eventRecorder record.EventRecorder\n\n    metadataClient metadata.Interface\n    // monitors are the producer of the graphChanges queue, graphBuilder alters\n    // the in-memory graph according to the changes.\n    graphChanges workqueue.RateLimitingInterface\n    // uidToNode doesn't require a lock to protect, because only the\n    // single-threaded GraphBuilder.processGraphChanges() reads/writes it.\n    uidToNode *concurrentUIDToNode\n    // GraphBuilder is the producer of attemptToDelete and attemptToOrphan, GC is the consumer.\n    attemptToDelete workqueue.RateLimitingInterface\n    attemptToOrphan workqueue.RateLimitingInterface\n    // GraphBuilder and GC share the absentOwnerCache. Objects that are known to\n    // be non-existent are added to the cached.\n    absentOwnerCache *ReferenceCache\n    sharedInformers  informerfactory.InformerFactory\n    ignoredResources map[schema.GroupResource]struct{}\n}\n</code></pre> <ol> <li>monitors: a set of monitors, each of which runs a cache.Controller (1. construct and run a Reflector and pumps objects/notifications to the Config.Queue. 2. pop from the queue and process with Config.ProcessFunc).</li> <li>graphChanges: workqueue to store events from informer (input of all process)</li> <li>absentOwnerCache: Objects that are known to be non-existent are added to the cached.</li> <li>uidToNode (graph): a pointer of concurrentUIDToNode <pre><code>type concurrentUIDToNode struct {\n    uidToNodeLock sync.RWMutex\n    uidToNode     map[types.UID]*node\n}\n</code></pre></li> <li>node <pre><code>type node struct {\n    identity objectReference\n    // dependents will be read by the orphan() routine, we need to protect it with a lock.\n    dependentsLock sync.RWMutex\n    // dependents are the nodes that have node.identity as a\n    // metadata.ownerReference.\n    dependents map[*node]struct{}\n    // this is set by processGraphChanges() if the object has non-nil DeletionTimestamp\n    // and has the FinalizerDeleteDependents.\n    deletingDependents     bool\n    deletingDependentsLock sync.RWMutex\n    // this records if the object's deletionTimestamp is non-nil.\n    beingDeleted     bool\n    beingDeletedLock sync.RWMutex\n    // this records if the object was constructed virtually and never observed via informer event\n    virtual     bool\n    virtualLock sync.RWMutex\n    // when processing an Update event, we need to compare the updated\n    // ownerReferences with the owners recorded in the graph.\n    owners []metav1.OwnerReference\n}\n</code></pre><ol> <li><code>isObserved</code>: <code>!virtual</code></li> <li><code>vitual</code>: In attemptToDeleteItem, if there's no corresponding object in API server or the latest uid is not same as item.identity.UID, the item is added back to graphChanges with virtual = true</li> </ol> </li> </ol>"},{"location":"kubernetes-components/kube-controller-manager/#322-steps","title":"3.2.2. Steps","text":"<p>NewGarbageCollector:</p> <ol> <li>GarbageCollector and GraphBuilder are initialized</li> </ol> <p>GarbageCollector.Run():</p> <ol> <li>Start gc.dependencyGraphBuilder.Run() (wait until cache is synced)     <pre><code>go gc.dependencyGraphBuilder.Run(ctx.Done())\n</code></pre><ol> <li>Start <code>gb.startMonitors()</code>: ensure the current set of monitors are running. Start each of the monitors     <pre><code>gb.sharedInformers.Start(gb.stopCh)\ngo monitor.Run()\n</code></pre></li> <li>Run <code>runProcessGraphChanges</code> every second     <pre><code>wait.Until(gb.runProcessGraphChanges, 1*time.Second, stopCh)\n</code></pre></li> <li>runProcessGraphChanges calls processGraphChanges in a for loop.     <pre><code>func (gb *GraphBuilder) runProcessGraphChanges() {\n    for gb.processGraphChanges() {\n    }\n}\n</code></pre></li> <li><code>processGraphChanges</code>: Get an item from <code>graphChanges</code> and put the corresponding node to <code>attemptToDelete</code> or <code>attemptToOrphan</code> queue.<ol> <li>if the node in <code>uidToNode</code> is not observed and now observed -&gt; node.<code>markObserved()</code> (Add a potentially invalid dependent to <code>attemptToDelete</code> queue) ref</li> <li>[<code>addEvent</code> or <code>updateEvent</code>] if not found in <code>uidToNode</code>, <code>insertNode</code> + <code>processTransitions</code> ref</li> <li>[<code>addEvent</code> or <code>updateEvent</code>] if found in <code>uidToNode</code>, reflect changes in ownerReferences and if being deleted, <code>markBeingDeleted()</code> + <code>processTransitions</code> ref</li> <li>[<code>deleteEvent</code>] if found<ol> <li>if <code>event.virtual</code> (event from GarbageCollector) -&gt; in some case set <code>removeExistingNode</code> to false as it's not certain. Detail: ref</li> <li>if <code>removeExistingNode</code>, <code>removeNode</code>, add dependents to <code>attemptToDelete</code>, and add owners to <code>attemptToDelete</code></li> </ol> </li> </ol> </li> </ol> </li> <li>Start gc workers     <pre><code>// gc workers\nfor i := 0; i &lt; workers; i++ {\n    go wait.UntilWithContext(ctx, gc.runAttemptToDeleteWorker, 1*time.Second)\n    go wait.Until(gc.runAttemptToOrphanWorker, 1*time.Second, ctx.Done())\n}\n</code></pre><ol> <li>run runAttemptToDeleteWorker() <pre><code>func (gc *GarbageCollector) runAttemptToDeleteWorker(ctx context.Context) {\n    for gc.processAttemptToDeleteWorker(ctx) {\n    }\n}\n</code></pre> processAttemptToDeleteWorker:<ol> <li>Get an item (node) from attemptToDelete queue.     <pre><code>action := gc.attemptToDeleteWorker(ctx, item)\n</code></pre></li> <li>Process it in attemptToDeleteWorker<ol> <li>In case the node, converted from the item in the queue, is not observed (meaning that it's added from <code>objectReference</code> whose object is not found in API server), forget the item if it doesn't exist in the graph or it's observed. ref</li> </ol> </li> <li>Delete the item with attemptToDeleteItem <pre><code>err := gc.attemptToDeleteItem(ctx, n)\n</code></pre><ol> <li><code>item.isBeingDeleted</code> &amp; <code>!item.isDeletingDependents</code> -&gt; Do nothing and return <code>nil</code></li> <li>Get the latest object from API server     <pre><code>latest, err := gc.getObject(item.identity)\n</code></pre></li> <li>err=NotFound -&gt; <code>enqueueVirtualDeleteEvent</code>(enqueue event to <code>graphChanges</code> with <code>virtual=true</code>)     <pre><code>gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity)\n</code></pre></li> <li><code>latest.GetUID() != item.identity.UID</code> -&gt; <code>enqueueVirtualDeleteEvent</code>(enqueue event to <code>graphChanges</code> with <code>virtual=true</code>) and return <code>enqueuedVirtualDeleteEventErr</code> <pre><code>gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity)\n</code></pre></li> <li><code>item.isDeletingDependents()</code> -&gt; <code>gc.processDeletingDependentsItem(item)</code> and return <code>enqueuedVirtualDeleteEventErr</code><ol> <li>If no <code>blockingDependents</code> -&gt; <code>gc.removeFinalizer(item, metav1.FinalizerDeleteDependents)</code></li> <li>For <code>blockingDependents</code>, if not <code>isBeingDeleted</code> -&gt; <code>gc.attemptToDelete.Add(dep)</code></li> </ol> </li> <li>If there's no <code>OwnerReferences</code> -&gt; nil</li> <li>Classify ownerReferences     <pre><code>solid, dangling, waitingForDependentsDeletion, err := gc.classifyReferences(ctx, item, ownerReferences)\n</code></pre></li> <li><code>len(solid) != 0</code><ol> <li><code>len(dangling) == 0 &amp;&amp; len(waitingForDependentsDeletion) == 0</code> -&gt; return <code>nil</code></li> <li>Delete owner references for <code>dangling</code> and <code>waitingForDependentsDeletion</code> (send PATCH request to API server)</li> </ol> </li> <li><code>len(waitingForDependentsDeletion) != 0 &amp;&amp; item.dependentsLength() != 0</code>     for all dependents, if <code>isBeingDeleted</code> -&gt; send <code>unblockOwnerReferencesStrategicMergePatch</code> PATCH request to API server<ol> <li>delete with for <code>DeletePropagationForeground</code> (API server)</li> </ol> </li> <li>default<ol> <li><code>hasOrphanFinalizer</code> -&gt; delete with <code>DeletePropagationOrphan</code></li> <li><code>hasDeleteDependentsFinalizer</code> -&gt; delete with <code>DeletePropagationForeground</code></li> <li>default -&gt; delete with <code>DeletePropagationBackground</code></li> </ol> </li> </ol> </li> </ol> </li> <li>run runAttemptToOrphanWorker() every second<ol> <li>Get owner from attemptToOrphan</li> <li><code>attemptToOrphanWorker</code><ol> <li><code>orphanDependents(owner, dependents)</code>: remove owner references via PATCH (API server)</li> <li><code>gc.removeFinalizer(owner, metav1.FinalizerOrphanDependents)</code></li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>GarbageCollector.Sync keeps updating the resources to monitor periodically. -&gt; <code>GraphBuilder.syncMonitors(resources)</code> but not found where it's called.</p> <p>TestCase:</p> <ol> <li>TestCascadingDeletion:<ol> <li>Pod with ownerreference to <code>toBeDeletedRC</code> replicationcontroller -&gt; deleted</li> <li>Pod with ownerrefrerece to <code>remainingRC</code> and <code>toBeDeletedRC</code> replicatioincontroller -&gt; remain</li> <li>Pod without ownerreference -&gt; remain</li> <li>Delete <code>toBeDeletedRCName</code> replicationcontroller</li> </ol> </li> <li> <p>TestOrphaning</p> <ol> <li>If deleted with <code>DeleteOptions.propagationPolicy=Orphan</code>, the ownerreferences are just removed without deleting the object itself.</li> </ol> <pre><code>kubectl create deploy nginx --image=nginx --replicas=1\ndeployment.apps/nginx created\nkubectl get deploy\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           3m25s\nkubectl delete deployment nginx --cascade=orphan\ndeployment.apps \"nginx\" deleted\nkubectl get deploy\nNo resources found in default namespace.\nkubectl get rs\nNAME              DESIRED   CURRENT   READY   AGE\nnginx-76d6c9b8c   1         1         1       3m40s\nkubectl get pod\nNAME                    READY   STATUS    RESTARTS   AGE\nnginx-76d6c9b8c-jcwxv   1/1     Running   0          3m46s\n</code></pre> </li> </ol>"},{"location":"kubernetes-components/kube-controller-manager/#323-ref","title":"3.2.3. Ref","text":"<ol> <li>gabagecollector.go</li> <li>graph_builder.go</li> <li>test/integration/gabagecollector/garbage_collector_test.go</li> <li>Enable garbage collection of custom resources</li> </ol>"},{"location":"kubernetes-components/kube-controller-manager/#324-memo","title":"3.2.4. Memo","text":"<ol> <li>Delete<ol> <li><code>propagationPolicy=Forground</code> -&gt; API-server updates <code>metadata.deletionTimestamp</code> and adds <code>foregroundDeletion</code> finalizer instead of removing the object itself.<ol> <li>curl: with <code>'{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}'</code></li> <li>kubectl: <code>kubectl delete --cascade=foregraound</code></li> </ol> </li> <li><code>PropagationPolicy=Backgound</code> (default) -&gt; API-server immediately deletes the object and its dependents.<ol> <li>curl: with <code>'{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Background\"}'</code></li> <li><code>kubectl delete --cascade=background</code> (or <code>kubectl delete</code>)</li> </ol> </li> <li><code>PropagationPolicy=Orphan</code> -&gt; API-server deletes the object but not deletes the dependent objects. Instead its dependent objects remain as orphans<ol> <li>curl: with <code>'{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}'</code></li> <li><code>kubectl delete --cascade=orphan</code></li> </ol> </li> </ol> </li> <li>OwnerReferences</li> <li>Finalizers</li> </ol>"},{"location":"kubernetes-components/kube-controller-manager/#33-namespacecontroller","title":"3.3. NamespaceController","text":"<pre><code>// NamespaceController is responsible for performing actions dependent upon a namespace phase\ntype NamespaceController struct {\n    // lister that can list namespaces from a shared cache\n    lister corelisters.NamespaceLister\n    // returns true when the namespace cache is ready\n    listerSynced cache.InformerSynced\n    // namespaces that have been queued up for processing by workers\n    queue workqueue.RateLimitingInterface\n    // helper to delete all resources in the namespace when the namespace is deleted.\n    namespacedResourcesDeleter deletion.NamespacedResourcesDeleterInterface\n}\n</code></pre> <ol> <li>API: https://github.com/kubernetes/api/blob/master/core/v1/types.go#L5601</li> <li> <p>startNamespaceController <pre><code>namespaceController := namespacecontroller.NewNamespaceController(\n    ctx,\n    namespaceKubeClient,\n    metadataClient,\n    discoverResourcesFn,\n    controllerContext.InformerFactory.Core().V1().Namespaces(),\n    controllerContext.ComponentConfig.NamespaceController.NamespaceSyncPeriod.Duration,\n    v1.FinalizerKubernetes,\n)\ngo namespaceController.Run(ctx, int(controllerContext.ComponentConfig.NamespaceController.ConcurrentNamespaceSyncs))\n</code></pre> FinalizerKubernetes <pre><code>const (\n    FinalizerKubernetes FinalizerName = \"kubernetes\"\n)\n</code></pre></p> </li> <li> <p>NewNamespaceController:</p> <ol> <li>Init NamespaceController     <pre><code>namespaceController := &amp;NamespaceController{\n    queue:                      workqueue.NewNamedRateLimitingQueue(nsControllerRateLimiter(), \"namespace\"),\n    namespacedResourcesDeleter: deletion.NewNamespacedResourcesDeleter(kubeClient.CoreV1().Namespaces(), metadataClient, kubeClient.CoreV1(), discoverResourcesFn, finalizerToken),\n}\n</code></pre></li> <li>Prepare event handler for namespaceInformer     <pre><code>namespaceInformer.Informer().AddEventHandlerWithResyncPeriod(\n    cache.ResourceEventHandlerFuncs{\n        AddFunc: func(obj interface{}) {\n            namespace := obj.(*v1.Namespace)\n            namespaceController.enqueueNamespace(namespace)\n        },\n        UpdateFunc: func(oldObj, newObj interface{}) {\n            namespace := newObj.(*v1.Namespace)\n            namespaceController.enqueueNamespace(namespace)\n        },\n    },\n    resyncPeriod,\n)\n</code></pre></li> </ol> </li> <li>Run -&gt; worker -&gt; workFunc</li> <li>workFunc:<ol> <li>get key from queue     <pre><code>key, quit := nm.queue.Get()\n</code></pre></li> <li>sync     <pre><code>err := nm.syncNamespaceFromKey(key.(string))\n</code></pre></li> <li>syncNamespaceFromKey:<ol> <li>get namespace     <pre><code>namespace, err := nm.lister.Get(key)\n</code></pre></li> <li>run deleter     <pre><code>nm.namespacedResourcesDeleter.Delete(namespace.Name)\n</code></pre><ol> <li>Get namespace     <pre><code>namespace, err := d.nsClient.Get(context.TODO(), nsName, metav1.GetOptions{})\n</code></pre></li> <li><code>namespace.DeletionTimestamp == nil</code> -&gt; return nil</li> <li>Update namespace status to terminating     <pre><code>namespace, err = d.retryOnConflictError(namespace, d.updateNamespaceStatusFunc)\n</code></pre> <code>newNamespace.Status.Phase = v1.NamespaceTerminating</code></li> <li>Delete all contents in the namespace     <pre><code>estimate, err := d.deleteAllContent(namespace)\n</code></pre></li> <li>finalizeNamespace removes the specified finalizerToken and finalizes the namespace     <pre><code>_, err = d.retryOnConflictError(namespace, d.finalizeNamespace)\n</code></pre></li> </ol> </li> </ol> </li> </ol> </li> </ol>"},{"location":"kubernetes-components/kube-controller-manager/#34-deploymentcontroller","title":"3.4. DeploymentController","text":"<p>ToDo</p>"},{"location":"kubernetes-components/kube-controller-manager/#references","title":"References","text":"<ul> <li>https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html</li> <li>https://engineering.bitnami.com/articles/kubewatch-an-example-of-kubernetes-custom-controller.html</li> <li>https://cloudnative.to/blog/client-go-informer-source-code/</li> <li>Start each controller: https://github.com/kubernetes/kubernetes/blob/fe099b2abdb023b21a17cd6a127e381b846c1a1f/cmd/kube-controller-manager/controller-manager.go</li> <li>Definition of each controller: https://github.com/kubernetes/kubernetes/tree/master/pkg/controller</li> </ul>"},{"location":"kubernetes-components/kube-proxy/","title":"kube-proxy","text":"<p>Modes:</p> <ul> <li>user space mode (legacy)</li> <li>iptable mode (default)</li> <li>ipvs mode</li> </ul> <p>Reference:</p> <ul> <li>kube-proxy\u8a73\u7d30</li> <li>Comparing kube-proxy modes: iptables or IPVS?</li> </ul>"},{"location":"kubernetes-components/kubectl/","title":"kubectl","text":""},{"location":"kubernetes-components/kubectl/#check-apis","title":"Check APIs","text":"<ol> <li> <p><code>--v=&lt;log level&gt;</code></p> <pre><code>kubectl get po -n kube-system --v=6\nI0301 06:49:23.060965   14466 loader.go:372] Config loaded from file:  /Users/masato-naka/.kube/config\nI0301 06:49:23.077877   14466 round_trippers.go:454] GET https://127.0.0.1:51938/api/v1/namespaces/kube-system/pods?limit=500 200 OK in 11 milliseconds\nNAME                                         READY   STATUS    RESTARTS   AGE\ncoredns-558bd4d5db-j68vq                     1/1     Running   23         28d\ncoredns-558bd4d5db-wrwlx                     1/1     Running   23         28d\netcd-kind-control-plane                      1/1     Running   23         28d\nkindnet-t57rn                                1/1     Running   23         28d\nkube-apiserver-kind-control-plane            1/1     Running   23         28d\nkube-controller-manager-kind-control-plane   1/1     Running   29         28d\nkube-proxy-8bqmv                             1/1     Running   23         28d\nkube-scheduler-kind-control-plane            1/1     Running   30         28d\n</code></pre> <pre><code>kubectl get rs -n kube-system --v=6\nI0301 06:51:05.443904   14732 loader.go:372] Config loaded from file:  /Users/masato-naka/.kube/config\nI0301 06:51:05.467278   14732 round_trippers.go:454] GET https://127.0.0.1:51938/api?timeout=32s 200 OK in 22 milliseconds\nI0301 06:51:05.514229   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis?timeout=32s 200 OK in 2 milliseconds\nI0301 06:51:05.569719   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/autoscaling/v2beta1?timeout=32s 200 OK in 5 milliseconds\nI0301 06:51:05.570835   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/kubeflow.org/v1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.570854   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/batch/v1beta1?timeout=32s 200 OK in 5 milliseconds\nI0301 06:51:05.573081   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/scheduling.k8s.io/v1beta1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.573087   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/authorization.k8s.io/v1beta1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.573119   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/extensions/v1beta1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.573150   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/discovery.k8s.io/v1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.573173   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/authentication.k8s.io/v1beta1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.573188   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/autoscaling/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.573175   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/authentication.k8s.io/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.573202   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/rbac.authorization.k8s.io/v1beta1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.573212   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/node.k8s.io/v1beta1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.573213   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/batch/v1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.573258   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/coordination.k8s.io/v1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.573276   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/authorization.k8s.io/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.573908   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/apps/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574005   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/admissionregistration.k8s.io/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574032   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/coordination.k8s.io/v1beta1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574008   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/node.k8s.io/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574021   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/apiregistration.k8s.io/v1?timeout=32s 200 OK in 9 milliseconds\nI0301 06:51:05.574057   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/events.k8s.io/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574414   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/storage.k8s.io/v1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:05.574439   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/networking.k8s.io/v1?timeout=32s 200 OK in 5 milliseconds\nI0301 06:51:05.574446   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/discovery.k8s.io/v1beta1?timeout=32s 200 OK in 9 milliseconds\nI0301 06:51:05.574465   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/apiextensions.k8s.io/v1beta1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574467   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/policy/v1beta1?timeout=32s 200 OK in 9 milliseconds\nI0301 06:51:05.574478   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/scheduling.k8s.io/v1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574475   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/policy/v1?timeout=32s 200 OK in 6 milliseconds\nI0301 06:51:05.574526   14732 round_trippers.go:454] GET https://127.0.0.1:51938/api/v1?timeout=32s 200 OK in 10 milliseconds\nI0301 06:51:05.574537   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/events.k8s.io/v1beta1?timeout=32s 200 OK in 8 milliseconds\nI0301 06:51:05.574666   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/admissionregistration.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds\nI0301 06:51:05.574702   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/rbac.authorization.k8s.io/v1?timeout=32s 200 OK in 4 milliseconds\nI0301 06:51:05.574679   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/certificates.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds\nI0301 06:51:05.574752   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/acid.zalan.do/v1?timeout=32s 200 OK in 4 milliseconds\nI0301 06:51:05.575375   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/apiregistration.k8s.io/v1beta1?timeout=32s 200 OK in 10 milliseconds\nI0301 06:51:05.575481   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/autoscaling/v2beta2?timeout=32s 200 OK in 9 milliseconds\nI0301 06:51:05.575600   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/apiextensions.k8s.io/v1?timeout=32s 200 OK in 6 milliseconds\nI0301 06:51:05.575743   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/storage.k8s.io/v1beta1?timeout=32s 200 OK in 9 milliseconds\nI0301 06:51:05.575745   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/networking.k8s.io/v1beta1?timeout=32s 200 OK in 10 milliseconds\nI0301 06:51:05.575778   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/flowcontrol.apiserver.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds\nI0301 06:51:05.575799   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/certificates.k8s.io/v1?timeout=32s 200 OK in 7 milliseconds\nI0301 06:51:07.689538   14732 round_trippers.go:454] GET https://127.0.0.1:51938/apis/apps/v1/namespaces/kube-system/replicasets?limit=500 200 OK in 2 milliseconds\nNAME                 DESIRED   CURRENT   READY   AGE\ncoredns-558bd4d5db   2         2         2       28d\n</code></pre> </li> <li> <p><code>kubectl proxy</code> <pre><code>kubectl proxy --port=8080\n</code></pre></p> <pre><code>curl localhost:8080\n</code></pre> <p>result <pre><code>{\n\"paths\": [\n    \"/.well-known/openid-configuration\",\n    \"/api\",\n    \"/api/v1\",\n    \"/apis\",\n    \"/apis/\",\n    \"/apis/acid.zalan.do\",\n    \"/apis/acid.zalan.do/v1\",\n    \"/apis/admissionregistration.k8s.io\",\n    \"/apis/admissionregistration.k8s.io/v1\",\n    \"/apis/admissionregistration.k8s.io/v1beta1\",\n    \"/apis/apiextensions.k8s.io\",\n    \"/apis/apiextensions.k8s.io/v1\",\n    \"/apis/apiextensions.k8s.io/v1beta1\",\n    \"/apis/apiregistration.k8s.io\",\n    \"/apis/apiregistration.k8s.io/v1\",\n    \"/apis/apiregistration.k8s.io/v1beta1\",\n    \"/apis/apps\",\n    \"/apis/apps/v1\",\n    \"/apis/authentication.k8s.io\",\n    \"/apis/authentication.k8s.io/v1\",\n    \"/apis/authentication.k8s.io/v1beta1\",\n    \"/apis/authorization.k8s.io\",\n    \"/apis/authorization.k8s.io/v1\",\n    \"/apis/authorization.k8s.io/v1beta1\",\n    \"/apis/autoscaling\",\n    \"/apis/autoscaling/v1\",\n    \"/apis/autoscaling/v2beta1\",\n    \"/apis/autoscaling/v2beta2\",\n    \"/apis/batch\",\n    \"/apis/batch/v1\",\n    \"/apis/batch/v1beta1\",\n    \"/apis/certificates.k8s.io\",\n    \"/apis/certificates.k8s.io/v1\",\n    \"/apis/certificates.k8s.io/v1beta1\",\n    \"/apis/coordination.k8s.io\",\n    \"/apis/coordination.k8s.io/v1\",\n    \"/apis/coordination.k8s.io/v1beta1\",\n    \"/apis/discovery.k8s.io\",\n    \"/apis/discovery.k8s.io/v1\",\n    \"/apis/discovery.k8s.io/v1beta1\",\n    \"/apis/events.k8s.io\",\n    \"/apis/events.k8s.io/v1\",\n    \"/apis/events.k8s.io/v1beta1\",\n    \"/apis/extensions\",\n    \"/apis/extensions/v1beta1\",\n    \"/apis/flowcontrol.apiserver.k8s.io\",\n    \"/apis/flowcontrol.apiserver.k8s.io/v1beta1\",\n    \"/apis/kubeflow.org\",\n    \"/apis/kubeflow.org/v1\",\n    \"/apis/networking.k8s.io\",\n    \"/apis/networking.k8s.io/v1\",\n    \"/apis/networking.k8s.io/v1beta1\",\n    \"/apis/node.k8s.io\",\n    \"/apis/node.k8s.io/v1\",\n    \"/apis/node.k8s.io/v1beta1\",\n    \"/apis/policy\",\n    \"/apis/policy/v1\",\n    \"/apis/policy/v1beta1\",\n    \"/apis/rbac.authorization.k8s.io\",\n    \"/apis/rbac.authorization.k8s.io/v1\",\n    \"/apis/rbac.authorization.k8s.io/v1beta1\",\n    \"/apis/scheduling.k8s.io\",\n    \"/apis/scheduling.k8s.io/v1\",\n    \"/apis/scheduling.k8s.io/v1beta1\",\n    \"/apis/storage.k8s.io\",\n    \"/apis/storage.k8s.io/v1\",\n    \"/apis/storage.k8s.io/v1beta1\",\n    \"/healthz\",\n    \"/healthz/autoregister-completion\",\n    \"/healthz/etcd\",\n    \"/healthz/log\",\n    \"/healthz/ping\",\n    \"/healthz/poststarthook/aggregator-reload-proxy-client-cert\",\n    \"/healthz/poststarthook/apiservice-openapi-controller\",\n    \"/healthz/poststarthook/apiservice-registration-controller\",\n    \"/healthz/poststarthook/apiservice-status-available-controller\",\n    \"/healthz/poststarthook/bootstrap-controller\",\n    \"/healthz/poststarthook/crd-informer-synced\",\n    \"/healthz/poststarthook/generic-apiserver-start-informers\",\n    \"/healthz/poststarthook/kube-apiserver-autoregistration\",\n    \"/healthz/poststarthook/priority-and-fairness-config-consumer\",\n    \"/healthz/poststarthook/priority-and-fairness-config-producer\",\n    \"/healthz/poststarthook/priority-and-fairness-filter\",\n    \"/healthz/poststarthook/rbac/bootstrap-roles\",\n    \"/healthz/poststarthook/scheduling/bootstrap-system-priority-classes\",\n    \"/healthz/poststarthook/start-apiextensions-controllers\",\n    \"/healthz/poststarthook/start-apiextensions-informers\",\n    \"/healthz/poststarthook/start-cluster-authentication-info-controller\",\n    \"/healthz/poststarthook/start-kube-aggregator-informers\",\n    \"/healthz/poststarthook/start-kube-apiserver-admission-initializer\",\n    \"/livez\",\n    \"/livez/autoregister-completion\",\n    \"/livez/etcd\",\n    \"/livez/log\",\n    \"/livez/ping\",\n    \"/livez/poststarthook/aggregator-reload-proxy-client-cert\",\n    \"/livez/poststarthook/apiservice-openapi-controller\",\n    \"/livez/poststarthook/apiservice-registration-controller\",\n    \"/livez/poststarthook/apiservice-status-available-controller\",\n    \"/livez/poststarthook/bootstrap-controller\",\n    \"/livez/poststarthook/crd-informer-synced\",\n    \"/livez/poststarthook/generic-apiserver-start-informers\",\n    \"/livez/poststarthook/kube-apiserver-autoregistration\",\n    \"/livez/poststarthook/priority-and-fairness-config-consumer\",\n    \"/livez/poststarthook/priority-and-fairness-config-producer\",\n    \"/livez/poststarthook/priority-and-fairness-filter\",\n    \"/livez/poststarthook/rbac/bootstrap-roles\",\n    \"/livez/poststarthook/scheduling/bootstrap-system-priority-classes\",\n    \"/livez/poststarthook/start-apiextensions-controllers\",\n    \"/livez/poststarthook/start-apiextensions-informers\",\n    \"/livez/poststarthook/start-cluster-authentication-info-controller\",\n    \"/livez/poststarthook/start-kube-aggregator-informers\",\n    \"/livez/poststarthook/start-kube-apiserver-admission-initializer\",\n    \"/logs\",\n    \"/metrics\",\n    \"/openapi/v2\",\n    \"/openid/v1/jwks\",\n    \"/readyz\",\n    \"/readyz/autoregister-completion\",\n    \"/readyz/etcd\",\n    \"/readyz/informer-sync\",\n    \"/readyz/log\",\n    \"/readyz/ping\",\n    \"/readyz/poststarthook/aggregator-reload-proxy-client-cert\",\n    \"/readyz/poststarthook/apiservice-openapi-controller\",\n    \"/readyz/poststarthook/apiservice-registration-controller\",\n    \"/readyz/poststarthook/apiservice-status-available-controller\",\n    \"/readyz/poststarthook/bootstrap-controller\",\n    \"/readyz/poststarthook/crd-informer-synced\",\n    \"/readyz/poststarthook/generic-apiserver-start-informers\",\n    \"/readyz/poststarthook/kube-apiserver-autoregistration\",\n    \"/readyz/poststarthook/priority-and-fairness-config-consumer\",\n    \"/readyz/poststarthook/priority-and-fairness-config-producer\",\n    \"/readyz/poststarthook/priority-and-fairness-filter\",\n    \"/readyz/poststarthook/rbac/bootstrap-roles\",\n    \"/readyz/poststarthook/scheduling/bootstrap-system-priority-classes\",\n    \"/readyz/poststarthook/start-apiextensions-controllers\",\n    \"/readyz/poststarthook/start-apiextensions-informers\",\n    \"/readyz/poststarthook/start-cluster-authentication-info-controller\",\n    \"/readyz/poststarthook/start-kube-aggregator-informers\",\n    \"/readyz/poststarthook/start-kube-apiserver-admission-initializer\",\n    \"/readyz/shutdown\",\n    \"/version\"\n]\n}%\n</code></pre> <pre><code>curl localhost:8080/api/v1  | jq -r '.resources[].name'\n</code></pre> <p>result <pre><code>bindings\ncomponentstatuses\nconfigmaps\nendpoints\nevents\nlimitranges\nnamespaces\nnamespaces/finalize\nnamespaces/status\nnodes\nnodes/proxy\nnodes/status\npersistentvolumeclaims\npersistentvolumeclaims/status\npersistentvolumes\npersistentvolumes/status\npods\npods/attach\npods/binding\npods/eviction\npods/exec\npods/log\npods/portforward\npods/proxy\npods/status\npodtemplates\nreplicationcontrollers\nreplicationcontrollers/scale\nreplicationcontrollers/status\nresourcequotas\nresourcequotas/status\nsecrets\nserviceaccounts\nserviceaccounts/token\nservices\nservices/proxy\nservices/status\n</code></pre> <pre><code>curl localhost:8080/apis/apps/v1  | jq -r '.resources[].name'\n</code></pre> <p>result <pre><code>controllerrevisions\ndaemonsets\ndaemonsets/status\ndeployments\ndeployments/scale\ndeployments/status\nreplicasets\nreplicasets/scale\nreplicasets/status\nstatefulsets\nstatefulsets/scale\nstatefulsets/status\n</code></pre>"},{"location":"kubernetes-components/kubelet/","title":"kubelet","text":""},{"location":"kubernetes-components/kubelet/#main-roles","title":"Main roles","text":"<ol> <li>configmap</li> <li>secret</li> <li>Create container</li> <li>Garbage Collection (image, container)</li> </ol>"},{"location":"kubernetes-components/kubelet/#implementation","title":"Implementation","text":"<ol> <li>Run</li> </ol>"},{"location":"kubernetes-components/kubelet/#mode","title":"Mode","text":"<p>standalone-kubelet-tutorial</p>"},{"location":"kubernetes-components/kubernetes-scheduler/","title":"Kubernetes Scheduler","text":"<ul> <li>kubernetes-scheduler</li> <li>ramdom scheduler</li> </ul>"},{"location":"kubernetes-extensions/","title":"Kubernetes Extension","text":""},{"location":"kubernetes-extensions/#kubernetes-scheduler","title":"Kubernetes Scheduler","text":"<p>kubernetes-scheduler</p>"},{"location":"kubernetes-extensions/#custom-resource-definition","title":"Custom Resource Definition","text":"<p>-&gt; API Server</p>"},{"location":"kubernetes-extensions/kubernetes-scheduler/","title":"Kubernetes Scheduler","text":""},{"location":"kubernetes-extensions/kubernetes-scheduler/#scheduling-framework","title":"Scheduling Framework","text":""},{"location":"kubernetes-extensions/kubernetes-scheduler/#extension-points","title":"Extension Points","text":"<ol> <li>queueSort</li> <li>preFilter</li> <li>filter</li> <li>postFilter</li> <li>preScore</li> <li>score</li> <li>reserve</li> <li>permit</li> <li>preBind</li> <li>bind</li> <li>postBind</li> <li>multiPoint</li> </ol>"},{"location":"kubernetes-extensions/kubernetes-scheduler/#default-plugins","title":"Default Plugins","text":"<p>https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins</p>"},{"location":"kubernetes-extensions/kubernetes-scheduler/#custom-scheduler","title":"Custom Scheduler","text":"<ul> <li>kube-batch</li> <li>Volcano</li> <li>random-scheduler: Simplest scheduler to start with.</li> <li>mini-kube-scheduler: You can create a Kubernetes scheduler from zero.<ul> <li>https://speakerdeck.com/sanposhiho/zi-zuo-sitexue-bukubernetes-schedulerru-men</li> <li>https://event.cloudnativedays.jp/cndt2021/talks/1184</li> <li>https://github.com/sanposhiho/mini-kube-scheduler</li> </ul> </li> </ul>"},{"location":"kubernetes-extensions/kubernetes-scheduler/random-scheduler/","title":"Kubernetes scheduler","text":""},{"location":"kubernetes-extensions/kubernetes-scheduler/random-scheduler/#components","title":"Components:","text":"<ul> <li><code>podQueue</code> channel</li> <li><code>quit</code> channel</li> <li><code>Scheduler</code><ul> <li>ScheduleOne:<ol> <li>Get a Pod from <code>podQueue</code>.</li> <li>findNode<ol> <li>Get Nodes from lister.</li> <li>Filter out unschedulable nodes.</li> <li>Give a score for each node.</li> <li>Get the node with the highest score.</li> </ol> </li> <li>bindNode</li> <li>emitEvent</li> </ol> </li> </ul> </li> </ul>"},{"location":"kubernetes-extensions/kubernetes-scheduler/random-scheduler/#getting-started","title":"Getting Started","text":"<ol> <li>Run scheduler.     <pre><code>go run main.go\n2022/08/12 20:19:51 Start a scheduler\n2022/08/12 20:19:51 Run is called\n2022/08/12 20:19:51 New node is added. docker-desktop\n</code></pre></li> <li> <p>Create a Pod.</p> <pre><code>kubectl apply -f pod.yaml\n</code></pre> </li> <li> <p>Check if the pod is scheduled.</p> <pre><code>2022/08/12 20:20:28 found a pod to schedule: [default/nginx]\n2022/08/12 20:20:28 calculated priorities: map[docker-desktop:47]\n2022/08/12 20:20:28 node docker-desktop is chosen for Pod [default/nginx]\n2022/08/12 20:20:28 pod [default/nginx] is successfully scheduled to node docker-desktop\n</code></pre> </li> </ol>"},{"location":"kubernetes-extensions/kubernetes-scheduler/random-scheduler/#steps","title":"Steps:","text":"<ol> <li> <p>Create <code>main</code>, <code>NewScheduler</code> and <code>Scheduler</code> struct with <code>Run</code> method.     <pre><code>package main\n\nimport (\n    \"log\"\n    \"k8s.io/api/core/v1\"\n)\n\nfunc main()  {\n    log.Println(\"Start a scheduler\")\n\n    podQueue := make(chan *v1.Pod, 300)\n    defer close(podQueue)\n\n    quit := make(chan struct{})\n    defer close(quit)\n\n    scheduler := NewScheduler(podQueue, quit)\n    scheduler.Run()\n}\n\ntype Scheduler struct {\n}\n\nfunc (s *Scheduler) Run() {\n\n}\n\nfunc NewScheduler(podQueue chan *v1.Pod, quit chan struct{}) Scheduler {\n    return Scheduler{}\n}\n</code></pre></p> <p>You can try running the empty scheduler:</p> <p><pre><code>go run main.go\n</code></pre> 1. Update <code>Scheduler</code> struct (Prepare nodeInformer, podInformer, randomPredicate, and randomPriority): 1. Update definitions.     <pre><code>type predicateFunc func(node *v1.Node, pod *v1.Pod) bool\ntype priorityFunc func(node *v1.Node, pod *v1.Pod) int\n\ntype Scheduler struct {\n    clientset  *kubernetes.Clientset\n    podQueue   chan *v1.Pod\n    nodeLister listersv1.NodeLister\n    predicates []predicateFunc\n    priorities []priorityFunc\n}\n</code></pre> 1. Initialize <code>clientset</code> in <code>NewScheduler</code>. 1. Define <code>randomPredicate</code> and <code>randomPriority</code>.     <pre><code>func randomPredicate(node *v1.Node, pod *v1.Pod) bool {\n    r := rand.Intn(2)\n    return r == 0\n}\n\nfunc randomPriority(node *v1.Node, pod *v1.Pod) int {\n    return rand.Intn(100)\n}\n</code></pre> 1. Define <code>initInformers</code>.</p> <pre><code>1. Create shared informer factory\n1. Create node informer with event handler for `AddFunc` (just print).\n1. Create pod informer with event handler for `AddFunc`.\n    1. Check `NodeName == \"\"`: Unscheduled Pods\n    1. Check `SchedulerName == scheduclerName`; this scheduler is specified.\n1. Start the factory.\n1. Return the nodeInformer lister.\n\n```go\nfunc initInformers(clientset *kubernetes.Clientset, podQueue chan *v1.Pod, quit chan struct{}) listersv1.NodeLister {\n    factory := informers.NewSharedInformerFactory(clientset, 0)\n    nodeInformer := factory.Core().V1().Nodes()\n    nodeInformer.Informer().AddEventHandler(\n        cache.ResourceEventHandlerFuncs{\n            AddFunc: func(obj interface{}) {\n                node, ok := obj.(*v1.Node)\n                if !ok {\n                    log.Println(\"this is not a node\")\n                    return\n                }\n                log.Printf(\"New node is added. %s\\n\", node.GetName())\n            },\n        },\n    )\n\n    podInformer := factory.Core().V1().Pods()\n    podInformer.Informer().AddEventHandler(\n        cache.ResourceEventHandlerFuncs{\n            AddFunc: func(obj interface{}){\n                pod, ok := obj.(*v1.Pod)\n                if !ok {\n                    log.Println(\"This is not a pod\")\n                    return\n                }\n                if pod.Spec.NodeName == \"\" &amp;&amp; pod.Spec.SchedulerName == schedulerName {\n                    podQueue &lt;- pod\n                }\n            },\n        },\n    )\n    factory.Start(quit)\n    return nodeInformer.Lister()\n}\n```\n</code></pre> <ol> <li> <p>Update <code>NewScheduler</code>:     <pre><code>func NewScheduler(podQueue chan *v1.Pod, quit chan struct{}) Scheduler {\n    // In-Cluster configuration\n    // config, err := rest.InClusterConfig()\n    // if err != nil {\n    //  log.Fatal(err)\n    // }\n\n    kubeConfigPath := filepath.Join(homedir.HomeDir(), \".kube\", \"config\")\n    config, err := clientcmd.BuildConfigFromFlags(\"\", kubeConfigPath)\n    if err != nil {\n        log.Printf(\"Building config from flags, %s\", err.Error())\n    }\n\n    clientset, err := kubernetes.NewForConfig(config)\n    if err != nil {\n        log.Fatal(err)\n    }\n    return Scheduler{\n        clientset: clientset,\n        podQueue: podQueue,\n        nodeLister: initInformers(clientset, podQueue, quit),\n        predicates: []predicateFunc{\n            randomPredicate,\n        },\n        priorities: []priorityFunc{\n            randomPriority,\n        },\n    }\n}\n</code></pre></p> </li> <li> <p>Try running the scheduler at this point. (Nothing happens as <code>Run</code> is still empty.)     <pre><code>go run main.go\nStart a scheduler\nRun is called\n</code></pre></p> </li> <li>Find best node for a pod in <code>ScheduleOne</code> function.</li> <li> <p>Define <code>ScheduleOne</code>.     The role of <code>ScheduleOne</code>:</p> <ol> <li>Get a pod from <code>podQueue</code>.</li> <li>Get the fit node from <code>findNode</code>.</li> </ol> <p>```go func (s *Scheduler) ScheduleOne() {     p := &lt;- s.podQueue     log.Println(\"found a pod to schedule:\", p.Namespace, \"/\", p.Name)</p> <pre><code>node, err := s.findNode(p)\nif err != nil {\n    log.Println(\"cannot find node that fits pod\", err.Error())\n    return\n}\nlog.Printf(\"node %s is chosen for Pod [%s/%s]\\n\", node, p.Namespace, p.Name)\n</code></pre> <p>}     1. Define <code>findNode</code> (find the best node for a given pod. If no schedulable node, return error.) The role of <code>findNode</code>: 1. Get nodes from the node lister. 1. Return error if there's no node. 1. Give a score with <code>prioritize</code> for each node. 1. Return the node with highest score <code>findBestNode</code>.</p> <pre><code>func (s *Scheduler) findNode(pod *v1.Pod) (string, error) {\n    nodes, err := s.nodeLister.List(labels.Everything())\n    if err != nil {\n        return \"\", err\n    }\n    if len(nodes) == 0 {\n        return \"\", errors.New(\"failed to find schedulable nodes\")\n    }\n    priorities := s.prioritize(nodes, pod)\n    return s.findBestNode(priorities), nil\n}\n</code></pre> </li> <li> <p>Define <code>prioritize</code>: Give a score with <code>priorities</code> for each node.     <pre><code>func (s *Scheduler) prioritize(nodes []*v1.Node, pod *v1.Pod) map[string]int {\n    priorities := make(map[string]int)\n    for _, node := range nodes {\n        for _, priority := range s.priorities {\n            priorities[node.Name] += priority(node, pod)\n        }\n    }\n    log.Println(\"calculated priorities:\", priorities)\n    return priorities\n}\n</code></pre></p> </li> <li><code>findBestNode</code>: Get the node with the highest score.     <pre><code>func (s *Scheduler) findBestNode(priorities map[string]int) string {\n    var maxP int\n    var bestNode string\n    for node, p := range priorities {\n        if p &gt; maxP {\n            maxP = p\n            bestNode = node\n        }\n    }\n    return bestNode\n}\n</code></pre></li> <li> <p>Update <code>Run</code> to call <code>ScheduleOne</code>.     <pre><code>func (s *Scheduler) Run(quit chan struct{}) {\n    log.Println(\"Run is called\")\n    wait.Until(s.ScheduleOne, 0, quit)\n}\n</code></pre></p> </li> <li> <p>Run and check the scheduler (Just choose a node):</p> <p>Run the scheduler: <pre><code>go run main.go\n2021/12/26 17:21:06 Start a scheduler\n2021/12/26 17:21:06 Run is called\n2021/12/26 17:21:06 New node is added. kind-control-plane\n</code></pre></p> <p>Create a pod with <code>schedulerName: random-scheduler</code>: <pre><code>kubectl apply -f pod.yaml\n</code></pre></p> <p>Scheduler's log: <pre><code>2021/12/26 17:21:06 found a pod to schedule: [default/nginx]\n2021/12/26 17:21:06 calculated priorities: map[kind-control-plane:47]\n2021/12/26 17:21:06 node kind-control-plane is chosen for Pod [default/nginx]\n</code></pre></p> <p><pre><code>kubectl get pod nginx\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   0/1     Pending   0          3m55s\n</code></pre> 1. Bind Node to Pod.     1. Add the following lines to <code>ScheduleOne</code>.</p> <p><pre><code>    err = s.bindPod(p, node)\n    if err != nil {\n        log.Println(\"failed to bind pod\", err.Error())\n        return\n    }\n</code></pre>     1. Define <code>bindPod</code>.</p> <p><pre><code>import (\n    \"context\"\n    ...\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    ...\n)\n...\nfunc (s *Scheduler) bindPod(pod *v1.Pod, node string) error {\n    return s.clientset.CoreV1().Pods(pod.Namespace).Bind(\n        context.Background(),\n        &amp;v1.Binding{\n            ObjectMeta: metav1.ObjectMeta{Name: pod.Name, Namespace: pod.Namespace},\n            Target:     v1.ObjectReference{APIVersion: \"v1\", Kind: \"Node\", Name: node},\n        },\n        metav1.CreateOptions{},\n    )\n}\n</code></pre>     1. Run the scheduler. 1. Run the scheduler     <pre><code>go run main.go\n2021/12/26 17:33:35 Start a scheduler\n2021/12/26 17:33:35 Run is called\n2021/12/26 17:33:35 New node is added. kind-control-plane\n</code></pre> 1. Create a Pod.     <pre><code>kubectl apply -f pod.yaml\n</code></pre> 1. Check logs.     <pre><code>2021/12/26 17:35:35 found a pod to schedule: [default/nginx]\n2021/12/26 17:35:35 calculated priorities: map[kind-control-plane:47]\n2021/12/26 17:35:35 node kind-control-plane is chosen for Pod [default/nginx]\n2021/12/26 17:35:35 pod [default/nginx] is successfully scheduled to node kind-control-plane\n</code></pre> 1. Check pod.     <pre><code>kubectl get pod nginx\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          26s\n</code></pre> 1. Emit event.     1. Add <code>emitEvent</code> function.     <pre><code>func (s *Scheduler) emitEvent(p *v1.Pod, message string) error {\n    timestamp := time.Now().UTC()\n    _, err := s.clientset.CoreV1().Events(p.Namespace).Create(\n        context.Background(),\n        &amp;v1.Event{\n            Count:          1,\n            Message:        message,\n            Reason:         \"Scheduled\",\n            LastTimestamp:  metav1.NewTime(timestamp),\n            FirstTimestamp: metav1.NewTime(timestamp),\n            Type:           \"Normal\",\n            Source: v1.EventSource{\n                Component: schedulerName,\n            },\n            InvolvedObject: v1.ObjectReference{\n                Kind:      \"Pod\",\n                Name:      p.Name,\n                Namespace: p.Namespace,\n                UID:       p.UID,\n            },\n            ObjectMeta: metav1.ObjectMeta{\n                GenerateName: p.Name + \"-\",\n            },\n        },\n        metav1.CreateOptions{},\n    )\n    if err != nil {\n        return err\n    }\n    return nil\n}\n</code></pre></p> </li> <li> <p>Add the following lines to the <code>ScheduleOne</code>.</p> <pre><code>    message := fmt.Sprintf(\"pod [%s/%s] is successfully scheduled to node %s\", p.Namespace, p.Name, node)\n    log.Println(message)\n\n    err = s.emitEvent(p, message)\n    if err != nil {\n        log.Println(\"failed to emit scheduled event\", err.Error())\n        return\n    }\n</code></pre> </li> <li> <p>Check.</p> <ol> <li>Create a Pod.     <pre><code>kubectl apply -f pod.yaml\n</code></pre></li> <li>Run the scheduler.     <pre><code>go run main.go\n2021/12/26 17:41:24 Start a scheduler\n2021/12/26 17:41:24 Run is called\n2021/12/26 17:41:24 New node is added. kind-control-plane\n2021/12/26 17:41:43 found a pod to schedule: [default/nginx]\n2021/12/26 17:41:43 calculated priorities: map[kind-control-plane:47]\n2021/12/26 17:41:43 node kind-control-plane is chosen for Pod [default/nginx]\n2021/12/26 17:41:43 pod [default/nginx] is successfully scheduled to node kind-control-plane\n</code></pre></li> <li> <p>Check event.</p> <pre><code>kubectl get event | grep Scheduled\n68s         Normal   Scheduled   pod/nginx   pod [default/nginx] is successfully scheduled to node kind-control-plane\n</code></pre> </li> </ol> </li> </ol> </li> </ol>"},{"location":"kubernetes-extensions/kubernetes-scheduler/random-scheduler/#reference","title":"Reference","text":"<ul> <li>https://banzaicloud.com/blog/k8s-custom-scheduler/</li> <li>https://github.com/martonsereg/random-scheduler</li> </ul>"},{"location":"kubernetes-features/","title":"Kubernetes Features","text":""},{"location":"kubernetes-features/#namespaces","title":"Namespaces","text":"<p>Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces.</p> <p>When to use multiple namespaces seems to be arguable.</p> <ul> <li>Kubernetes - Namespaces<ul> <li>For clusters with a few to tens of users, you should not need to create or think about namespaces at all. (in the Kubernetes document.)</li> </ul> </li> <li>RedHat - Kubernetes Namespaces Demystified - How To Make The Most of Them<ul> <li>Do not overload namespaces with multiple workloads that perform unrelated tasks.</li> <li>Users should create namespaces for a specific application or microservice and all of the application requirements. Reasons:<ul> <li>Simplified recreation of the entire application</li> <li>Fine-grained network management</li> <li>Greater scalability</li> <li>Greater observability</li> </ul> </li> </ul> </li> <li>Google Cloud - Kubernetes best practices: Organizing with Namespaces<ul> <li>Small team: 5~10 microservices \u2192 <code>default</code> Namespace</li> <li>Rapidly growing teams: 10+ microservices \u2192 each team owns their own microservices -&gt; Use multiple clusters or namespaces for production and development or Each team may choose to have their own namespace</li> <li>Large: not everyone knows everyone else. \u2192 each team definitely needs its own namespace. Each team might even opt for multiple namespaces to run its development and production environments.</li> </ul> </li> </ul>"},{"location":"kubernetes-features/#admission-controllers","title":"Admission Controllers","text":"<p>An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.</p> <p>Admission controllers may be \"validating\", \"mutating\", or both.</p> <p>You can turn on each of them by the argument <code>--enable-admission-plugins</code> of api-server.</p> <pre><code>kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...\n</code></pre> <p></p> <p>Admission controllers list:</p> <ol> <li>DefaultStorageClass: Set default storage class for <code>PersistentVolumeClaim</code></li> <li>AlwaysPullIMages: Set imagePullPolicy to <code>Always</code></li> <li>MutatingAdmissionWebhook (dynamic admission control): execute mutating admission control webhook</li> <li>ValidatingAdmissionWebhook (dynamic admission control): execute validating admission control webhook</li> <li>and more...</li> </ol>"},{"location":"kubernetes-features/#owner-references","title":"Owner References","text":"<p>Owner References</p>"},{"location":"kubernetes-features/#garbage-collection","title":"Garbage Collection","text":"<p>If you want to know about garbage collection, please read Garbage Collection.</p>"},{"location":"kubernetes-features/owner-references/","title":"Owner References","text":""},{"location":"kubernetes-features/owner-references/#example1-pet-person","title":"Example1: Pet -&gt; Person","text":"<ol> <li> <p>Prepare CRD <code>Person</code> and <code>Pet</code></p> <p>Person <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: persons.example.com\nspec:\n  group: example.com\n  names:\n    kind: Person\n    listKind: PersonList\n    plural: persons\n    singular: person\n  scope: Namespaced\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                name:\n                  type: string\n                age:\n                  type: integer\n                address:\n                  type: string\n                email:\n                  type: string\n                phone:\n                  type: string\n                hobbies:\n                  type: array\n                  items:\n                    type: string\n              required:\n                - name\n</code></pre> <p>Pet <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: pets.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                name:\n                  type: string\n                age:\n                  type: integer\n                breed:\n                  type: string\n                owner:\n                  type: string\n            status:\n              type: object\n              properties:\n                message:\n                  type: string\n  scope: Namespaced\n  names:\n    plural: pets\n    singular: pet\n    kind: Pet\n    listKind: PetList\n</code></pre> <li> <p>Install the CRD</p> <pre><code>kubectl apply -f crd/person.yaml,crd/pet.yaml\n</code></pre> </li> <li> <p>Create <code>Person</code> named <code>Alice</code> <pre><code>kubectl apply -f example/person-alice.yaml\n</code></pre></p> </li> <li>Get Alice's uid and update <code>ownerReferences</code> in  <code>example/pet-dog.yaml</code> <pre><code>uid=$(kubectl get -f example/person-alice.yaml -o yaml | yq .metadata.uid); echo $uid\nyq e -i \".metadata.ownerReferences[0].uid = \\\"$uid\\\"\" example/pet-dog.yaml\n</code></pre></li> <li>Create a <code>Pet</code> <code>dog</code> <pre><code>kubectl apply -f example/pet-dog.yaml\n</code></pre></li> <li>Get Person and Dog     <pre><code>kubectl get person,pet\nNAME                       AGE\nperson.example.com/alice   5m25s\n\nNAME                  AGE\npet.example.com/dog   8s\n</code></pre></li> <li>Delete alice     <pre><code>kubectl delete person alice\nperson.example.com \"alice\" deleted\n</code></pre></li> <li>The owned resource <code>dog</code> is also deleted. (cascading deletion)     <pre><code>kubectl get person,pet\nNo resources found in default namespace.\n</code></pre></li>"},{"location":"kubernetes-features/owner-references/#example2-ownerreferences-finalizer","title":"Example2: OwnerReferences &amp; Finalizer","text":"<ol> <li>Create <code>Person</code> named <code>Alice</code> (same as above)     <pre><code>kubectl apply -f example/person-alice.yaml\n</code></pre></li> <li>Get Alice's uid and update <code>ownerReferences</code> in  <code>example/pet-dog.yaml</code> <pre><code>uid=$(kubectl get -f example/person-alice.yaml -o yaml | yq .metadata.uid); echo $uid\nyq e -i \".metadata.ownerReferences[0].uid = \\\"$uid\\\"\" example/pet-dog-with-finalizer.yaml\n</code></pre></li> <li>Create a <code>Pet</code> <code>dog</code> <pre><code>kubectl apply -f example/pet-dog-with-finalizer.yaml\n</code></pre></li> <li>Delete alice     <pre><code>kubectl delete person alice\nperson.example.com \"alice\" deleted\n</code></pre></li> <li> <p>You can check alice is deleted, while <code>Pet</code> still exists as it has finalizer.     <pre><code>kubectl get person alice\nError from server (NotFound): persons.example.com \"alice\" not found\n</code></pre></p> <p><pre><code>kubectl get pet\nNAME   AGE\ndog    43s\n</code></pre> 1. To clean up the service, <code>finalizers</code> must be removed. <pre><code>kubectl patch pet dog -p '{\"metadata\":{\"finalizers\": []}}' --type=merge\n</code></pre></p> </li> </ol> <p>Garbage Collection does not wait to delete the owner object until the dependent object is actually deleted.</p>"},{"location":"kubernetes-features/owner-references/#example3-ownerreferences-blockownerdeletiontrue-finalizer","title":"Example3: OwnerReferences + BlockOwnerDeletion=true &amp; Finalizer","text":"<ol> <li>Create <code>Person</code> named <code>Alice</code> (same as above)     <pre><code>kubectl apply -f example/person-alice.yaml\n</code></pre></li> <li>Get Alice's uid and update <code>ownerReferences</code> in  <code>example/pet-dog.yaml</code> <pre><code>uid=$(kubectl get -f example/person-alice.yaml -o yaml | yq .metadata.uid); echo $uid\nyq e -i \".metadata.ownerReferences[0].uid = \\\"$uid\\\"\" example/pet-dog-with-finalizer-and-blockownerdeletion.yaml\n</code></pre></li> <li>Create a <code>Pet</code> <code>dog</code> <pre><code>kubectl apply -f example/pet-dog-with-finalizer-and-blockownerdeletion.yaml\n</code></pre></li> <li>Delete alice     <pre><code>kubectl delete person alice\nperson.example.com \"alice\" deleted\n</code></pre></li> <li> <p>You can check alice is deleted, while <code>Pet</code> still exists as it has finalizer.     <pre><code>kubectl get person alice\nError from server (NotFound): persons.example.com \"alice\" not found\n</code></pre></p> <p><pre><code>kubectl get pet\nNAME   AGE\ndog    43s\n</code></pre> 1. To clean up the service, <code>finalizers</code> must be removed. <pre><code>kubectl patch pet dog -p '{\"metadata\":{\"finalizers\": []}}' --type=merge\n</code></pre></p> </li> </ol> <p>Garbage Collection does not wait to delete the owner object until the dependent object is actually deleted.</p>"},{"location":"kubernetes-features/owner-references/#example4-ownerreferences-blockownerdeletiontrue-finalizer-delete-cascadeforeground","title":"Example4: OwnerReferences + BlockOwnerDeletion=true &amp; Finalizer + delete --cascade=foreground","text":"<ol> <li>Create <code>Person</code> named <code>Alice</code> (same as above)     <pre><code>kubectl apply -f example/person-alice.yaml\n</code></pre></li> <li> <p>Get Alice's uid and update <code>ownerReferences</code> in  <code>example/pet-dog.yaml</code> <pre><code>uid=$(kubectl get -f example/person-alice.yaml -o yaml | yq .metadata.uid); echo $uid\n</code></pre></p> <p><pre><code>yq e -i \".metadata.ownerReferences[0].uid = \\\"$uid\\\"\" example/pet-dog-with-finalizer-and-blockownerdeletion.yaml\n</code></pre> 1. Create a <code>Pet</code> <code>dog</code> <pre><code>kubectl apply -f example/pet-dog-with-finalizer-and-blockownerdeletion.yaml\n</code></pre> 1. Delete alice with <code>--cascade=foreground</code> <pre><code>kubectl delete person alice --cascade=foreground\n</code></pre> The command gets stuck as it waits until all the dependent objects are removed. 1. Remove <code>finalizers</code> manually in another terminal. <pre><code>kubectl patch pet dog -p '{\"metadata\":{\"finalizers\": []}}' --type=merge\n</code></pre></p> <p>Once the finalizer is removed, the command above completes the deletion of the owner object. The pet is also deleted when the finalizer is removed.</p> </li> </ol> <p>the owner object is deleted after the dependent object deletion completed.</p>"},{"location":"kubernetes-features/owner-references/#example5-ownerreferences-blockownerdeletiontrue-finalizer-crd-with-propagationpolicy-background","title":"Example5: OwnerReferences + BlockOwnerDeletion=true &amp; Finalizer + CRD with propagationPolicy: Background","text":"<ol> <li> <p>Install the CRD</p> <pre><code>kubectl apply -f crd/person-deletionpolicy-foreground.yaml,crd/pet.yaml\n</code></pre> </li> </ol>"},{"location":"kubernetes-features/owner-references/#ref","title":"Ref","text":"<ol> <li>https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/</li> <li>https://stackoverflow.com/questions/60153700/foreground-cascading-deletion-not-working-as-documentation-suggests</li> </ol>"},{"location":"kubernetes-features/resource-deletion/","title":"Resource deletion","text":"<ol> <li>finalization</li> <li>removal</li> </ol> <p>For more details in the logic of API server, please read kube-apiserver#delete</p>"},{"location":"kubernetes-features/resource-deletion/#ref","title":"Ref","text":"<ol> <li>https://thenewstack.io/deletion-garbage-collection-kubernetes-objects/</li> </ol>"},{"location":"kubernetes-operator/","title":"Kubernetes Operator Study Journey","text":""},{"location":"kubernetes-operator/#1-try-using-existing-operators","title":"1. Try using existing operators","text":"<ol> <li>prometheus-operator</li> <li>postgres-operator</li> <li>strimzi</li> <li>argocd: appcontroller.go</li> <li>grafana-operator</li> <li>mysql-operator</li> <li>terraform-k8s</li> </ol> <p>And more</p>"},{"location":"kubernetes-operator/#2-understand-what-is-kubernetes-operator","title":"2. Understand what is Kubernetes operator.","text":"<ol> <li>Kubernetes Controller components.</li> <li>How Kubernetes Controlloer works.</li> <li>Custom Resource.</li> </ol> <p>Kubernetes Operator</p> <p>A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user.</p> <p>From What is a Kubernetes operator?</p> <p>Operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components. Operators follow Kubernetes principles, notably the control loop.</p> <p>From https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</p> <p>An Operator is like an automated Site Reliability Engineer for its application.</p> <p>From Kubernetes Operators ~ Automating the Container Orchestration Platform ~</p> <p>Operator vs. Controller</p> <ul> <li>Controller\uff08Custom Controller\uff09:Custom Resource\u306e\u7ba1\u7406\u3092\u884c\u3046Controller\u3002Control Loop\uff08Reconciliation Loop\uff09\u3092\u5b9f\u884c\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8</li> <li>Operator: CRD\u3068Custom Controller\u306e\u30bb\u30c3\u30c8\u3002etcd operator\u3084mysql operator\u306a\u3069\u306e\u3088\u3046\u306b\u3001\u7279\u5b9a\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u7ba1\u7406\u3092\u81ea\u52d5\u5316\u3059\u308b\u305f\u3081\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2</li> </ul> <p>From \u5b9f\u8df5\u5165\u9580Kubernetes\u30ab\u30b9\u30bf\u30e0\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u30fc\u3078\u306e\u9053</p> <ul> <li>Controllers can act on core resources such as deployments or services, which are typically part of the Kubernetes controller manager in the control plane, or can watch and manipulate user-defined custom resources.</li> <li>Operators are controllers that encode some operational knowledge, such as application lifecycle management, along with the custom resources defined in Chapter 4.</li> </ul> <p>From Programming Kubernetes</p> <ul> <li>A controller is a loop that reads desired state (\"spec), observed cluster state (others' \"status\"), and external state, and the reconciles cluster state and external state with the desired state, writing any observations down (to our own \"status\").</li> <li>All of Kubernetes functions on this model.</li> <li>An operator is a controller that encodes human operational knowledge: how do I run and manage a specific piece of complex software.</li> <li>All operators are controllers, but not all controllers are operators.</li> </ul> <p>From Tutorial: Zero to Operator in 90 Minutes! - Solly Ross, Google (YouTube)</p> <p>For more detail: - CNCF Operator White Paper - Final Version - CNCF White Paper</p>"},{"location":"kubernetes-operator/#3-create-a-sample-operator-following-a-tutorial","title":"3. Create a sample operator following a tutorial","text":"<p>There are several ways to create an operator. You can try any of them:</p> <ol> <li>operator-sdk<ol> <li>go-based: https://github.com/nakamasato/memcached-operator</li> <li>helm-based: https://github.com/nakamasato/nginx-operator</li> <li>ansible-based: https://github.com/nakamasato/memcached-operator-with-ansible</li> </ol> </li> <li>kubebuilder<ol> <li>Tutorial: Building CronJob</li> </ol> </li> <li>metacontroller</li> <li>KUDO (Kubernetes Universal Declarative Operator)</li> <li>\u3064\u304f\u3063\u3066\u5b66\u3076Kubebuilder</li> </ol> <p>You can also reference example controllers:</p> <ol> <li>Sample Controller</li> <li>Istio Example Controller</li> <li>Foo Controller with Kubebuilder</li> <li>Memcached Operator with Operator SDK</li> </ol>"},{"location":"kubernetes-operator/#4-understand-more-detail-about-each-component","title":"4. Understand more detail about each component","text":"<p>Simplified:</p> <p></p> <p>Detailed:</p> <p></p> <p>More Detailed:</p> <p></p> <p>from https://github.com/kubernetes/sample-controller/blob/master/docs/images/client-go-controller-interaction.jpeg</p> <ol> <li>client-go:<ol> <li>clientset is a client for the built-in API resources.</li> <li>informer: watch the changes of objects and reflect the changes to the in-memory-cache.<ol> <li>factory: <code>informers.NewSharedInformerFactory</code></li> <li>watcher</li> <li>lister</li> <li>indexer</li> <li>event handler</li> <li>reflector</li> </ol> </li> <li>lister: Get data from in-memory cache.</li> <li>indexer: in-memory cache</li> <li>workqueue: A queue to store items that the controller will process.</li> </ol> </li> <li>code-generator:<ol> <li>Generate codes for clientset for a custom resource.</li> </ol> </li> <li>apimachinery:<ol> <li>Scheme: connects Kubernetes API and Go Types, API version conversion</li> </ol> </li> <li>controller-runtime<ol> <li>builder</li> <li>cache</li> <li>client</li> <li>cluster</li> <li>controller</li> <li>eventhandler</li> <li>inject</li> <li>log</li> <li>manager</li> <li>reconciler</li> <li>source</li> <li>webhook</li> </ol> </li> </ol> <p>Reference:</p> <ul> <li>https://adevjoe.com/post/client-go-informer/</li> <li>https://www.huweihuang.com/kubernetes-notes/code-analysis/kube-controller-manager/sharedIndexInformer.html</li> </ul>"},{"location":"kubernetes-operator/#5-create-your-own-operator","title":"5. Create your own operator","text":"<p>After creating a sample operator, you should have deeper understanding of Kubernetes operator. Now you can think about what kind of problem that you want to resolve by utilizing operator pattern.</p> <p>To clarify a problem to resolve with a new operator, you can reference existing operators:</p> operator role language prometheus-operator Manage Prometheus, Alertmanager and their configuration Golang mysql-operator Manage MySQL cluster Python postgres-operator Manage PostgreSQL cluster (version upgrade, live volume resize, ...) Golang strimzi-kafka-operator Manage Kafka cluster, user, and topic Java ... ... ... <p>Considerations:</p> <ul> <li>Finalizer</li> <li>docs: Owners and Dependents: example</li> <li>Reconciliation Loop<ul> <li>[operator-sdk] Based on the return value of Reconcile() the reconcile Request may be requeued and the loop may be triggered again: (Building a Go-based Memcached Operator using the Operator SDK)     <pre><code>// Reconcile successful - don't requeue\nreturn reconcile.Result{}, nil\n// Reconcile failed due to error - requeue\nreturn reconcile.Result{}, err\n// Requeue for any reason other than error\nreturn reconcile.Result{Requeue: true}, nil\n</code></pre></li> <li>https://github.com/operator-framework/operator-sdk/issues/4209#issuecomment-729916367</li> <li>How can I have separate logic for Create, Update, and Delete events? When reconciling an object can I access its previous state? -&gt; You should not have separate logic. Instead design your reconciler to be idempotent.<ul> <li>Q: How do I have different logic in my reconciler for different types of events (e.g. create, update, delete)? in controller-runtime</li> </ul> </li> </ul> </li> <li>Testing<ul> <li>KUbernetes Testing TooL (kuttl): https://kuttl.dev/ KUTTL is built to support some kubernetes integration test scenarios and is most valuable as an end-to-end (e2e) test harness.</li> <li>Ginkgo (A Golang BDD Testing Framework): https://onsi.github.io/ginkgo/</li> <li>Gomega (Ginkgo's preferred matcher library): https://onsi.github.io/gomega/</li> <li>kubetest2: https://github.com/kubernetes-sigs/kubetest2: Kubetest2 is the framework for launching and running end-to-end tests on Kubernetes. It is intended to be the next significant iteration of kubetest.</li> </ul> </li> <li>Managing Errors<ul> <li>https://cloud.redhat.com/blog/kubernetes-operators-best-practices</li> <li>Return the error in the status of the object.</li> <li>Generate an event describing the error.</li> </ul> </li> <li> <p>Webhook</p> <ul> <li>Admission Webhook<ul> <li>two types:<ul> <li>Mutating Webhook: Make some modifications for a request. e.g. set default value. (Defined with <code>MutatingAdmissionConfiguration</code>)</li> <li>Validating Webhook: Validate a request. (Defined by <code>ValidatingAdmissionConfiguration</code>)</li> </ul> </li> <li>Request: <code>AdmissionReview</code></li> <li>Response: <code>AdmissionReview</code> with <code>response.allowed</code> boolean field. </li> </ul> </li> <li>Conversion Webhook<ul> <li>hub &amp; spoke</li> <li>isConvertible: need to have Hub &amp; all non-Hub types must be able to convert to/from Hub</li> <li>request &amp; response: ConversionReview </li> </ul> </li> </ul> </li> <li> <p>Indexing</p> </li> </ul>"},{"location":"kubernetes-operator/#6-tools","title":"6. Tools","text":"<ul> <li>https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/controller/controllerutil</li> <li>https://github.com/spf13/cobra: a library for creating powerful modern CLI applications &amp; a program to generate applications and command files.<ul> <li>Cobra is used in many Go projects such as Kubernetes, Hugo, and Github CLI to name a few. This list contains a more extensive list of projects using Cobra.</li> </ul> </li> </ul>"},{"location":"kubernetes-operator/#7-study-golang-for-better-code-quality","title":"7. Study Golang for better code quality","text":"<ol> <li>golang-standanrds/project-layout</li> <li>Learn Go with tests</li> <li>Go\u3068Dependency Injection\u306e\u73fe\u5728</li> <li>Go Blog</li> <li>Gopher Reading List</li> <li>Type Embedding</li> </ol>"},{"location":"kubernetes-operator/#8-other-topics","title":"8. Other Topics","text":"<ol> <li>Write Kubernetes Operator in other languages<ul> <li>kopf for Python</li> <li>fabric8io/kubernetes-client for Java</li> <li>java-operator-sdk/java-operator-sdk Build Kubernetes Operators in Java Without Hassle</li> </ul> </li> <li>Optimistic Concurrency Control</li> </ol>"},{"location":"kubernetes-operator/#9-keep-learning","title":"9. Keep learning","text":"<ol> <li>47 Things To Become a Kubernetes Expert</li> <li>Kubernetes API Basics - Resources, Kinds, and Objects</li> <li>Kubernetes API Conventions</li> <li>How To Call Kubernetes API using Simple HTTP Client</li> <li>How To Call Kubernetes API using Go - Types and Common Machinery</li> <li>How To Extend Kubernetes API - Kubernetes vs. Django</li> <li>\u5728\u4e0d\u751f\u6210 crd client \u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7 client-go \u589e\u5220\u6539\u67e5 k8s crd \u8d44\u6e90</li> <li>kubebuilder vs operator-sdk (2019-04-10)</li> <li>client-go \u4e2d\u7684 informer \u6e90\u7801\u5206\u6790</li> <li>Operator Best Practices</li> </ol>"},{"location":"kubernetes-operator/apimachinery/","title":"apimachinery","text":"<p>Scheme, typing, encoding, decoding, and conversion packages for Kubernetes and Kubernetes-like API objects.</p> <p>This library is a shared dependency for servers and clients to work with Kubernetes API infrastructure without direct type dependencies. Its first consumers are k8s.io/kubernetes, k8s.io/client-go, and k8s.io/apiserver.</p> <p>Packages: 1. api 1. apis 1. conversion 1. fields 1. labels 1. runtime 1. selection 1. test 1. types 1. util 1. version 1. watch</p>"},{"location":"kubernetes-operator/apimachinery/#runtime","title":"runtime","text":""},{"location":"kubernetes-operator/apimachinery/#scheme","title":"Scheme","text":"<p>Scheme defines 1. methods for serializing and deserializing API objects, 1. a type registry for converting group, version, and kind information to and from Go schemas, and 1. mappings between Go schemas of different versions.</p> <p>A scheme is the foundation for a versioned API and versioned configuration over time.</p> <p></p> <pre><code>type Scheme struct {\n    // gvkToType allows one to figure out the go type of an object with\n    // the given version and name.\n    gvkToType map[schema.GroupVersionKind]reflect.Type\n\n    // typeToGVK allows one to find metadata for a given go object.\n    // The reflect.Type we index by should *not* be a pointer.\n    typeToGVK map[reflect.Type][]schema.GroupVersionKind\n\n    // unversionedTypes are transformed without conversion in ConvertToVersion.\n    unversionedTypes map[reflect.Type]schema.GroupVersionKind\n\n    // unversionedKinds are the names of kinds that can be created in the context of any group\n    // or version\n    // TODO: resolve the status of unversioned types.\n    unversionedKinds map[string]reflect.Type\n\n    // Map from version and resource to the corresponding func to convert\n    // resource field labels in that version to internal version.\n    fieldLabelConversionFuncs map[schema.GroupVersionKind]FieldLabelConversionFunc\n\n    // defaulterFuncs is a map to funcs to be called with an object to provide defaulting\n    // the provided object must be a pointer.\n    defaulterFuncs map[reflect.Type]func(interface{})\n\n    // converter stores all registered conversion functions. It also has\n    // default converting behavior.\n    converter *conversion.Converter\n\n    // versionPriority is a map of groups to ordered lists of versions for those groups indicating the\n    // default priorities of these versions as registered in the scheme\n    versionPriority map[string][]string\n\n    // observedVersions keeps track of the order we've seen versions during type registration\n    observedVersions []schema.GroupVersion\n\n    // schemeName is the name of this scheme.  If you don't specify a name, the stack of the NewScheme caller will be used.\n    // This is useful for error reporting to indicate the origin of the scheme.\n    schemeName string\n}\n</code></pre>   Methods: 1. Converter 1. AddUnversionedTypes: add 1. AddKnownTypes: add 1. AddKnownTypeWithName: add 1. KnownTypes: 1. VersionsForGroupKind: `GroupKind` -&gt; `[]schema.GroupVersion{}` 1. AllKnownTypes: -&gt; `gvkToType` 1. ObjectKinds: Object -&gt; `GroupVersionKind` 1. Recognizes: GroupVersionKind -&gt; bool 1. IsUnversioned: Object -&gt; bool 1. New: GroupVersionKind -&gt; Object 1. AddIgnoredConversionType 1. AddConversionFunc 1. AddGeneratedConversionFunc 1. AddFieldLabelConversionFunc 1. AddTypeDefaultingFunc 1. Default 1. Convert: convert `in` into `out` 1. ConvertFieldLabel ..."},{"location":"kubernetes-operator/apimachinery/#usage","title":"Usage","text":"<p>In an API types definition, there's <code>register.go</code>.</p> <pre><code>SchemeBuilder      = runtime.NewSchemeBuilder(addKnownTypes)\nlocalSchemeBuilder = &amp;SchemeBuilder\nAddToScheme        = localSchemeBuilder.AddToScheme\n</code></pre> Example: register.go  [apps/v1/register.go](https://github.com/kubernetes/api/blob/v0.24.3/apps/v1/register.go)  <pre><code>// GroupName is the group name use in this package\nconst GroupName = \"apps\"\n\n// SchemeGroupVersion is group version used to register these objects\nvar SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: \"v1\"}\n\n// Resource takes an unqualified resource and returns a Group qualified GroupResource\nfunc Resource(resource string) schema.GroupResource {\n    return SchemeGroupVersion.WithResource(resource).GroupResource()\n}\n\nvar (\n    // TODO: move SchemeBuilder with zz_generated.deepcopy.go to k8s.io/api.\n    // localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes.\n    SchemeBuilder      = runtime.NewSchemeBuilder(addKnownTypes)\n    localSchemeBuilder = &amp;SchemeBuilder\n    AddToScheme        = localSchemeBuilder.AddToScheme\n)\n\n// Adds the list of known types to the given scheme.\nfunc addKnownTypes(scheme *runtime.Scheme) error {\n    scheme.AddKnownTypes(SchemeGroupVersion,\n        &amp;Deployment{},\n        &amp;DeploymentList{},\n        &amp;StatefulSet{},\n        &amp;StatefulSetList{},\n        &amp;DaemonSet{},\n        &amp;DaemonSetList{},\n        &amp;ReplicaSet{},\n        &amp;ReplicaSetList{},\n        &amp;ControllerRevision{},\n        &amp;ControllerRevisionList{},\n    )\n    metav1.AddToGroupVersion(scheme, SchemeGroupVersion)\n    return nil\n}\n</code></pre> <p>What exactly does it mean?</p> <ol> <li> <p><code>SchemeBuilder</code> type is just a container to store functions that are applied to Scheme.</p> <p><pre><code>type SchemeBuilder []func(*Scheme) error\n</code></pre> 1. With <code>NewSchemeBuilder</code>, initialize a <code>SchemeBuilder</code> with the given functions via the arguments. 1. <code>SchemeBuilder.AddToScheme(s *Scheme)</code> is a function to apply all the stored functions to the given Scheme and return error if any of the stored functions fails to apply.</p> </li> </ol> <p>Where is <code>AddToScheme</code> used?</p> <p><code>AddToScheme</code> is used in <code>scheme</code> package of clientset.</p> <p>kubernetes/scheme/register.go registers all the built-in API groups.</p> <pre><code>var Scheme = runtime.NewScheme()\nvar Codecs = serializer.NewCodecFactory(Scheme)\nvar ParameterCodec = runtime.NewParameterCodec(Scheme)\nvar localSchemeBuilder = runtime.SchemeBuilder{\n    admissionregistrationv1.AddToScheme,\n    admissionregistrationv1beta1.AddToScheme,\n    internalv1alpha1.AddToScheme,\n    ...\n}\nvar AddToScheme = localSchemeBuilder.AddToScheme\n\nfunc init() {\n    v1.AddToGroupVersion(Scheme, schema.GroupVersion{Version: \"v1\"})\n    utilruntime.Must(AddToScheme(Scheme))\n}\n</code></pre> <p><code>init()</code> is executed prior to the <code>main()</code> function.</p>"},{"location":"kubernetes-operator/apimachinery/#example","title":"Example","text":"<pre><code>tree\n.\n\u251c\u2500\u2500 main.go\n\u2514\u2500\u2500 scheme\n    \u2514\u2500\u2500 register.go\n\n1 directory, 2 files\n</code></pre> <pre><code>go run main.go\n</code></pre> <pre><code>scheme/register.go is called\nmain\nGroupVersionKind[Group: Version:v1      Kind:Status], reflect.Type: v1.Status\nGroupVersionKind[Group:apps     Version:v1      Kind:DaemonSetList], reflect.Type: v1.DaemonSetList\nGroupVersionKind[Group:apps     Version:v1      Kind:ReplicaSetList], reflect.Type: v1.ReplicaSetList\nGroupVersionKind[Group:apps     Version:v1      Kind:ListOptions], reflect.Type: v1.ListOptions\nGroupVersionKind[Group:apps     Version:v1      Kind:DeleteOptions], reflect.Type: v1.DeleteOptions\nGroupVersionKind[Group: Version:v1      Kind:GetOptions], reflect.Type: v1.GetOptions\nGroupVersionKind[Group: Version:v1      Kind:APIGroup], reflect.Type: v1.APIGroup\nGroupVersionKind[Group:apps     Version:v1      Kind:StatefulSetList], reflect.Type: v1.StatefulSetList\nGroupVersionKind[Group:apps     Version:v1      Kind:ReplicaSet], reflect.Type: v1.ReplicaSet\nGroupVersionKind[Group:apps     Version:v1      Kind:DaemonSet], reflect.Type: v1.DaemonSet\nGroupVersionKind[Group:apps     Version:v1      Kind:ControllerRevision], reflect.Type: v1.ControllerRevision\nGroupVersionKind[Group:apps     Version:v1      Kind:GetOptions], reflect.Type: v1.GetOptions\nGroupVersionKind[Group: Version:__internal      Kind:WatchEvent], reflect.Type: v1.InternalEvent\nGroupVersionKind[Group: Version:v1      Kind:CreateOptions], reflect.Type: v1.CreateOptions\nGroupVersionKind[Group:apps     Version:__internal      Kind:WatchEvent], reflect.Type: v1.InternalEvent\nGroupVersionKind[Group:apps     Version:v1      Kind:CreateOptions], reflect.Type: v1.CreateOptions\nGroupVersionKind[Group:apps     Version:v1      Kind:StatefulSet], reflect.Type: v1.StatefulSet\nGroupVersionKind[Group:apps     Version:v1      Kind:ControllerRevisionList], reflect.Type: v1.ControllerRevisionList\nGroupVersionKind[Group:apps     Version:v1      Kind:WatchEvent], reflect.Type: v1.WatchEvent\nGroupVersionKind[Group:apps     Version:v1      Kind:UpdateOptions], reflect.Type: v1.UpdateOptions\nGroupVersionKind[Group: Version:v1      Kind:DeleteOptions], reflect.Type: v1.DeleteOptions\nGroupVersionKind[Group: Version:v1      Kind:UpdateOptions], reflect.Type: v1.UpdateOptions\nGroupVersionKind[Group: Version:v1      Kind:APIVersions], reflect.Type: v1.APIVersions\nGroupVersionKind[Group:apps     Version:v1      Kind:DeploymentList], reflect.Type: v1.DeploymentList\nGroupVersionKind[Group: Version:v1      Kind:APIGroupList], reflect.Type: v1.APIGroupList\nGroupVersionKind[Group:apps     Version:v1      Kind:Deployment], reflect.Type: v1.Deployment\nGroupVersionKind[Group:apps     Version:v1      Kind:PatchOptions], reflect.Type: v1.PatchOptions\nGroupVersionKind[Group: Version:v1      Kind:WatchEvent], reflect.Type: v1.WatchEvent\nGroupVersionKind[Group: Version:v1      Kind:ListOptions], reflect.Type: v1.ListOptions\nGroupVersionKind[Group: Version:v1      Kind:PatchOptions], reflect.Type: v1.PatchOptions\nGroupVersionKind[Group: Version:v1      Kind:APIResourceList], reflect.Type: v1.APIResourceList\nThose scheme are set in scheme/register.go\n</code></pre>"},{"location":"kubernetes-operator/client-go/","title":"client-go","text":"<p>Version: v0.25.0</p> <ol> <li>clientset: a set of clients to access Kubernetes API</li> <li>indexer: An indexed in-memory key-value store for objects</li> <li>informer<ol> <li>indexer</li> <li>reflector</li> <li>ListerWatcher</li> </ol> </li> <li>lister<ol> <li>indexer</li> </ol> </li> <li>watcher</li> <li>reflector: watches a specified resource with listerwatcher and reflects all changes to the configured store (FIFO).</li> <li>listerwatcher: list and watch the API server. used in reflector.</li> </ol>"},{"location":"kubernetes-operator/client-go/clientset/","title":"clientset","text":""},{"location":"kubernetes-operator/client-go/clientset/#overview","title":"Overview","text":""},{"location":"kubernetes-operator/client-go/clientset/#usage","title":"Usage","text":"<pre><code>clientset.AppsV1().Deployments(\"namespace\").List()\n</code></pre> <ol> <li><code>clientset</code> has set of clients as the name indicates.</li> <li>Get a specific client with <code>AppsV1()</code> for a group version.</li> <li>Get <code>deployment</code> with <code>Deployments()</code> which has operation methods (e.g. <code>Get</code>, <code>Update</code>, <code>Patch</code>, <code>List</code>)</li> </ol>"},{"location":"kubernetes-operator/client-go/clientset/#example","title":"Example","text":"<p>List Pods with client-go:</p> <ol> <li> <p>Get config</p> <pre><code>config, _ := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig)\n</code></pre> </li> <li> <p>Init clientset with config</p> <pre><code>// NewForConfig creates a new Clientset for the given config.\nclientset, _ := kubernetes.NewForConfig(config)\n</code></pre> <p>Internally, call <code>xxxx.NewForConfigAndClient</code> to get a client for each group version.</p> </li> <li> <p>Use the clientset to list Pods     <pre><code>pods, _ := clientset.CoreV1().Pods(\"\").List(context.Background(), metav1.ListOptions{})\n</code></pre></p> </li> </ol> <pre><code>go run podlist.go\n[Pod Name 0]coredns-f9fd979d6-5n4pw\n[Pod Name 1]coredns-f9fd979d6-cp5pl\n[Pod Name 2]etcd-docker-desktop\n[Pod Name 3]kube-apiserver-docker-desktop\n[Pod Name 4]kube-controller-manager-docker-desktop\n[Pod Name 5]kube-proxy-8qp9g\n[Pod Name 6]kube-scheduler-docker-desktop\n[Pod Name 7]storage-provisioner\n[Pod Name 8]vpnkit-controller\n</code></pre>"},{"location":"kubernetes-operator/client-go/clientset/#appendix","title":"Appendix","text":""},{"location":"kubernetes-operator/client-go/deltafifo/","title":"DeltaFIFO","text":""},{"location":"kubernetes-operator/client-go/deltafifo/#overview","title":"Overview","text":"<p>DeltaFIFO is a producer-consumer queue, where a Reflector is intended to be the producer, and the consumer is whatever calls the Pop() method.</p> <ol> <li>The actual data is stored in <code>items</code> in the for of <code>map[string]Deltas</code>.</li> <li>The order is stored in <code>queue</code> as <code>[]string</code>.</li> </ol> <pre><code>type DeltaFIFO struct {\n    lock sync.RWMutex\n    cond sync.Cond\n    items map[string]Deltas\n    queue []string\n    populated bool\n    initialPopulationCount int\n    keyFunc KeyFunc\n    knownObjects KeyListerGetter\n    closed bool\n    emitDeltaTypeReplaced bool\n}\n</code></pre> <pre><code>type Deltas []Delta\ntype DeltaType string\ntype Delta struct {\n    Type   DeltaType\n    Object interface{}\n}\n</code></pre> <ul> <li> <p>Difference between DeltaFIFO and FIFO (Need to summarize the long explanation later)</p> <ol> <li> <p>One is that the accumulator associated with a given object's key is not that object but rather a Deltas, which is a slice of Delta values for that object. Applying an object to a Deltas means to append a Delta except when the potentially appended Delta is a Deleted and the Deltas already ends with a Deleted. In that case the Deltas does not grow, although the terminal Deleted will be replaced by the new Deleted if the older Deleted's object is a DeletedFinalStateUnknown.</p> </li> <li> <p>The other difference is that DeltaFIFO has two additional ways that an object can be applied to an accumulator: Replaced and Sync. If EmitDeltaTypeReplaced is not set to true, Sync will be used in replace events for backwards compatibility. Sync is used for periodic resync events.</p> </li> <li>DeltaFIFO solves this use case</li> <li>You want to process every object change (delta) at most once.</li> <li>When you process an object, you want to see everything that's happened to it since you last processed it.</li> <li>You want to process the deletion of some of the objects.</li> <li>You might want to periodically reprocess objects.</li> </ol> </li> </ul>"},{"location":"kubernetes-operator/client-go/deltafifo/#usage-how-deltafifo-is-used-in-informer","title":"Usage: how DeltaFIFO is used in informer","text":"<ol> <li>Create Indexer</li> <li>Create DeltaFIFO</li> <li>Call <code>fifo.Add(xx)</code> or <code>fifo.Update(xx)</code> or <code>fifo.Delete(xx)</code></li> <li>Call <code>fifo.Pop(process)</code> with <code>process</code> function <code>type PopProcessFunc func(interface{}) error</code>, which converts the object into Deltas and process deltas with <code>processDeltas</code>.     <pre><code>func process(obj interface{}) error { // type PopProcessFunc func(interface{}) error\n    if deltas, ok := obj.(cache.Deltas); ok {\n        return processDeltas(deltas)\n    }\n    return errors.New(\"object given as Process argument is not Deltas\")\n}\n</code></pre></li> <li><code>processDeltas</code> updates/add/delete indexer.</li> </ol> <p>For more details, you can check informer</p>"},{"location":"kubernetes-operator/client-go/indexer/","title":"Indexer","text":""},{"location":"kubernetes-operator/client-go/indexer/#overview","title":"Overview","text":"<p>The <code>Indexer</code> interface:</p> <pre><code>type Indexer interface {\n    Store\n    Index(indexName string, obj interface{}) ([]interface{}, error)\n    IndexKeys(indexName, indexedValue string) ([]string, error)\n    ListIndexFuncValues(indexName string) []string\n    ByIndex(indexName, indexedValue string) ([]interface{}, error)\n    GetIndexers() Indexers\n    AddIndexers(newIndexers Indexers) error\n}\n</code></pre> <p>Indexer extends <code>Store</code> with multiple indices.</p> <p>The <code>Store</code> interface:</p> <pre><code>type Store interface {\n    Add(obj interface{}) error\n    Update(obj interface{}) error\n    Delete(obj interface{}) error\n    List() []interface{}\n    ListKeys() []string\n    Get(obj interface{}) (item interface{}, exists bool, err error)\n    GetByKey(key string) (item interface{}, exists bool, err error)\n    Replace([]interface{}, string) error\n    Resync() error\n}\n</code></pre> <p>Implementation:</p> <pre><code>type cache struct {\n    // cacheStorage bears the burden of thread safety for the cache\n    cacheStorage ThreadSafeStore\n    // keyFunc is used to make the key for objects stored in and retrieved from items, and\n    // should be deterministic.\n    keyFunc KeyFunc\n}\n</code></pre> <p>ThreadSafeStore</p> <pre><code>type ThreadSafeStore interface {\n    Add(key string, obj interface{})\n    Update(key string, obj interface{})\n    Delete(key string)\n    Get(key string) (item interface{}, exists bool)\n    List() []interface{}\n    ListKeys() []string\n    Replace(map[string]interface{}, string)\n    Index(indexName string, obj interface{}) ([]interface{}, error)\n    IndexKeys(indexName, indexKey string) ([]string, error)\n    ListIndexFuncValues(name string) []string\n    ByIndex(indexName, indexKey string) ([]interface{}, error)\n    GetIndexers() Indexers\n    AddIndexers(newIndexers Indexers) error\n    Resync() error\n}\n</code></pre> <p>Implementation <pre><code>type threadSafeMap struct {\n    lock  sync.RWMutex\n    items map[string]interface{}\n    indexers Indexers // indexers maps a name to an IndexFunc\n    indices Indices // indices maps a name to an Index\n}\n</code></pre></p> <p>NewIndexer:</p> <pre><code>func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer {\n    return &amp;cache{\n        cacheStorage: NewThreadSafeStore(indexers, Indices{}),\n        keyFunc:      keyFunc,\n    }\n}\n</code></pre> <p>NewThreadSafeStore:</p> <pre><code>func NewThreadSafeStore(indexers Indexers, indices Indices) ThreadSafeStore {\n    return &amp;threadSafeMap{\n        items:    map[string]interface{}{},\n        indexers: indexers,\n        indices:  indices,\n    }\n}\n</code></pre> <p>Index-related terms: 1. <code>Indexer</code> is an interface (embeds <code>Store</code>) 1. <code>Indexers</code> is a type <code>map[string]IndexFunc</code> 1. <code>Index</code> is a type <code>Index map[string]sets.String</code> 1. <code>Indices</code> is a type <code>type Indices map[string]Index</code></p>"},{"location":"kubernetes-operator/client-go/indexer/#usage","title":"Usage","text":"<ol> <li> <p>Create a indexer with <code>KeyFunc</code> and <code>Indexers</code>.</p> <pre><code>indexer := cache.NewIndexer(\n    cache.MetaNamespaceKeyFunc, // Use &lt;namespace&gt;/&lt;name&gt; as a key if &lt;namespace&gt; exists, otherwise &lt;name&gt;\n    cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, // default index function that indexes based on an object's namespace\n)\n</code></pre> <p>Arguments: 1. <code>KeyFunc</code>: KeyFunc knows how to make a key from an object.</p> <pre><code>```go\ntype KeyFunc func(obj interface{}) (string, error)\n```\n\nExample KeyFunc: [cache.MetaNamespaceKeyFunc](https://pkg.go.dev/k8s.io/client-go/tools/cache#MetaNamespaceKeyFunc) (e.g. object: Deployment with name `test` in `default` namespace -&gt; key: `default/test`)\n</code></pre> <ol> <li> <p><code>Indexers</code>: Indexers maps a name to an IndexFunc.</p> <p>We can have multiple indexes in a store (indexer). e.g. index by namespace, index by label, etc.</p> <pre><code>type Indexers map[string]IndexFunc\n</code></pre> </li> </ol> <p>Dependencies: 1. <code>IndexFunc</code>: IndexFunc knows how to compute the set of indexed values for an object. This function determines which value to use for indexing.</p> <pre><code>```go\ntype IndexFunc func(obj interface{}) ([]string, error)\n```\n\nExample: [cache.MetaNamespaceIndexFunc](https://pkg.go.dev/k8s.io/client-go/tools/cache#MetaNamespaceIndexFunc) (e.g. object: Deployment with name `test` in `default` namespace -&gt; set of indexed values: `[]string{\"default\"}`)\n</code></pre> </li> <li> <p>Set an object to the indexer.</p> <p>Indexer store the object with indexes based on the configured IndexFunc for each indexer.</p> <pre><code>err := indexer.Add(&amp;appsv1.Deployment{...})\n</code></pre> </li> <li> <p>Get objects from the indexer.</p> <pre><code>// List indexer\nobjs := indexer.List()\nfmt.Printf(\"indexer.List got %d objects\\n\", len(objs))\n</code></pre> <pre><code>// Get object by key\nobj, exists, err := indexer.GetByKey(\"default/test\")\n</code></pre> </li> </ol>"},{"location":"kubernetes-operator/client-go/informer/","title":"informer","text":""},{"location":"kubernetes-operator/client-go/informer/#overview","title":"Overview","text":""},{"location":"kubernetes-operator/client-go/informer/#factory-informers","title":"Factory &amp; Informers","text":""},{"location":"kubernetes-operator/client-go/informer/#single-informer","title":"Single Informer","text":"<p>Informer monitors the changes of target resource. An informer is created for each of the target resources if you need to handle multiple resources (e.g. podInformer, deploymentInformer).</p>"},{"location":"kubernetes-operator/client-go/informer/#types","title":"types","text":""},{"location":"kubernetes-operator/client-go/informer/#interface-sharedinformerfactory","title":"Interface SharedInformerFactory","text":"<pre><code>type SharedInformerFactory interface {\n    internalinterfaces.SharedInformerFactory\n    ForResource(resource schema.GroupVersionResource) (GenericInformer, error)\n    WaitForCacheSync(stopCh &lt;-chan struct{}) map[reflect.Type]bool\n\n    Admissionregistration() admissionregistration.Interface\n    Internal() apiserverinternal.Interface\n    Apps() apps.Interface\n    Autoscaling() autoscaling.Interface\n    Batch() batch.Interface\n    Certificates() certificates.Interface\n    Coordination() coordination.Interface\n    Core() core.Interface\n    Discovery() discovery.Interface\n    Events() events.Interface\n    Extensions() extensions.Interface\n    Flowcontrol() flowcontrol.Interface\n    Networking() networking.Interface\n    Node() node.Interface\n    Policy() policy.Interface\n    Rbac() rbac.Interface\n    Scheduling() scheduling.Interface\n    Storage() storage.Interface\n}\n</code></pre>"},{"location":"kubernetes-operator/client-go/informer/#implementation-sharedinformerfactory","title":"Implementation sharedInformerFactory","text":"<pre><code>type sharedInformerFactory struct {\n    client           kubernetes.Interface\n    namespace        string\n    tweakListOptions internalinterfaces.TweakListOptionsFunc\n    lock             sync.Mutex\n    defaultResync    time.Duration\n    customResync     map[reflect.Type]time.Duration\n\n    informers map[reflect.Type]cache.SharedIndexInformer\n    // startedInformers is used for tracking which informers have been started.\n    // This allows Start() to be called multiple times safely.\n    startedInformers map[reflect.Type]bool\n}\n</code></pre> <p>Fields:</p> <ol> <li><code>client</code>: clientset to interact with API server</li> <li><code>namespace</code>: you can specify a namespace or all namespaces (v1.NamespaceAll) by default</li> <li><code>informers</code>: store created informers to start them when <code>factory.Start</code> is called.</li> </ol> <p>Methods: Get group's interface (e.g. <code>Apps()</code>) which returns version interface, and eventually you can get the corresponding informer.</p> <p>How a new informer is created with a Factory: 1. Create a factory.     <pre><code>kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)\n</code></pre> 1. Create a new informer for a target resource. (e.g. <code>Deployment</code>)</p> <pre><code>```go\ndeploymentInformer := kubeInformerFactory.Apps().V1().Deployments()\n```\n\n1. [kubeInformerFactory.Apps()](https://github.com/kubernetes/client-go/blob/v0.25.1/informers/factory.go#L220) returns `apps.New(f, f.namespace, f.tweakListOptions)`\n    1. [apps.New(f, f.namespace, f.tweakListOptions)](https://github.com/kubernetes/client-go/blob/v0.25.0/informers/apps/interface.go#L45) returns `&amp;group{factory: f, namespace: namespace, tweakListOptions: tweakListOptions}`\n    1. `kubeInformerFactory.Apps()` is `&amp;group`.\n1. `kubeInformerFactory.Apps().V1()` is `group.V1()` and [group.V1()](https://github.com/kubernetes/client-go/blob/v0.25.0/informers/apps/interface.go#L50) returns `v1.New(g.factory, g.namespace, g.tweakListOptions)`\n    1. [v1.New](https://github.com/kubernetes/client-go/blob/v0.25.0/informers/apps/v1/interface.go#L46) returns `&amp;version{factory: f, namespace: namespace, tweakListOptions: tweakListOptions}`\n    1. `kubeInformerFactory.Apps().V1()` is `&amp;version`.\n1. `kubeInformerFactory.Apps().V1().Deployments()` is `&amp;version` and [version.Deployments()](https://github.com/kubernetes/client-go/blob/v0.25.0/informers/apps/v1/interface.go#L61) returns `&amp;deploymentInformer{factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions}`.\n    1. [deploymentInformer](https://github.com/kubernetes/client-go/blob/v0.25.0/informers/apps/v1/deployment.go#L42)\n\nNote that there's nothing happening but just creating `deploymentInformer` at this moment.\n</code></pre> <ol> <li> <p>Pass the informer to a controller.     Example:     <code>go     NewController(         deploymentInformer,         ...     )</code></p> <ol> <li>Inside the controller, call <code>deploymentInformer.Informer().AddEventHandler(..)</code> e.g. sample-controller/blob/v0.0.6/controller.go#L102</li> <li>deploymentInformer.Informer() returns <code>f.factory.InformerFor(&amp;appsv1.Deployment{}, f.defaultInformer)</code></li> <li>factory.InformerFor create a new informer and register it to <code>factory.informers</code> &lt;- This is the moment the new informer is created!! <pre><code>informer = newFunc(f.client, resyncPeriod)\nf.informers[informerType] = informer\n</code></pre> <code>newFunc = defaultInformer</code> in this example. (<code>defaultInformer</code> is defined each informer)     e.g. deploymentInformer.defaultInformer <pre><code>func NewFilteredDeploymentInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {\n    return cache.NewSharedIndexInformer(\n        &amp;cache.ListWatch{\n            ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {\n                if tweakListOptions != nil {\n                    tweakListOptions(&amp;options)\n                }\n                return client.AppsV1().Deployments(namespace).List(context.TODO(), options)\n            },\n            WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n                if tweakListOptions != nil {\n                    tweakListOptions(&amp;options)\n                }\n                return client.AppsV1().Deployments(namespace).Watch(context.TODO(), options)\n            },\n        },\n        &amp;appsv1.Deployment{},\n        resyncPeriod,\n        indexers,\n    )\n}\n</code></pre></li> </ol> </li> <li> <p>Start factory.     <pre><code>kubeInformerFactory.Start(stopCh)\n</code></pre></p> <ol> <li>factory.Start() run all the informers in the factory by <code>informer.Run(stopCh)</code></li> </ol> </li> <li> <p>informer.Run: you can reference below</p> </li> </ol>"},{"location":"kubernetes-operator/client-go/informer/#interface-sharedinformer","title":"Interface SharedInformer","text":"<ul> <li>Interface:     SharedInformer     <pre><code>type SharedInformer interface {\n    AddEventHandler(handler ResourceEventHandler)\n    AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration)\n    GetStore() Store\n    GetController() Controller\n    Run(stopCh &lt;-chan struct{})\n    HasSynced() bool\n    LastSyncResourceVersion() string\n    SetWatchErrorHandler(handler WatchErrorHandler) error\n    SetTransform(handler TransformFunc) error\n}\n</code></pre>     SharedIndexInformer     <pre><code>type SharedIndexInformer interface {\n    SharedInformer\n    // AddIndexers add indexers to the informer before it starts.\n    AddIndexers(indexers Indexers) error\n    GetIndexer() Indexer\n}\n</code></pre></li> </ul>"},{"location":"kubernetes-operator/client-go/informer/#implementation-sharedindexinformer","title":"Implementation sharedIndexInformer","text":"<p><pre><code>type sharedIndexInformer struct {\n    indexer    Indexer\n    controller Controller\n    processor             *sharedProcessor\n    cacheMutationDetector MutationDetector\n    listerWatcher ListerWatcher\n    objectType runtime.Object\n    resyncCheckPeriod time.Duration\n    defaultEventHandlerResyncPeriod time.Duration\n    clock clock.Clock\n    started, stopped bool\n    startedLock      sync.Mutex\n    blockDeltas sync.Mutex\n    watchErrorHandler WatchErrorHandler\n    transform TransformFunc\n}\n</code></pre> Components: - Indexer - controller: explained below - sharedProcessor: explained below - LiserWatcher</p> <p>sharedIndexInformer.Run:</p> <ol> <li>Create DeltaFifo by NewDeltaFIFOWithOptions</li> <li>Create Controller with New</li> <li>Run s.cacheMutationDetector.Run</li> <li>Run <code>s.processor.run</code> &lt;- start all listeners. listeners are added via <code>AddEventHandler</code>. (usually with <code>cache.ResourceEventHandlerFuncs{AddFunc: xx, UpdateFunc: xx, DeleteFunc: xx}</code>)</li> <li>Run s.controller.Run &lt;- refer the controller section         1. Create a new Reflector and call r.Run (ListAndWatch is called inside)</li> </ol> <p>NewSharedInformer:</p> <ol> <li>NewSharedInformer: call NewSharedIndexInformer with <code>Indexers{}</code>.     <pre><code>NewSharedIndexInformer(lw, exampleObject, defaultEventHandlerResyncPeriod, Indexers{})\n</code></pre></li> <li>NewSharedIndexInformer <pre><code>func NewSharedIndexInformer(lw ListerWatcher, exampleObject runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer {\n    realClock := &amp;clock.RealClock{}\n    sharedIndexInformer := &amp;sharedIndexInformer{\n        processor:                       &amp;sharedProcessor{clock: realClock},\n        indexer:                         NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers),\n        listerWatcher:                   lw,\n        objectType:                      exampleObject,\n        resyncCheckPeriod:               defaultEventHandlerResyncPeriod,\n        defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod,\n        cacheMutationDetector:           NewCacheMutationDetector(fmt.Sprintf(\"%T\", exampleObject)),\n        clock:                           realClock,\n    }\n    return sharedIndexInformer\n}\n</code></pre></li> </ol>"},{"location":"kubernetes-operator/client-go/informer/#sharedprocessor","title":"sharedProcessor","text":"<p>Role: hold a collection of listeners and distribute a notification object to those listeners.</p> <pre><code>type sharedProcessor struct {\n    listenersStarted bool\n    listenersLock    sync.RWMutex\n    listeners        []*processorListener\n    syncingListeners []*processorListener\n    clock            clock.Clock\n    wg               wait.Group\n}\n</code></pre> <ol> <li><code>Listeners</code> are added for ResourceEventHandler via AddEventHandler</li> <li><code>distribute()</code> calls <code>listener.add</code> to propagate new events to each listener. <code>distribute()</code> is called by <code>informer.OnAdd</code>, <code>informer.OnUpdate</code>,  and <code>informer.OnDelete</code></li> <li><code>run()</code> calls <code>listener.run</code> and <code>listener.pop</code> for all listeners. <code>handler.OnAdd</code>, <code>handler.OnUpdate</code>, <code>handler.OnDelete</code> based on the notification type.</li> </ol>"},{"location":"kubernetes-operator/client-go/informer/#controller","title":"Controller","text":"<p>Role: Run a reflector and enqueue item to Queue from ListerWatcher and process item from the queue with processfunc.</p> <p>Interface:</p> <pre><code>// Controller is a low-level controller that is parameterized by a\n// Config and used in sharedIndexInformer.\ntype Controller interface {\n    // Run does two things.  One is to construct and run a Reflector\n    // to pump objects/notifications from the Config's ListerWatcher\n    // to the Config's Queue and possibly invoke the occasional Resync\n    // on that Queue.  The other is to repeatedly Pop from the Queue\n    // and process with the Config's ProcessFunc.  Both of these\n    // continue until `stopCh` is closed.\n    Run(stopCh &lt;-chan struct{})\n\n    // HasSynced delegates to the Config's Queue\n    HasSynced() bool\n\n    // LastSyncResourceVersion delegates to the Reflector when there\n    // is one, otherwise returns the empty string\n    LastSyncResourceVersion() string\n}\n</code></pre> <p>Implementation: <pre><code>type controller struct {\n    config         Config\n    reflector      *Reflector\n    reflectorMutex sync.RWMutex\n    clock          clock.Clock\n}\n</code></pre></p> <ol> <li>Most things are passed by <code>Config</code> (ListerWatcher, ObjectType, Queue (FifoDeltaQueue))</li> </ol> <p>Run:</p> <ol> <li>Create a Reflector with NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration)</li> <li>Run <code>reflector.Run</code> (details -&gt; ref reflector)<ol> <li><code>ListAndWatch</code></li> <li><code>watchHandler</code>:<ol> <li>event.Added -&gt; store.Add</li> <li>event.Modified -&gt; store.Update</li> <li>event.Deleted -&gt; store.Delete (store = Queue)</li> </ol> </li> </ol> </li> <li>Run processLoop every second.<ol> <li>Pop item from the Queue and process it repeatedly. (Actual process is given by <code>Config.Process</code>, controller is just a container to execute <code>Process</code>)<ul> <li><code>Config.Process</code>: HandleDeltas <code>HandleDeltas</code> calls processDeltas(s, s.indexer, s.transform, deltas)<ul> <li><code>handler</code>: sharedIndexInformer</li> <li><code>clientState</code>: s.indexer</li> </ul> </li> <li>Keep indexer up-to-date by calling <code>indexer.Update()</code>, <code>indexer.Add()</code>, <code>indexer.Delete()</code>.</li> <li>Distribute notification and add object to cacheMutationDetector by calling <code>sharedIndexInformer.OnUpdate()</code>, <code>sharedIndexInformer.OnAdd()</code>, <code>sharedIndexInformer.OnDelete()</code></li> </ul> </li> </ol> </li> </ol>"},{"location":"kubernetes-operator/client-go/informer/#mutationdetector","title":"MutationDetector","text":"<p>Role: Check if a cached object is mutated. Call failurefunc or panic if mutated.</p> <ol> <li>By default, mutation detector is not enabled. (You can skip this components)     <pre><code>var mutationDetectionEnabled = false\n\nfunc init() {\n    mutationDetectionEnabled, _ = strconv.ParseBool(os.Getenv(\"KUBE_CACHE_MUTATION_DETECTOR\"))\n}\n</code></pre></li> <li>Run periodically calls CompareObjects.</li> <li>CompareObjects compares <code>cached</code> and <code>copied</code> of <code>cacheObj</code> in <code>d.cachedObjs</code> and <code>d.retainedCachedObjs</code>.     <pre><code>type cacheObj struct {\n    cached interface{}\n    copied interface{}\n}\n</code></pre></li> <li>If any object is altered, call <code>failureFunc</code>. (if created with NewCacheMutationDetector, it doesn't have failureFunc, the program goes <code>panic</code>)</li> <li>AddObject adds an object to <code>d.addedObjs</code>.</li> <li>Test: you can enable mutation detector and you'll get error <code>panic: cache *v1.Pod modified</code>.     <pre><code>KUBE_CACHE_MUTATION_DETECTOR=true go run informer.go\n</code></pre></li> </ol>"},{"location":"kubernetes-operator/client-go/informer/#example","title":"Example","text":"<ol> <li>Initialize clientset with <code>.kube/config</code></li> <li>Create an informer factory with the following line.     <pre><code>informerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30)\n</code></pre>     The second argument specifies ResyncPeriod, which defines the interval of resync (The resync operation consists of delivering to the handler an update notification for every object in the informer's local cache). For more detail, please read NewSharedInformer</li> <li> <p>Create an informer for Pods, which watches Pod's changes.     <pre><code>podInformer := informerFactory.Core().V1().Pods()\n</code></pre></p> <p>factory -&gt; group -&gt; version -&gt; kind</p> <pre><code>type PodInformer interface {\n    Informer() cache.SharedIndexInformer\n    Lister() v1.PodLister\n}\n</code></pre> <ol> <li><code>Informer()</code> returns <code>SharedIndexInformer</code><ol> <li>call <code>f.factory.InformerFor(&amp;corev1.Pod{}, f.defaultInformer)</code></li> <li>create new informer with NewFilteredPodInformer if not exist</li> <li>return the informer</li> </ol> </li> <li><code>Lister()</code> returns PodLister<ol> <li>call <code>v1.NewPodLister(f.Informer().GetIndexer())</code></li> <li>NewPodLister returns podLister with the given indexer.     <pre><code>type podLister struct {\n    indexer cache.Indexer\n}\n</code></pre></li> </ol> </li> </ol> </li> <li> <p>Add event handlers (<code>AddFunc</code>, <code>UpdateFunc</code>, and <code>DeleteFunc</code>) to the pod informer.     <pre><code>podInformer.Informer().AddEventHandler(\n    cache.ResourceEventHandlerFuncs{\n        AddFunc:    handleAdd,\n        UpdateFunc: handleUpdate,\n        DeleteFunc: handleDelete,\n    },\n)\n</code></pre></p> <p><code>handleAdd</code>, <code>handleUpdate</code>, and <code>handleDelete</code> define custom logic for each event. In this example, just print <code>\"handleXXX is called\"</code></p> </li> <li> <p>Create a stop channel and start the factory.     <pre><code>ch := make(chan struct{}) // stop channel\ninformerFactory.Start(ch)\n</code></pre></p> </li> <li> <p>Wait until the cache is synced.     <pre><code>cacheSynced := podInformer.Informer().HasSynced\nif ok := cache.WaitForCacheSync(ch, cacheSynced); !ok {\n    log.Printf(\"cache is not synced\")\n}\nlog.Println(\"cache is synced\")\n</code></pre></p> <p>WaitForCacheSync</p> <pre><code>func WaitForCacheSync(stopCh &lt;-chan struct{}, cacheSyncs ...InformerSynced) bool {\n    err := wait.PollImmediateUntil(syncedPollPeriod,\n        func() (bool, error) {\n            for _, syncFunc := range cacheSyncs {\n                if !syncFunc() {\n                    return false, nil\n                }\n            }\n            return true, nil\n        },\n        stopCh)\n    if err != nil {\n        klog.V(2).Infof(\"stop requested\")\n        return false\n    }\n\n    klog.V(4).Infof(\"caches populated\")\n    return true\n}\n</code></pre> <p>wait.PollImmediateUntil</p> <pre><code>// ContextForChannel derives a child context from a parent channel.\n//\n// The derived context's Done channel is closed when the returned cancel function\n// is called or when the parent channel is closed, whichever happens first.\n//\n// Note the caller must *always* call the CancelFunc, otherwise resources may be leaked.\nfunc ContextForChannel(parentCh &lt;-chan struct{}) (context.Context, context.CancelFunc) {\n    ctx, cancel := context.WithCancel(context.Background())\n\n    go func() {\n        select {\n        case &lt;-parentCh:\n            cancel()\n        case &lt;-ctx.Done():\n        }\n    }()\n    return ctx, cancel\n}\n</code></pre> </li> <li> <p>Run <code>run</code> function every 10 seconds     <pre><code>go wait.Until(run, time.Second*10, ch)\n&lt;-ch\n</code></pre></p> </li> </ol>"},{"location":"kubernetes-operator/client-go/informer/#run-and-check","title":"Run and check","text":"<ol> <li> <p>Run     <pre><code>go run informer.go\n</code></pre></p> </li> <li> <p>All Pods are synced in the cache.</p> <p><pre><code>2021/12/21 09:05:08 handleAdd is called for Pod (key: local-path-storage/local-path-provisioner-547f784dff-lhwfk)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/kube-scheduler-kind-control-plane)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/etcd-kind-control-plane)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/kube-apiserver-kind-control-plane)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/kindnet-nzc7p)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/coredns-558bd4d5db-b4wjg)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/kube-controller-manager-kind-control-plane)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/kube-proxy-vrcbc)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: kube-system/coredns-558bd4d5db-8q78s)\n2021/12/21 09:05:08 handleAdd is called for Pod (key: default/foo-sample-688594b488-782kw)\n2021/12/21 09:05:08 cache is synced\n2021/12/21 09:05:08 run\n</code></pre> 1. Create a <code>Pod</code> with name <code>nginx</code>. <pre><code>kubectl run nginx --image=nginx\n</code></pre> 1. Handlers are called by the events of the created <code>Pod</code>. <pre><code>2021/12/21 09:05:18 run\n2021/12/21 09:05:20 handleAdd is called for Pod (key: default/nginx)\n2021/12/21 09:05:20 handleUpdate is called for Pod (key: default/nginx)\n2021/12/21 09:05:20 handleUpdate is called for Pod (key: default/nginx)\n</code></pre> 1. Delete the <code>Pod</code> <pre><code>kubectl delete po nginx\n</code></pre> 1. Handlers are called by the events of the Pod deletion. <pre><code>2021/12/21 09:05:29 handleUpdate is called for Pod (key: default/nginx)\n2021/12/21 09:05:30 handleUpdate is called for Pod (key: default/nginx)\n2021/12/21 09:05:31 handleUpdate is called for Pod (key: default/nginx)\n2021/12/21 09:05:31 handleUpdate is called for Pod (key: default/nginx)\n2021/12/21 09:05:31 handlDelete is called for Pod (key: default/nginx)\n</code></pre> 1. <code>run</code> function is called every 10 seconds. <pre><code>2021/12/21 09:26:08 run\n2021/12/21 09:26:18 run\n2021/12/21 09:26:28 run\n</code></pre> 1. The cached is resynced every 30 seconds.</p> <pre><code>2021/12/21 09:27:08 handleUpdate is called for Pod (key: local-path-storage/local-path-provisioner-547f784dff-lhwfk)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/kube-apiserver-kind-control-plane)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/coredns-558bd4d5db-b4wjg)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/kube-controller-manager-kind-control-plane)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/coredns-558bd4d5db-8q78s)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: default/foo-sample-688594b488-782kw)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/kube-scheduler-kind-control-plane)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/etcd-kind-control-plane)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/kindnet-nzc7p)\n2021/12/21 09:27:08 handleUpdate is called for Pod (key: kube-system/kube-proxy-vrcbc)\n</code></pre> </li> </ol>"},{"location":"kubernetes-operator/client-go/informer/#reference","title":"reference","text":"<ul> <li>https://adevjoe.com/post/client-go-informer/</li> <li>https://www.huweihuang.com/kubernetes-notes/code-analysis/kube-controller-manager/sharedIndexInformer.html</li> <li>https://yangxikun.com/kubernetes/2020/03/05/informer-lister.html</li> </ul>"},{"location":"kubernetes-operator/client-go/lister/","title":"Lister","text":""},{"location":"kubernetes-operator/client-go/lister/#overview","title":"Overview","text":"<p>Lister is any object that knows how to perform an initial list.</p> <p>Interface:</p> <pre><code>type Lister interface {\n    // List should return a list type object; the Items field will be extracted, and the\n    // ResourceVersion field will be used to start the watch in the right place.\n    List(options metav1.ListOptions) (runtime.Object, error)\n}\n</code></pre>"},{"location":"kubernetes-operator/client-go/lister/#usage","title":"Usage","text":"<ol> <li> <p>Prepare Indexer (dependency). Read indexer for more details.</p> <pre><code>indexer := cache.NewIndexer(\n    cache.MetaNamespaceKeyFunc,\n    cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc},\n)\n</code></pre> <p>Add object to indexer with/without labels (which will be used as selector when listing with Lister below).</p> <pre><code>// Add deployment with label\nerr := indexer.Add(getDeployment(\"deployment-with-label\", map[string]string{\"watch\": \"true\"}))\nif err != nil {\n    t.Errorf(\"indexer.Add failed %v\\n\", err)\n}\n// Add deployment without label\nerr = indexer.Add(getDeployment(\"deployment-without-label\", map[string]string{}))\nif err != nil {\n    t.Errorf(\"indexer.Add failed %v\\n\", err)\n}\n</code></pre> </li> <li> <p>Create a Lister for a target resource.</p> <p>For a built-in resource:</p> <p><pre><code>import (\n    appsv1lister \"k8s.io/client-go/listers/apps/v1\"\n)\n</code></pre> <pre><code>deploymentLister := appsv1lister.NewDeploymentLister(indexer)\n</code></pre></p> <p>For a custom resource: Need to generate lister with code-generator</p> </li> <li> <p>List object with selector.</p> <pre><code>selector := labels.SelectorFromSet(labels.Set{\"watch\": \"true\"})\nret, _ := deploymentLister.List(selector)\n\nfor _, deploy := range ret {\n    fmt.Println(deploy.Name)\n}\n</code></pre> <p>Objects are got from the indexers.</p> </li> </ol>"},{"location":"kubernetes-operator/client-go/listerwatcher/","title":"ListerWatcher","text":""},{"location":"kubernetes-operator/client-go/listerwatcher/#type","title":"type","text":""},{"location":"kubernetes-operator/client-go/listerwatcher/#interface","title":"Interface","text":"<pre><code>// ListerWatcher is any object that knows how to perform an initial list and start a watch on a resource.\ntype ListerWatcher interface {\n    Lister\n    Watcher\n}\n</code></pre> <pre><code>// Lister is any object that knows how to perform an initial list.\ntype Lister interface {\n    // List should return a list type object; the Items field will be extracted, and the\n    // ResourceVersion field will be used to start the watch in the right place.\n    List(options metav1.ListOptions) (runtime.Object, error)\n}\n\n// Watcher is any object that knows how to start a watch on a resource.\ntype Watcher interface {\n    // Watch should begin a watch at the specified version.\n    Watch(options metav1.ListOptions) (watch.Interface, error)\n}\n</code></pre>"},{"location":"kubernetes-operator/client-go/listerwatcher/#listwatch-struct","title":"ListWatch struct","text":"<pre><code>// ListFunc knows how to list resources\ntype ListFunc func(options metav1.ListOptions) (runtime.Object, error)\n\n// WatchFunc knows how to watch resources\ntype WatchFunc func(options metav1.ListOptions) (watch.Interface, error)\n\n// ListWatch knows how to list and watch a set of apiserver resources.  It satisfies the ListerWatcher interface.\n// It is a convenience function for users of NewReflector, etc.\n// ListFunc and WatchFunc must not be nil\ntype ListWatch struct {\n    ListFunc  ListFunc\n    WatchFunc WatchFunc\n    // DisableChunking requests no chunking for this list watcher.\n    DisableChunking bool\n}\n</code></pre> <p>watch.Interface:</p> <pre><code>type Interface interface {\n    // Stop stops watching. Will close the channel returned by ResultChan(). Releases\n    // any resources used by the watch.\n    Stop()\n\n    // ResultChan returns a chan which will receive all the events. If an error occurs\n    // or Stop() is called, the implementation will close this channel and\n    // release any resources used by the watch.\n    ResultChan() &lt;-chan Event\n}\n</code></pre>"},{"location":"kubernetes-operator/client-go/listerwatcher/#how-listwatch-is-used","title":"How ListWatch is used","text":"<ol> <li> <p>Created with NewFilteredListWatchFromClient:</p> <pre><code>func NewFilteredListWatchFromClient(c Getter, resource string, namespace string, optionsModifier func(options *metav1.ListOptions)) *ListWatch {\n    listFunc := func(options metav1.ListOptions) (runtime.Object, error) {\n        optionsModifier(&amp;options)\n        return c.Get().\n            Namespace(namespace).\n            Resource(resource).\n            VersionedParams(&amp;options, metav1.ParameterCodec).\n            Do(context.TODO()).\n            Get()\n    }\n    watchFunc := func(options metav1.ListOptions) (watch.Interface, error) {\n        options.Watch = true\n        optionsModifier(&amp;options)\n        return c.Get().\n            Namespace(namespace).\n            Resource(resource).\n            VersionedParams(&amp;options, metav1.ParameterCodec).\n            Watch(context.TODO())\n    }\n    return &amp;ListWatch{ListFunc: listFunc, WatchFunc: watchFunc}\n}\n</code></pre> </li> <li> <p>Getter can be got from <code>clientset</code>: <code>clientset.CoreV1().RESTClient()</code></p> <p><pre><code>podListWatcher := cache.NewListWatchFromClient(clientset.CoreV1().RESTClient(), \"pods\", v1.NamespaceDefault, fields.Everything())\n</code></pre> 1. listFunc and watchFunc is set in <code>NewFilteredListWatchFromClient</code>: 1. <code>c.Get()</code> calls <code>NewRequest().Verb(\"GET\")</code> 1. rest.NewRequest creates and returns a request. 1. Request.Watch     1. get retry func     1. in a for loop, run <code>retry.Before</code>, <code>client.Do(req)</code>, and <code>retry.After</code></p> </li> <li> <p>Then, listwatcher will be passed to informer.     <pre><code>indexer, informer := cache.NewIndexerInformer(podListWatcher, &amp;v1.Pod{}, 0, cache.ResourceEventHandlerFuncs{...\n</code></pre> <code>ListWatcher</code> is used to initialize the store (FIFODelta) with <code>List</code> and keep the store up-to-date with <code>Watch</code>. For more details, you can check informer</p> </li> </ol>"},{"location":"kubernetes-operator/client-go/listerwatcher/#example","title":"Example","text":"<p>Code: You can get the event through <code>w.ResultChan()</code>. The example is simplified version (no error handling).</p> <pre><code>    w, err := podListWatcher.Watch(metav1.ListOptions{}) // returns watch.Interface\n    if err != nil {\n        klog.Fatal(err)\n    }\nloop:\n    for {\n        event, ok := &lt;-w.ResultChan()\n        if !ok {\n            break loop\n        }\n\n        meta, err := meta.Accessor(event.Object)\n        if err != nil {\n            continue\n        }\n        resourceVersion := meta.GetResourceVersion()\n        klog.Infof(\"event: %s, resourceVersion: %s\", event.Type, resourceVersion)\n    }\n</code></pre> <p>Run:</p> <ol> <li>Start the ListerWatcher for Pods.     <pre><code>go run main.go\nI0913 07:54:29.394053   92277 main.go:57] resourceVersion: 2728\n</code></pre></li> <li>Create a Pod     <pre><code>kubectl run nginx --image=nginx\n</code></pre>     You'll see the event logs:     <pre><code>I0913 07:54:29.394239   92277 main.go:64] items: 1\nI0913 07:54:29.401483   92277 main.go:84] event: ADDED, resourceVersion: 503\nI0913 07:55:20.475959   92277 main.go:84] event: MODIFIED, resourceVersion: 2789\nI0913 07:55:38.769688   92277 main.go:84] event: MODIFIED, resourceVersion: 2812\n</code></pre></li> <li>Patch the Pod     <pre><code>kubectl patch pod nginx -p '{\"metadata\":{\"annotations\": {\"key\": \"val\"}}}' --type=merge\n</code></pre>     You'll see <code>MODIFIED</code> in the logs.</li> <li> <p>Delete the Pod     <pre><code>kubectl delete pod nginx\n</code></pre></p> <pre><code>I0913 08:02:23.486861   92277 main.go:84] event: MODIFIED, resourceVersion: 3294\nI0913 08:02:23.929399   92277 main.go:84] event: MODIFIED, resourceVersion: 3298\nI0913 08:02:24.239499   92277 main.go:84] event: MODIFIED, resourceVersion: 3300\nI0913 08:02:24.244502   92277 main.go:84] event: DELETED, resourceVersion: 3301\n</code></pre> </li> </ol> <p>Referece: example</p>"},{"location":"kubernetes-operator/client-go/reflector/","title":"Reflector","text":""},{"location":"kubernetes-operator/client-go/reflector/#overview","title":"Overview","text":"<p>Reflector watches a specified resource and causes all changes to be reflected in the given store.</p> <pre><code>// Reflector watches a specified resource and causes all changes to be reflected in the given store.\ntype Reflector struct {\n    name string\n    expectedTypeName string\n    expectedType reflect.Type\n    expectedGVK *schema.GroupVersionKind\n    store Store\n    listerWatcher ListerWatcher\n    backoffManager wait.BackoffManager\n    initConnBackoffManager wait.BackoffManager\n    resyncPeriod time.Duration\n    ShouldResync func() bool\n    clock clock.Clock\n    paginatedResult bool\n    lastSyncResourceVersion string\n    isLastSyncResourceVersionUnavailable bool\n    lastSyncResourceVersionMutex sync.RWMutex\n    WatchListPageSize int64\n    watchErrorHandler WatchErrorHandler\n}\n</code></pre> <ol> <li><code>store</code>: DeltaFIFO can be used for store.</li> <li> <p><code>reflector.ListAndWatch</code> function is called in <code>Run</code>. (tools/cache/reflector.go#L223)</p> <pre><code>// Run repeatedly uses the reflector's ListAndWatch to fetch all the\n// objects and subsequent deltas.\n// Run will exit when stopCh is closed.\nfunc (r *Reflector) Run(stopCh &lt;-chan struct{}) {\n    klog.V(3).Infof(\"Starting reflector %s (%s) from %s\", r.expectedTypeName, r.resyncPeriod, r.name)\n    wait.BackoffUntil(func() {\n        if err := r.ListAndWatch(stopCh); err != nil {\n            r.watchErrorHandler(r, err)\n        }\n    }, r.backoffManager, true, stopCh)\n    klog.V(3).Infof(\"Stopping reflector %s (%s) from %s\", r.expectedTypeName, r.resyncPeriod, r.name)\n}\n</code></pre> <p>wait.BackoffUntil: BackoffUntil loops until stop channel is closed, run f every duration given by BackoffManager. If sliding is true, the period is computed after f runs. If it is false then period includes the runtime for f.</p> </li> <li> <p><code>r.ListAndWatch</code> function calls</p> <ol> <li>Call <code>list</code> func (reflector.go#357)<ol> <li>Get resourceVersion (v1.meta/ListMeta - The metadata.resourceVersion of a resource collection (the response to a list) identifies the resource version at which the collection was constructed.) from api concept)</li> <li>Get all items and call syncWith(items, resourceVersion)</li> <li>syncWith replaces the store's items with the given items.     <pre><code>r.store.Replace(found, resourceVersion)\n</code></pre></li> </ol> </li> <li>In a for loop, call <code>r.store.Resync()</code> periodically if resync is necessary.</li> <li>In a for loop, call listerwatcher.Watch     <pre><code>w, err := listerwatcher.Watch()\n</code></pre></li> <li>Call the watchHandler <pre><code>watchHandler(start, w, r.store, r.expectedType, r.expectedGVK, r.name, r.expectedTypeName, r.setLastSyncResourceVersion, r.clock, resyncerrc, stopCh)\n</code></pre>     -&gt; <code>store.Add</code>, <code>store.Update</code>, <code>store.Delete</code></li> </ol> </li> </ol>"},{"location":"kubernetes-operator/client-go/reflector/#usage","title":"Usage","text":"<p>TBD</p>"},{"location":"kubernetes-operator/client-go/workqueue/","title":"workqueue","text":""},{"location":"kubernetes-operator/controller-runtime/","title":"controller-runtime","text":"<p>controller-runtime is a subproject of kubebuilder which provides a lot of useful tools that help develop Kubernetes Operator.</p> <p>Version: v0.13.0</p>"},{"location":"kubernetes-operator/controller-runtime/#overview","title":"Overview","text":"<ol> <li>Create a Manager.<ol> <li>Cluster, which has <code>Cache</code>, <code>Client</code>, <code>Scheme</code>, etc, is created internally. Read cluster for more details.</li> </ol> </li> <li>Create one or multiple Reconcilers. For more details, read reconciler.</li> <li>Build the <code>Reconciler</code>(s) with the <code>Manager</code> using <code>Builder</code>.<ol> <li>Internally, <code>builder.doWatch</code> and <code>builder.doController</code> are called.</li> <li>bldr.doController calls newController to create a new Controller and add it to <code>manager.runnables.Others</code> by <code>Manager.Add(Runnable)</code>. (controller)<ol> <li>Inject dependencies to Reconciler and Controller. e.g. Cache.</li> </ol> </li> <li>bldr.doWatch creates Kind (Source) and call controller.Watch for <code>For</code>, <code>Owns</code>, and <code>Watches</code>.<ol> <li>controller.Watch calls <code>source.Start()</code>, which gets informer from the injected cache and add the event handler.</li> </ol> </li> </ol> </li> <li>Start the <code>Manager</code>, which trigger to start all the <code>mgr.runnables</code> (<code>Caches</code>, <code>Webhooks</code>, <code>Others</code>) in the <code>Manager</code>.<ol> <li><code>Informer.Run</code> is called in <code>cm.runnables.Caches.Start(cm.internalCtx)</code></li> </ol> </li> </ol> <p>For more details, you can check the architecture in book.kubebuilder.io: </p> <p>List of components:</p> <ol> <li>Manager: Package manager is required to create Controllers and provides shared dependencies such as clients, caches, schemes, etc.</li> <li>Controller: Package controller provides types and functions for building Controllers. The Controller MUST be started by calling Manager.Start.<ol> <li>Event</li> <li>Builder</li> <li>Source</li> <li>Handler</li> <li>Predicate</li> </ol> </li> <li>Client: Package client contains functionality for interacting with Kubernetes API servers.<ol> <li>delegatingClient: The default client type whose Get and List get object from cache.CacheReader, which reduces the API requests to API server.</li> </ol> </li> <li>Cache<ol> <li>client.Reader: Cache acts as a client to objects stored in the cache.</li> <li>Informers: Cache loads informers and adds field indices.</li> </ol> </li> <li>Scheme: Wraps apimachinery/Scheme.</li> <li>Webhook</li> <li>Envtest</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/#components","title":"Components","text":"<ol> <li>manager</li> <li>reconciler</li> <li>log</li> <li>controller</li> <li>cluster<ol> <li>client</li> <li>cache</li> </ol> </li> <li>inject</li> <li>source</li> <li>builder</li> <li>handler</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/#examples","title":"Examples","text":"<ol> <li>example-controller</li> <li>envtest</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/#memo","title":"Memo","text":"<ol> <li>v0.11.0: Allow Specification of the Log Timestamp Format. -&gt; Default EpochTimeEncoder</li> <li> <p>v0.15.0</p> <ol> <li> <p>\u26a0\ufe0f Refactor source/handler/predicate packages to remove dep injection #2120</p> <pre><code>-       kindWithCacheMysqlUser := source.NewKindWithCache(mysqluser, cache)\n-       kindWithCacheMysql := source.NewKindWithCache(mysql, cache)\n-       kindWithCachesecret := source.NewKindWithCache(secret, cache)\n+       kindWithCacheMysqlUser := source.Kind(cache, mysqluser)\n+       kindWithCacheMysql := source.Kind(cache, mysql)\n+       kindWithCachesecret := source.Kind(cache, secret)\n</code></pre> </li> <li> <p>Example PR: https://github.com/nakamasato/secret-mirror-operator/pull/28</p> </li> </ol> </li> <li> <p>v0.16.0</p> <ol> <li> <p>\u26a0 Introduce Metrics Options struct &amp; secure metrics serving #2407</p> <pre><code>import (\n+ metricsserver \"sigs.k8s.io/controller-runtime/pkg/metrics/server\"\n)\n- MetricsBindAddress: metricsAddr\n+ Metrics: metricsserver.Options{BindAddress: metricsAddr},\n</code></pre> </li> <li> <p>\u26a0 Remove deprecated manager, webhook and cluster options #2422</p> </li> <li>Example PR: https://github.com/nakamasato/secret-mirror-operator/pull/28</li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/builder/","title":"builder","text":""},{"location":"kubernetes-operator/controller-runtime/builder/#overview","title":"Overview","text":"<p>The main role of Builder is:</p> <ol> <li>Create a Controller from the given Reconciler</li> <li>Configure target resources for the controller</li> <li>Register the controller to the Manager</li> </ol> <p>About how the registered controllers are triggered, you can study in Manager. The controller registered to the manager by Builder will be in runnables.Others in Manager object, which will be started by <code>Manager.Start()</code>.</p> <p></p>"},{"location":"kubernetes-operator/controller-runtime/builder/#types","title":"Types","text":""},{"location":"kubernetes-operator/controller-runtime/builder/#builder_1","title":"Builder","text":"<pre><code>// Builder builds a Controller.\ntype Builder struct {\n    forInput         ForInput\n    ownsInput        []OwnsInput\n    watchesInput     []WatchesInput\n    mgr              manager.Manager\n    globalPredicates []predicate.Predicate\n    ctrl             controller.Controller\n    ctrlOptions      controller.Options\n    name             string\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/builder/#controllermanagedby-initialize-builder-with-a-manager","title":"<code>ControllerManagedBy</code>: Initialize Builder with a Manager","text":"<p>Initialize a Builder with the specified manager.</p> <pre><code>func ControllerManagedBy(m manager.Manager) *Builder {\n return &amp;Builder{mgr: m}\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/builder/#for-owns-and-watches-define-what-object-to-watch","title":"For, Owns, and Watches: Define what object to watch","text":"<ol> <li><code>For(object client.Object, opts ...ForOption) *Builder</code>: only one resource can be configured. Same as     <pre><code>Watches(&amp;source.Kind{Type: apiType}, &amp;handler.EnqueueRequestForObject{})\n</code></pre></li> <li><code>Owns(object client.Object, opts ...OwnsOption) *Builder</code>: Owns defines types of Objects being generated by the ControllerManagedBy, and configures the ControllerManagedBy to respond to create / delete / update events by reconciling the owner object. Same as the following code:     <pre><code>Watches(object, handler.EnqueueRequestForOwner([...], ownerType, OnlyControllerOwner()))\n</code></pre> EnqueueRequestForOwner: Extract owner object from ownerReferences and enqueue it to the queue.</li> <li><code>Watches(src source.Source, eventhandler handler.EventHandler, opts ...WatchesOption) *Builder</code>: Watches exposes the lower-level ControllerManagedBy Watches functions through the builder. Consider using Owns or For instead of Watches directly.</li> </ol> <p>Example:</p> <pre><code>err = builder.\n  ControllerManagedBy(mgr).  // Create the ControllerManagedBy\n  For(&amp;alpha1v1.Foo{}). // Foo is the Application API\n  Owns(&amp;corev1.Pod{}).       // Foo owns Pods created by it\n  Complete(&amp;FooReconciler{})\n</code></pre> <p></p>"},{"location":"kubernetes-operator/controller-runtime/builder/#complete-receive-reconciler-and-build-a-controller","title":"<code>Complete</code>: Receive reconciler and build a controller","text":"<p>Receive <code>reconcile.Reconciler</code> and call <code>Build</code>:</p> <pre><code>// Complete builds the Application Controller.\nfunc (blder *Builder) Complete(r reconcile.Reconciler) error {\n    _, err := blder.Build(r)\n    return err\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/builder/#build-create-a-controller-and-return-the-controller","title":"<code>Build</code>: Create a controller and return the controller.","text":"<pre><code>func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) {\n    ...\n    // Set the ControllerManagedBy\n    if err := blder.doController(r); err != nil {\n        return nil, err\n    }\n\n    // Set the Watch\n    if err := blder.doWatch(); err != nil {\n        return nil, err\n    }\n    return blder.ctrl, nil\n}\n</code></pre> <ol> <li>bldr.doController to register the controler to the buidler<ol> <li>Create a new controller.     <pre><code>blder.ctrl, err = newController(controllerName, blder.mgr, ctrlOptions)\n</code></pre></li> <li>the controller is added to <code>manager.runnables.Others</code> by <code>Manager.Add(Runnable)</code> in <code>newController</code>. (controller)</li> </ol> </li> <li>bldr.doWatch to start watching the target resources configured by <code>For</code>, <code>Owns</code>, and <code>Watches</code>.<ol> <li>The actual implementation of <code>Watch</code> function is in the controller</li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/builder/#convert-clientobject-to-source","title":"Convert <code>client.Object</code> to <code>Source</code>","text":"<ol> <li>Controller.Watch needs <code>Source</code> as the first argument.     <pre><code>Watch(src source.Source, eventhandler handler.EventHandler, predicates ...predicate.Predicate) error\n</code></pre></li> <li><code>client.Object</code> is set in <code>ForInput</code>, <code>OwnsInput</code>, and <code>WatchesInput</code> for <code>For</code>, <code>Owns</code>, and <code>Watches</code> respectively.</li> <li> <p>Before calling <code>Controller.Watch</code>, the <code>client.Object</code> needs to be projected into Source based on <code>objectProjection</code>:</p> <ol> <li><code>projectAsNormal</code>: Use the object as it is. (In most cases)</li> <li><code>projectAsMetadata</code>: Extract only metadata.</li> </ol> <pre><code>typeForSrc, err := blder.project(blder.forInput.object, blder.forInput.objectProjection)\nif err != nil {\n    return err\n}\nsrc := &amp;source.Kind{Type: typeForSrc}\n</code></pre> <p>Kind implements the Source interface.</p> <pre><code>type Kind struct {\n    // Type is the type of object to watch.  e.g. &amp;v1.Pod{}\n    Type client.Object\n\n    // cache used to watch APIs\n    cache cache.Cache\n\n    // started may contain an error if one was encountered during startup. If its closed and does not\n    // contain an error, startup and syncing finished.\n    started     chan error\n    startCancel func()\n}\n</code></pre> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/cache/","title":"cache","text":"<ol> <li>So-called \"Cache\" in stored in cluster is informerCache.</li> <li>informerCache implements Cache, Informers, and client.Reader interfaces.</li> <li>informerCache has a specificInformersMap for structured, unstructured, and metadata.<ol> <li>What is structured, unstructured. and metadata? ref: Unstructured, Caching unstrctured objects using controller-runtime</li> </ol> </li> <li>specificInformersMap has several fields but the most important field is informersByGVK.</li> <li>informersByGVK, as the variable name indicates, is a map from GroupVersionKind to MapEntry</li> <li>MapEntry is a pair of <code>cache.SharedIndexInformer</code> and <code>CacheReader</code></li> <li>NewSharedIndexInformer requires <code>lw ListerWatcher, exampleObject runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers</code>. Those functions use <code>k8s.io/client-go/metadata</code>, <code>k8s.io/client-go/dynamic</code>, and <code>k8s.io/client-go/rest</code> to get <code>ListFunc</code> and <code>WatchFunc</code> that are necessary to generate [cache.ListWatch]<ol> <li>In controller-runtime, <code>ListerWatcher</code> is created by <code>createListWatcher</code> which is passed to <code>newSpecificInformersMap</code>. Specifically, createStructuredListWatch, createUnstructuredListWatch, createMetadataListWatch</li> <li>In controller-runtime, <code>Indexers</code> is generated as follows:     <pre><code>cache.Indexers{\n    cache.NamespaceIndex: cache.MetaNamespaceIndexFunc,\n}\n</code></pre></li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/cache/#types","title":"Types","text":""},{"location":"kubernetes-operator/controller-runtime/cache/#cache-interface","title":"Cache interface","text":"<pre><code>// Cache knows how to load Kubernetes objects, fetch informers to request\n// to receive events for Kubernetes objects (at a low-level),\n// and add indices to fields on the objects stored in the cache.\ntype Cache interface {\n    // Cache acts as a client to objects stored in the cache.\n    client.Reader\n\n    // Cache loads informers and adds field indices.\n    Informers\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/cache/#informers-interface","title":"Informers interface","text":"<pre><code>// Informers knows how to create or fetch informers for different\n// group-version-kinds, and add indices to those informers.  It's safe to call\n// GetInformer from multiple threads.\ntype Informers interface {\n    // GetInformer fetches or constructs an informer for the given object that corresponds to a single\n    // API kind and resource.\n    GetInformer(ctx context.Context, obj client.Object) (Informer, error)\n\n    // GetInformerForKind is similar to GetInformer, except that it takes a group-version-kind, instead\n    // of the underlying object.\n    GetInformerForKind(ctx context.Context, gvk schema.GroupVersionKind) (Informer, error)\n\n    // Start runs all the informers known to this cache until the context is closed.\n    // It blocks.\n    Start(ctx context.Context) error\n\n    // WaitForCacheSync waits for all the caches to sync.  Returns false if it could not sync a cache.\n    WaitForCacheSync(ctx context.Context) bool\n\n    // Informers knows how to add indices to the caches (informers) that it manages.\n    client.FieldIndexer\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/cache/#informer-interface","title":"Informer interface","text":"<pre><code>// Informer - informer allows you interact with the underlying informer.\ntype Informer interface {\n    // AddEventHandler adds an event handler to the shared informer using the shared informer's resync\n    // period.  Events to a single handler are delivered sequentially, but there is no coordination\n    // between different handlers.\n    AddEventHandler(handler toolscache.ResourceEventHandler)\n    // AddEventHandlerWithResyncPeriod adds an event handler to the shared informer using the\n    // specified resync period.  Events to a single handler are delivered sequentially, but there is\n    // no coordination between different handlers.\n    AddEventHandlerWithResyncPeriod(handler toolscache.ResourceEventHandler, resyncPeriod time.Duration)\n    // AddIndexers adds more indexers to this store.  If you call this after you already have data\n    // in the store, the results are undefined.\n    AddIndexers(indexers toolscache.Indexers) error\n    // HasSynced return true if the informers underlying store has synced.\n    HasSynced() bool\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/cache/#informercache","title":"informerCache","text":""},{"location":"kubernetes-operator/controller-runtime/cache/#new","title":"New","text":"<ol> <li>Cache.New initializes and returns informerCache.     <pre><code>im := internal.NewInformersMap(config, opts.Scheme, opts.Mapper, *opts.Resync, opts.Namespace, selectorsByGVK, disableDeepCopyByGVK, transformByGVK)\nreturn &amp;informerCache{InformersMap: im}, nil\n</code></pre><ol> <li><code>disableDeepCopyByGVK</code> is determined from <code>opts</code>:     <pre><code>disableDeepCopyByGVK, err := convertToDisableDeepCopyByGVK(opts.UnsafeDisableDeepCopyByObject, opts.Scheme)\n</code></pre></li> <li>The returned value is the following type:     <pre><code>type DisableDeepCopyByGVK map[schema.GroupVersionKind]bool\n</code></pre></li> <li>Default value is empty: <code>internal.DisableDeepCopyByGVK{}</code></li> </ol> </li> <li>informerCache <pre><code>type informerCache struct {\n    *internal.InformersMap\n}\n</code></pre></li> <li> <p>InformersMap <pre><code>type InformersMap struct {\n    // we abstract over the details of structured/unstructured/metadata with the specificInformerMaps\n    // TODO(directxman12): genericize this over different projections now that we have 3 different maps\n\n    structured   *specificInformersMap\n    unstructured *specificInformersMap\n    metadata     *specificInformersMap\n\n    // Scheme maps runtime.Objects to GroupVersionKinds\n    Scheme *runtime.Scheme\n}\n</code></pre></p> <p>Initialized with NewInformersMap: <pre><code>&amp;InformersMap{\n    structured:   newStructuredInformersMap(config, scheme, mapper, resync, namespace, selectors, disableDeepCopy, transformers),\n    unstructured: newUnstructuredInformersMap(config, scheme, mapper, resync, namespace, selectors, disableDeepCopy, transformers),\n    metadata:     newMetadataInformersMap(config, scheme, mapper, resync, namespace, selectors, disableDeepCopy, transformers),\n\n    Scheme: scheme,\n}\n</code></pre> 1. All newXXXInformersMap calls specificInformersMap: <pre><code>// newStructuredInformersMap creates a new InformersMap for structured objects.\nfunc newStructuredInformersMap(config *rest.Config, scheme *runtime.Scheme, mapper meta.RESTMapper, resync time.Duration,\n    namespace string, selectors SelectorsByGVK, disableDeepCopy DisableDeepCopyByGVK, transformers TransformFuncByObject) *specificInformersMap {\n    return newSpecificInformersMap(config, scheme, mapper, resync, namespace, selectors, disableDeepCopy, transformers, createStructuredListWatch)\n}\n\n// newUnstructuredInformersMap creates a new InformersMap for unstructured objects.\nfunc newUnstructuredInformersMap(config *rest.Config, scheme *runtime.Scheme, mapper meta.RESTMapper, resync time.Duration,\n    namespace string, selectors SelectorsByGVK, disableDeepCopy DisableDeepCopyByGVK, transformers TransformFuncByObject) *specificInformersMap {\n    return newSpecificInformersMap(config, scheme, mapper, resync, namespace, selectors, disableDeepCopy, transformers, createUnstructuredListWatch)\n}\n\n// newMetadataInformersMap creates a new InformersMap for metadata-only objects.\nfunc newMetadataInformersMap(config *rest.Config, scheme *runtime.Scheme, mapper meta.RESTMapper, resync time.Duration,\n    namespace string, selectors SelectorsByGVK, disableDeepCopy DisableDeepCopyByGVK, transformers TransformFuncByObject) *specificInformersMap {\n    return newSpecificInformersMap(config, scheme, mapper, resync, namespace, selectors, disableDeepCopy, transformers, createMetadataListWatch)\n}\n</code></pre> 1. specificInformersMap Initialized with newSpecificInformersMap <pre><code>ip := &amp;specificInformersMap{\n    config:            config,\n    Scheme:            scheme,\n    mapper:            mapper,\n    informersByGVK:    make(map[schema.GroupVersionKind]*MapEntry),\n    codecs:            serializer.NewCodecFactory(scheme),\n    paramCodec:        runtime.NewParameterCodec(scheme),\n    resync:            resync,\n    startWait:         make(chan struct{}),\n    createListWatcher: createListWatcher,\n    namespace:         namespace,\n    selectors:         selectors.forGVK,\n    disableDeepCopy:   disableDeepCopy,\n    transformers:      transformers,\n}\n</code></pre></p> </li> <li> <p>MapEntry</p> <p><pre><code>// MapEntry contains the cached data for an Informer.\ntype MapEntry struct {\n    // Informer is the cached informer\n    Informer cache.SharedIndexInformer\n\n    // CacheReader wraps Informer and implements the CacheReader interface for a single type\n    Reader CacheReader\n}\n</code></pre> 1. CacheReader <pre><code>// CacheReader wraps a cache.Index to implement the client.CacheReader interface for a single type.\ntype CacheReader struct {\n    // indexer is the underlying indexer wrapped by this cache.\n    indexer cache.Indexer\n\n    // groupVersionKind is the group-version-kind of the resource.\n    groupVersionKind schema.GroupVersionKind\n\n    // scopeName is the scope of the resource (namespaced or cluster-scoped).\n    scopeName apimeta.RESTScopeName\n\n    // disableDeepCopy indicates not to deep copy objects during get or list objects.\n    // Be very careful with this, when enabled you must DeepCopy any object before mutating it,\n    // otherwise you will mutate the object in the cache.\n    disableDeepCopy bool\n}\n</code></pre> 1. <code>MapEntry</code> and <code>CacheReader</code> is initialized in addInformerToMap:</p> <pre><code>i := &amp;MapEntry{\n    Informer: ni,\n    Reader: CacheReader{\n        indexer:          ni.GetIndexer(),\n        groupVersionKind: gvk,\n        scopeName:        rm.Scope.Name(),\n        disableDeepCopy:  ip.disableDeepCopy.IsDisabled(gvk),\n    },\n}\n</code></pre> <p><code>disableDeepCopy</code> is determined by isDisabled() originally given from <code>opts.UnsafeDisableDeepCopyByObject</code>.</p> <p><pre><code>// IsDisabled returns whether a GroupVersionKind is disabled DeepCopy.\nfunc (disableByGVK DisableDeepCopyByGVK) IsDisabled(gvk schema.GroupVersionKind) bool {\n    if d, ok := disableByGVK[gvk]; ok {\n        return d\n    } else if d, ok = disableByGVK[GroupVersionKindAll]; ok {\n        return d\n    }\n    return false\n}\n</code></pre> As you can see, if it's empty (the default value), it returns <code>false</code>, which means when cacheReader gets an obejct from the cache, it internally deepcopies the object. You can modified the got object without deepcopying by yourself.</p> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/cache/#how-cache-is-used","title":"How cache is used","text":"<ol> <li>Create cache.</li> <li>Start cache.</li> <li>Wait until cache is synced.</li> <li>Get informer.</li> <li>Wait until informer is synced.</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/cache/#example-get-nginx-pod","title":"Example: Get nginx Pod","text":"<p>Use ResourceEventHandlerFuncs (Note that this is in client-go package! not in controller-runtime!)</p> <p>The informer interface is confusing in that the arguments of the informer's methods are using another cache package in client-go.</p> <p>toolcache here is https://pkg.go.dev/k8s.io/client-go@v0.25.0/tools/cache</p> <pre><code>type Informer interface {\n    // AddEventHandler adds an event handler to the shared informer using the shared informer's resync\n    // period.  Events to a single handler are delivered sequentially, but there is no coordination\n    // between different handlers.\n    AddEventHandler(handler toolscache.ResourceEventHandler)\n    // AddEventHandlerWithResyncPeriod adds an event handler to the shared informer using the\n    // specified resync period.  Events to a single handler are delivered sequentially, but there is\n    // no coordination between different handlers.\n    AddEventHandlerWithResyncPeriod(handler toolscache.ResourceEventHandler, resyncPeriod time.Duration)\n    // AddIndexers adds more indexers to this store.  If you call this after you already have data\n    // in the store, the results are undefined.\n    AddIndexers(indexers toolscache.Indexers) error\n    // HasSynced return true if the informers underlying store has synced.\n    HasSynced() bool\n}\n</code></pre> <p>ResourceEventHandler:</p> <pre><code>type ResourceEventHandler interface {\n    OnAdd(obj interface{})\n    OnUpdate(oldObj, newObj interface{})\n    OnDelete(obj interface{})\n}\n</code></pre> <ol> <li>Run nginx pod     <pre><code>kubectl run nginx --image=nginx\n</code></pre></li> <li> <p>Run     <pre><code>go run main.go\n</code></pre></p> <p><pre><code>cache is started\ncache is synced\n&amp;Pod{ObjectMeta:{nginx  default  36f44710-c965-4b02-8fb5-05b0948104fa 190616 0 2022-09-15 07:14:46 +0900 JST &lt;nil&gt; &lt;nil&gt; map[run:nginx] map[] [] [] [{kubectl-run Update v1 2022-09-15 07:14:46 +0900 JST FieldsV1 {\"f:metadata\":{\"f:labels\":{\".\":{},\"f:run\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"nginx\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:enableServiceLinks\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}} } {kubelet Update v1 2022-09-15 07:14:46 +0900 JST FieldsV1 {\"f:status\":{\"f:conditions\":{\"k:{\\\"type\\\":\\\"ContainersReady\\\"}\":{\".\":{},\"f:lastProbeTime\":{},\"f:lastTransitionTime\":{},\"f:message\":{},\"f:reason\":{},\"f:status\":{},\"f:type\":{}},\"k:{\\\"type\\\":\\\"Initialized\\\"}\":{\".\":{},\"f:lastProbeTime\":{},\"f:lastTransitionTime\":{},\"f:status\":{},\"f:type\":{}},\"k:{\\\"type\\\":\\\"Ready\\\"}\":{\".\":{},\"f:lastProbeTime\":{},\"f:lastTransitionTime\":{},\"f:message\":{},\"f:reason\":{},\"f:status\":{},\"f:type\":{}}},\"f:containerStatuses\":{},\"f:hostIP\":{},\"f:startTime\":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8flt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&amp;ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&amp;ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&amp;ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&amp;DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&amp;ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:nginx,Image:nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8flt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-control-plane,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&amp;PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-15 07:14:46 +0900 JST,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-15 07:14:46 +0900 JST,Reason:ContainersNotReady,Message:containers with unready status: [nginx],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-15 07:14:46 +0900 JST,Reason:ContainersNotReady,Message:containers with unready status: [nginx],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-15 07:14:46 +0900 JST,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.2,PodIP:,StartTime:2022-09-15 07:14:46 +0900 JST,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:nginx,State:ContainerState{Waiting:&amp;ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:nginx,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}\n</code></pre> 1. Create/Delete nginx pod. You'll see the object in the logs.</p> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/client/","title":"client","text":""},{"location":"kubernetes-operator/controller-runtime/client/#client-interface","title":"Client interface","text":"<pre><code>// Client knows how to perform CRUD operations on Kubernetes objects.\ntype Client interface {\n    Reader\n    Writer\n    StatusClient\n\n    // Scheme returns the scheme this client is using.\n    Scheme() *runtime.Scheme\n    // RESTMapper returns the rest this client is using.\n    RESTMapper() meta.RESTMapper\n}\n</code></pre> <pre><code>// Reader knows how to read and list Kubernetes objects.\ntype Reader interface {\n    Get(ctx context.Context, key ObjectKey, obj Object) error\n    List(ctx context.Context, list ObjectList, opts ...ListOption) error\n}\n</code></pre> <pre><code>// Writer knows how to create, delete, and update Kubernetes objects.\ntype Writer interface {\n    Create(ctx context.Context, obj Object, opts ...CreateOption) error\n    Delete(ctx context.Context, obj Object, opts ...DeleteOption) error\n    Update(ctx context.Context, obj Object, opts ...UpdateOption) error\n    Patch(ctx context.Context, obj Object, patch Patch, opts ...PatchOption) error\n    DeleteAllOf(ctx context.Context, obj Object, opts ...DeleteAllOfOption) error\n}\n</code></pre> <pre><code>// StatusClient knows how to create a client which can update status subresource\n// for kubernetes objects.\ntype StatusClient interface {\n    Status() StatusWriter\n}\n\n// StatusWriter knows how to update status subresource of a Kubernetes object.\ntype StatusWriter interface {\n    Update(ctx context.Context, obj Object, opts ...UpdateOption) error\n    Patch(ctx context.Context, obj Object, patch Patch, opts ...PatchOption) error\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/client/#delegatingclient","title":"delegatingClient","text":"<pre><code>type delegatingClient struct {\n    Reader\n    Writer\n    StatusClient\n\n    scheme *runtime.Scheme\n    mapper meta.RESTMapper\n}\n</code></pre> <p>There's a function called shouldBypassCache to check if the target object is cached or not. If cached, call cacheReader, otherwise call clientReader</p>"},{"location":"kubernetes-operator/controller-runtime/client/#how-client-is-used","title":"How <code>client</code> is used","text":"<ol> <li>When a Manager is created, a Cluster is created internally. (You can check more details in cluster)</li> <li> <p>When creating a cluster, client is also created. If <code>Options.NewClient</code> is not specified DefaultNewClient is used, which calls NewDelegatingClient to return a client.</p> <pre><code>if options.NewClient == nil {\n    options.NewClient = DefaultNewClient\n}\n</code></pre> <pre><code>writeObj, err := options.NewClient(cache, config, clientOptions, options.ClientDisableCacheFor...)\n</code></pre> </li> <li> <p>In DefaultNewClient, new client is created first, and then delegatingClient is created.     <pre><code>c, err := client.New(config, options)\n</code></pre></p> <pre><code>client.NewDelegatingClient(client.NewDelegatingClientInput{\n    CacheReader:     cache,\n    Client:          c,\n    UncachedObjects: uncachedObjects,\n})\n</code></pre> <p>As you can see, there's a struct for the input: <pre><code>// NewDelegatingClientInput encapsulates the input parameters to create a new delegating client.\ntype NewDelegatingClientInput struct {\n    CacheReader       Reader\n    Client            Client\n    UncachedObjects   []Object\n    CacheUnstructured bool\n}\n</code></pre></p> </li> <li> <p><code>delegatingClient</code> is initialized in NewDelegatingClient</p> <p>Three roles: 1. <code>Reader</code>: client + cache &lt;- utilize the cache to reduce API requests (<code>Get</code> and <code>List</code>) 1. <code>Writer</code>: client (<code>Create</code>, <code>Update</code>, <code>Delete</code>, etc) 1. <code>StatusClient</code>: client (<code>Status().Update()</code> or <code>Status().Patch()</code>)</p> <pre><code>&amp;delegatingClient{\n    scheme: in.Client.Scheme(),\n    mapper: in.Client.RESTMapper(),\n    Reader: &amp;delegatingReader{\n        CacheReader:       in.CacheReader,\n        ClientReader:      in.Client,\n        scheme:            in.Client.Scheme(),\n        uncachedGVKs:      uncachedGVKs,\n        cacheUnstructured: in.CacheUnstructured,\n    },\n    Writer:       in.Client,\n    StatusClient: in.Client,\n}\n</code></pre> <p>cacheReader:</p> <pre><code>// CacheReader wraps a cache.Index to implement the client.CacheReader interface for a single type.\ntype CacheReader struct {\n    indexer cache.Indexer\n    groupVersionKind schema.GroupVersionKind\n    scopeName apimeta.RESTScopeName\n    disableDeepCopy bool\n}\n</code></pre> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/client/#new","title":"New","text":"<pre><code>func newClient(config *rest.Config, options Options) (*client, error) {\n</code></pre> <pre><code>c := &amp;client{\n    typedClient: typedClient{\n        cache:      clientcache,\n        paramCodec: runtime.NewParameterCodec(options.Scheme),\n    },\n    unstructuredClient: unstructuredClient{\n        cache:      clientcache,\n        paramCodec: noConversionParamCodec{},\n    },\n    metadataClient: metadataClient{\n        client:     rawMetaClient,\n        restMapper: options.Mapper,\n    },\n    scheme: options.Scheme,\n    mapper: options.Mapper,\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/client/#tips","title":"Tips","text":"<ol> <li>https://zoetrope.github.io/kubebuilder-training/controller-runtime/client.html: When to use <code>Patch</code>? <code>MergeFrom</code> vs. <code>StrategicMergeFrom</code></li> </ol>"},{"location":"kubernetes-operator/controller-runtime/cluster/","title":"cluster","text":"<p>Cluster provides various methods to interact with a cluster. Cluster is initialized and stored in Manager with cluster.New.</p> <p>Most of the fields in a cluster (scheme, cache, client, apiReader, recorderProvider, etc.) are used to injected to related components (Controller, EventHandlers, Sources, Predicates)</p>"},{"location":"kubernetes-operator/controller-runtime/cluster/#types","title":"Types","text":""},{"location":"kubernetes-operator/controller-runtime/cluster/#1-cluster-interface","title":"1. Cluster interface","text":"<pre><code>type Cluster interface {\n    SetFields(interface{}) error\n    GetConfig() *rest.Config\n    GetScheme() *runtime.Scheme\n    GetClient() client.Client\n    GetFieldIndexer() client.FieldIndexer\n    GetCache() cache.Cache\n    GetEventRecorderFor(name string) record.EventRecorder\n    GetRESTMapper() meta.RESTMapper\n    GetAPIReader() client.Reader\n    Start(ctx context.Context) error\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/cluster/#2-cluster-struct","title":"2. cluster struct","text":"<pre><code>type cluster struct {\n    config *rest.Config\n    scheme *runtime.Scheme // scheme is injected into Controllers, EventHandlers, Sources and Predicates.\n    cache cache.Cache // injected is injected into Sources\n    client client.Client // client is injected into Controllers (and EventHandlers, Sources and Predicates).\n    apiReader client.Reader // apiReader is the reader that will make requests to the api server and not the cache.\n    fieldIndexes client.FieldIndexer\n    recorderProvider *intrec.Provider // recorderProvider is used to generate event recorders that will be injected into Controllers (and EventHandlers, Sources and Predicates).\n    mapper meta.RESTMapper // mapper is used to map resources to kind, and map kind and version.\n    logger logr.Logger\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/cluster/#new","title":"New","text":"<ol> <li> <p>SetOptionDefaults     For more details, check below</p> </li> <li> <p>Create a <code>mapper</code> <pre><code>mapper, err := options.MapperProvider(config)\n</code></pre></p> </li> <li> <p>Create a <code>cache</code> with <code>NewCache</code> (cache.New)     <pre><code>cache, err := options.NewCache(config, cache.Options{Scheme: options.Scheme, Mapper: mapper, Resync: options.SyncPeriod, Namespace: options.Namespace})\n</code></pre></p> <p>For more details, read cache</p> </li> <li> <p>Create <code>apiReader</code> <pre><code>apiReader, err := client.New(config, clientOptions)\n</code></pre></p> </li> <li> <p>Create a <code>writeObj</code> with <code>NewClient</code> (DefaultNewClient -&gt; NewDelegatingClient)     <pre><code>writeObj, err := options.NewClient(cache, config, clientOptions, options.ClientDisableCacheFor...)\n</code></pre></p> <pre><code>if options.NewClient == nil {\n    options.NewClient = DefaultNewClient\n}\n</code></pre> <pre><code>// DefaultNewClient creates the default caching client.\nfunc DefaultNewClient(cache cache.Cache, config *rest.Config, options client.Options, uncachedObjects ...client.Object) (client.Client, error) {\n    c, err := client.New(config, options)\n    if err != nil {\n        return nil, err\n    }\n\n    return client.NewDelegatingClient(client.NewDelegatingClientInput{\n        CacheReader:     cache,\n        Client:          c,\n        UncachedObjects: uncachedObjects,\n    })\n}\n</code></pre> <pre><code>&amp;delegatingClient{\n    scheme: in.Client.Scheme(),\n    mapper: in.Client.RESTMapper(),\n    Reader: &amp;delegatingReader{\n        CacheReader:       in.CacheReader,\n        ClientReader:      in.Client,\n        scheme:            in.Client.Scheme(),\n        uncachedGVKs:      uncachedGVKs,\n        cacheUnstructured: in.CacheUnstructured,\n    },\n    Writer:       in.Client,\n    StatusClient: in.Client,\n}\n</code></pre> </li> <li> <p>Create a <code>recorderProvider</code> <pre><code>recorderProvider, err := options.newRecorderProvider(config, options.Scheme, options.Logger.WithName(\"events\"), options.makeBroadcaster)\n</code></pre></p> </li> <li>Create cluster     <pre><code>&amp;cluster{\n    config:           config,\n    scheme:           options.Scheme,\n    cache:            cache,\n    fieldIndexes:     cache,\n    client:           writeObj,\n    apiReader:        apiReader,\n    recorderProvider: recorderProvider,\n    mapper:           mapper,\n    logger:           options.Logger,\n}\n</code></pre></li> </ol>"},{"location":"kubernetes-operator/controller-runtime/cluster/#setoptiondefaults","title":"SetOptionDefaults","text":"name value where to use Scheme scheme.Scheme MapperProvider <code>func(c *rest.Config) (meta.RESTMapper, error) {return apiutil.NewDynamicRESTMapper(c, nil)}</code> NewClient DefaultNewClient NewCache cache.New newRecorderProvider intrec.NewProvider makeBroadcaster <code>func() (record.EventBroadcaster, bool) {return record.NewBroadcaster(), true}</code> Logger logf.RuntimeLog.WithName(\"cluster\") <ol> <li><code>options.Scheme = scheme.Scheme</code>(Use the Kubernetes client-go scheme if none is specified)</li> <li>MapperProvider     <pre><code>options.MapperProvider = func(c *rest.Config) (meta.RESTMapper, error) {\n    return apiutil.NewDynamicRESTMapper(c, nil)\n}\n</code></pre></li> <li> <p><code>options.NewClient = DefaultNewClient</code> <pre><code>func DefaultNewClient(cache cache.Cache, config *rest.Config, options client.Options, uncachedObjects ...client.Object) (client.Client, error) {\n    c, err := client.New(config, options)\n    if err != nil {\n        return nil, err\n    }\n\n    return client.NewDelegatingClient(client.NewDelegatingClientInput{\n        CacheReader:     cache,\n        Client:          c,\n        UncachedObjects: uncachedObjects,\n    })\n}\n</code></pre></p> <p><code>GetClient()</code> returns <code>cluster.client</code>, a delegatingClient by default. For more details about <code>delegatingClient</code> you can check client</p> </li> <li> <p><code>options.NewCache = cache.New</code></p> </li> <li><code>options.newRecorderProvider = intrec.NewProvider</code></li> <li><code>record.NewBroadcaster()</code></li> <li><code>options.Logger = logf.RuntimeLog.WithName(\"cluster\")</code></li> </ol>"},{"location":"kubernetes-operator/controller-runtime/cluster/#setfields","title":"SetFields","text":"<pre><code>func (c *cluster) SetFields(i interface{}) error {\n    if _, err := inject.ConfigInto(c.config, i); err != nil {\n        return err\n    }\n    if _, err := inject.ClientInto(c.client, i); err != nil {\n        return err\n    }\n    if _, err := inject.APIReaderInto(c.apiReader, i); err != nil {\n        return err\n    }\n    if _, err := inject.SchemeInto(c.scheme, i); err != nil {\n        return err\n    }\n    if _, err := inject.CacheInto(c.cache, i); err != nil {\n        return err\n    }\n    if _, err := inject.MapperInto(c.mapper, i); err != nil {\n        return err\n    }\n    return nil\n}\n</code></pre> <ol> <li><code>cluster.SetFields</code> is called in manager.SetFields</li> <li><code>cluster.SetFields</code> injects <code>Config</code>, <code>Client</code>, <code>APIReader</code>, <code>Scheme</code>, <code>Cache</code> and <code>Mapper</code> into the specified <code>i</code>.</li> <li> <p>manager.SetFields's usage:</p> <ol> <li> <p>used for reconciler passed via builder in controller <pre><code>// Inject dependencies into Reconciler\nif err := mgr.SetFields(options.Reconciler); err != nil {\n    return nil, err\n}\n</code></pre></p> </li> <li> <p>used for runnables added to the Manager with add function <pre><code>// Add sets dependencies on i, and adds it to the list of Runnables to start.\nfunc (cm *controllerManager) Add(r Runnable) error {\n    cm.Lock()\n    defer cm.Unlock()\n    return cm.add(r)\n}\n\nfunc (cm *controllerManager) add(r Runnable) error {\n    // Set dependencies on the object\n    if err := cm.SetFields(r); err != nil {\n        return err\n    }\n    return cm.runnables.Add(r)\n}\n</code></pre></p> <ol> <li>Controller is passed in controller.New</li> </ol> </li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/controller/","title":"Controller","text":"<p>Controllers (pkg/controller) use events (pkg/event) to eventually trigger reconcile requests. They may be constructed manually, but are often constructed with a Builder (pkg/builder), which eases the wiring of event sources (pkg/source), like Kubernetes API object changes, to event handlers (pkg/handler), like \"enqueue a reconcile request for the object owner\". Predicates (pkg/predicate) can be used to filter which events actually trigger reconciles. There are pre-written utilities for the common cases, and interfaces and helpers for advanced cases.</p>"},{"location":"kubernetes-operator/controller-runtime/controller/#controller-interface","title":"Controller interface","text":"<p>Controller embeds reconcile.Reconciler.</p> <pre><code>type Controller interface {\n    // Reconciler is called to reconcile an object by Namespace/Name\n    reconcile.Reconciler\n\n    // Watch takes events provided by a Source and uses the EventHandler to\n    // enqueue reconcile.Requests in response to the events.\n    //\n    // Watch may be provided one or more Predicates to filter events before\n    // they are given to the EventHandler.  Events will be passed to the\n    // EventHandler if all provided Predicates evaluate to true.\n    Watch(src source.Source, eventhandler handler.EventHandler, predicates ...predicate.Predicate) error\n\n    // Start starts the controller.  Start blocks until the context is closed or a\n    // controller has an error starting.\n    Start(ctx context.Context) error\n\n    // GetLogger returns this controller logger prefilled with basic information.\n    GetLogger() logr.Logger\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/controller/#controller-type","title":"Controller type","text":"<pre><code>type Controller struct {\n    Name string\n    MaxConcurrentReconciles int\n    Do reconcile.Reconciler\n    MakeQueue func() workqueue.RateLimitingInterface\n    Queue workqueue.RateLimitingInterface\n    SetFields func(i interface{}) error\n    mu sync.Mutex\n    Started bool\n    ctx context.Context\n    CacheSyncTimeout time.Duration\n    startWatches []watchDescription\n    LogConstructor func(request *reconcile.Request) logr.Logger\n    RecoverPanic bool\n}\n</code></pre> <ol> <li><code>Do</code>: reconcile.Reconciler</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/controller/#how-controller-is-used","title":"How Controller is used","text":"<p>Interestingly, <code>New</code> requires Manager and when a controller is created, the controller is added to the manager. A Controller must be run by a Manager. A controller is added to controllerManager via builder, more specifically <code>NewControllerManagedBy</code>. For more details, you can also check manager.</p> <pre><code>// New returns a new Controller registered with the Manager.  The Manager will ensure that shared Caches have\n// been synced before the Controller is Started.\nfunc New(name string, mgr manager.Manager, options Options) (Controller, error) {\n    c, err := NewUnmanaged(name, mgr, options)\n    if err != nil {\n        return nil, err\n    }\n\n    // Add the controller as a Manager components\n    return c, mgr.Add(c)\n}\n</code></pre> <ol> <li><code>Manager</code> and <code>Reconciler</code> need to be prepared.</li> <li>Call <code>NewControllerManagedBy(mgr)</code> and <code>Complete(r)</code> with the manager and reconciler.</li> <li><code>Builer.build</code> calls <code>bldr.doController</code> to create a controller with Controller.New.<ol> <li>Inject dependencies to reconciler with <code>Manager.SetFields(reconciler)</code> (ref)</li> <li>Inititialize Controller     <pre><code>&amp;controller.Controller{\n    Do: options.Reconciler,\n    MakeQueue: func() workqueue.RateLimitingInterface {\n        return workqueue.NewNamedRateLimitingQueue(options.RateLimiter, name)\n    },\n    MaxConcurrentReconciles: options.MaxConcurrentReconciles,\n    CacheSyncTimeout:        options.CacheSyncTimeout,\n    SetFields:               mgr.SetFields,\n    Name:                    name,\n    LogConstructor:          options.LogConstructor,\n    RecoverPanic:            options.RecoverPanic,\n}\n</code></pre><ol> <li><code>Manager.SetFields</code> is passed to <code>ctrl.SetFields</code></li> <li><code>workqueue.NewNamedRateLimitingQueue</code> is set to <code>ctrl.MakeQueue</code></li> <li><code>Reconciler</code> is set to <code>ctrl.Do</code></li> </ol> </li> <li>Register the controller to manager with <code>mgr.Add(controller)</code> (ref)</li> <li>The created controller is set to <code>bldr.ctrl</code> (ref)</li> </ol> </li> <li><code>Builder.build</code> also calls <code>bldr.doWatch</code><ol> <li><code>Kind</code> is created for <code>For</code> and <code>Owns</code>.</li> <li><code>EventHandler</code> is created.</li> <li>Call <code>bldr.ctrl.Watch(src, hdlr, allPredicates...)</code></li> </ol> </li> <li><code>controller.Watch</code><ol> <li>Inject the cache to the <code>src</code> with <code>SetFields</code> (given by the Manager).</li> <li>Inject (sth) to the <code>eventHandler</code> and <code>Predicates</code> with <code>SetFields</code> (not checked what's injected).</li> <li>If the controller hasn't started yet, store the watches locally (<code>startWatches</code>) and return.</li> <li>If the controller has started, calls <code>src.Start</code></li> </ol> </li> <li><code>controller.Start</code> is called by the <code>Manager</code> when <code>Manager.Start</code> is called.<ol> <li>Controller can be started only once.</li> <li>Create a <code>Queue</code> with <code>MakeQueue</code> func which is specified in <code>New</code>.</li> <li>For the stored watches (<code>startWatches</code>), call <code>watch.src.Start</code> to start src to monitor API server and enqueue modified objects.</li> <li>Convert <code>Kind</code> to <code>syncingSource</code> and call <code>syncingSource.WaitForSync</code>. This waits until the cache is synced in <code>src.Start</code> by checking ks.started channel.</li> <li>Clean up the <code>startWatches</code> after the caches of all the watches are in-sync.</li> <li>Call <code>processNextWorkItem</code> with <code>MaxConcurrentReconciles</code> go routines.</li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/controller/#watch-func","title":"<code>Watch</code> func","text":"<ol> <li>Where is <code>Watch</code> called?<ol> <li><code>Watch</code> is called in bldr.doWatch in builder for <code>For</code>, <code>Owns</code>, and <code>Watches</code> configured with a controller builder.</li> </ol> </li> <li><code>Watch</code> calls <code>SetFields</code> for <code>Source</code>, <code>EventHandler</code>, and <code>Predicate</code>s.     <pre><code>if err := c.SetFields(src); err != nil {\n    return err\n}\nif err := c.SetFields(evthdler); err != nil {\n    return err\n}\nfor _, pr := range prct {\n    if err := c.SetFields(pr); err != nil {\n        return err\n    }\n}\n</code></pre><ol> <li><code>SetFields</code> is one of the Controller's field <code>SetFields func(i interface{}) error</code>, which is set when initializing in NewUnmanaged from <code>mgr.SetFields</code>.</li> <li>For more details, you can check inject and manager</li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/controller/#start-func","title":"<code>Start</code> func","text":"<ol> <li>Calls <code>processNextWorkItem</code> until it returns <code>false</code> here.</li> <li>Where is <code>Start</code> called? -&gt; Called from manager.</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/example-controller/","title":"Example Controller","text":"<p>First example in controller-runtime</p>"},{"location":"kubernetes-operator/controller-runtime/example-controller/#example","title":"Example","text":"<ol> <li> <p>Run example controller.</p> <pre><code>go run example-controller.go\n</code></pre> <p>What this controller does:</p> <ol> <li>Read the ReplicaSet</li> <li>Read the Pods</li> <li>Set a Label on the ReplicaSet with the Pod count.</li> </ol> </li> <li> <p>Create Deployment</p> <pre><code>kubectl create deploy test --replicas=3 --image=nginx\n</code></pre> <p>You would see some errors, these errors happen because replicaset is updated by replicaset controller according to new Pod's status.</p> <pre><code>1.6565476218562e+09     ERROR   controller.replicaset   Reconciler error        {\"reconciler group\": \"apps\", \"reconciler kind\": \"ReplicaSet\", \"name\": \"test-8499f4f74\", \"namespace\": \"default\", \"error\": \"Operation cannot be fulfilled on replicasets.apps \\\"test-8499f4f74\\\": the object has been modified; please apply your changes to the latest version and try again\"}\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n        /Users/nakamasato/.gvm/pkgsets/go1.17.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.11.2/pkg/internal/controller/controller.go:266\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\n        /Users/nakamasato/.gvm/pkgsets/go1.17.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.11.2/pkg/internal/controller/controller.go:227\n</code></pre> </li> <li> <p>Check <code>pod-count=3</code> labels added to the ReplicaSet</p> <pre><code>kubectl get rs -o jsonpath='{.items[].metadata.labels}'\n{\"app\":\"test\",\"pod-count\":\"3\",\"pod-template-hash\":\"8499f4f74\"}\n</code></pre> </li> <li> <p>Clean up     <pre><code>kubectl delete deploy test\n</code></pre></p> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/handler/","title":"handler","text":"<p>Package <code>handler</code> defines <code>EventHandlers</code> that enqueue <code>reconcile.Request</code>s in response to Create, Update, Deletion Events observed from Watching Kubernetes APIs. Users should provide a <code>source.Source</code> and <code>handler.EventHandler</code> to <code>Controller.Watch</code> in order to generate and enqueue <code>reconcile.Request</code> work items.</p> <p><code>handler.EventHandler</code> is an argument to <code>Controller.Watch</code> that enqueues <code>reconcile.Request</code>s in response to events.</p> <ol> <li>Unless you are implementing your own EventHandler, you can ignore the functions on the <code>EventHandler</code> interface.</li> <li>Most users shouldn't need to implement their own EventHandler.</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/handler/#eventhandler-interface","title":"EventHandler interface","text":"<pre><code>// * Use EnqueueRequestsFromMapFunc to transform an event for an object to a reconcile of an object\n// of a different type - do this for events for types the Controller may be interested in, but doesn't create.\n// (e.g. If Foo responds to cluster size events, map Node events to Foo objects.)\n//\n// Unless you are implementing your own EventHandler, you can ignore the functions on the EventHandler interface.\n// Most users shouldn't need to implement their own EventHandler.\ntype EventHandler interface {\n    // Create is called in response to an create event - e.g. Pod Creation.\n    Create(event.CreateEvent, workqueue.RateLimitingInterface)\n\n    // Update is called in response to an update event -  e.g. Pod Updated.\n    Update(event.UpdateEvent, workqueue.RateLimitingInterface)\n\n    // Delete is called in response to a delete event - e.g. Pod Deleted.\n    Delete(event.DeleteEvent, workqueue.RateLimitingInterface)\n\n    // Generic is called in response to an event of an unknown type or a synthetic event triggered as a cron or\n    // external trigger request - e.g. reconcile Autoscaling, or a Webhook.\n    Generic(event.GenericEvent, workqueue.RateLimitingInterface)\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/handler/#enqueuerequestforobject","title":"EnqueueRequestForObject","text":"<p>This is used by default in builder.doWatch. If you create an operator with kubebuilder, you're using this eventhandler. This function converts events received from the Source into <code>reconcile.Request</code>s object and enqueue them to the given queue.</p> <ol> <li><code>Create</code>, <code>Delete</code>, <code>Generic</code>:     <pre><code>q.Add(reconcile.Request{NamespacedName: types.NamespacedName{\n    Name:      evt.Object.GetName(),\n    Namespace: evt.Object.GetNamespace(),\n}})\n</code></pre></li> <li><code>Update</code>: Enqueue ObjectNew (ObjectOld if ObjectNew doesn't exist)     <pre><code>q.Add(reconcile.Request{NamespacedName: types.NamespacedName{\n    Name:      evt.ObjectNew.GetName(),\n    Namespace: evt.ObjectNew.GetNamespace(),\n}})\n</code></pre></li> </ol>"},{"location":"kubernetes-operator/controller-runtime/inject/","title":"inject","text":""},{"location":"kubernetes-operator/controller-runtime/inject/#inject-interface-table","title":"Inject interface table","text":"interface name required func func to inject Implemented by Cache InjectCache CacheInto Kind (source) APIReader InjectAPIReader APIReaderInto Config InjectConfig ConfigInto Client InjectClient ClientInto Scheme InjectScheme SchemeInto DeferredFileLoader, Webhook Stoppable InjectStopChannel StopChannelInto Channel (source) Mapper InjectMapper MapperInto EnqueueRequestForOwner Injector InjectFunc InjectorInto Webhook, enqueueRequestsFromMapFunc, Controller, and, or (predicate), multiMutating, multiValidating, etc. Logger InjectLogger LoggerInto Webhook <p><code>Injector</code> interface</p> <pre><code>// Func injects dependencies into i.\ntype Func func(i interface{}) error\n\n// Injector is used by the ControllerManager to inject Func into Controllers.\ntype Injector interface {\n    InjectFunc(f Func) error\n}\n\n// InjectorInto will set f and return the result on i if it implements Injector.  Returns\n// false if i does not implement Injector.\nfunc InjectorInto(f Func, i interface{}) (bool, error) {\n    if ii, ok := i.(Injector); ok {\n        return true, ii.InjectFunc(f)\n    }\n    return false, nil\n}\n</code></pre> <p><code>SetFields</code> set dependencies to the object that implments inject interface.</p>"},{"location":"kubernetes-operator/controller-runtime/inject/#usage","title":"Usage","text":"<p>Controller implement Injector with InjectFunc</p> <pre><code>// InjectFunc implement SetFields.Injector.\nfunc (c *Controller) InjectFunc(f inject.Func) error {\n    c.SetFields = f\n    return nil\n}\n</code></pre> <p>With this implementation, any function can be injected to the <code>controller.SetFields</code> with <code>InjectorInto(func, controller)</code>.</p> <p>This function is used in the manager.SetFields</p> <p><pre><code>if _, err := inject.InjectorInto(cm.SetFields, i); err != nil {\n</code></pre> This means set <code>clusterManager</code>'s <code>SetFields</code> function to <code>i</code>, specifically, <code>controller</code>. By the controller's <code>InjectFunc</code> implementation, controller has exactly the same <code>SetFields</code> function as <code>clusterManager</code></p>"},{"location":"kubernetes-operator/controller-runtime/leaderelection/","title":"leaderelection","text":"<p>Package leaderelection contains a constructor for a leader election resource lock. This is used to ensure that multiple copies of a controller manager can be run with only one active set of controllers, for active-passive HA.</p> <p>It uses built-in Kubernetes leader election APIs.</p>"},{"location":"kubernetes-operator/controller-runtime/leaderelection/#types","title":"Types","text":""},{"location":"kubernetes-operator/controller-runtime/leaderelection/#1-leader-for-life","title":"1. Leader-for-life","text":"<p>In the \"leader for life\" approach, a specific Pod is the leader. Once established (by creating a lock record), the Pod is the leader until it is destroyed. There is no possibility for multiple pods to think they are the leader at the same time. The leader does not need to renew a lease, consider stepping down, or do anything related to election activity once it becomes the leader.</p>"},{"location":"kubernetes-operator/controller-runtime/leaderelection/#2-lease-baed","title":"2. Lease-baed","text":"<p>Leases provide a way to indirectly observe whether the leader still exists. The leader must periodically renew its lease, usually by updating a timestamp in its lock record. If it fails to do so, it is presumed dead, and a new election takes place. If the leader is in fact still alive but unreachable, it is expected to gracefully step down. A variety of factors can cause a leader to fail at updating its lease, but continue acting as the leader before succeeding at stepping down.</p>"},{"location":"kubernetes-operator/controller-runtime/leaderelection/#references","title":"References","text":"<ol> <li>Go packages<ol> <li>client-go/tools/leaderelection</li> <li>controller-runtime/pkg/leaderelection: lease-based leader election</li> <li>operator-framework/operator-lib/leader: Leader For Life</li> </ol> </li> <li>Readings<ol> <li>Operator SDK # Leader Election</li> <li>https://d-kuro.github.io/post/kubernetes-leader-election/</li> <li>Leader election in Kubernetes using client-go</li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/log/","title":"log","text":"<p>Package log contains utilities for fetching a new logger when one is not already available.</p> <ol> <li>This package contains a root logr.Logger Log.</li> <li>The sub-package zap provides helpers for setting up logr backed by Zap (go.uber.org/zap).</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/log/#logr","title":"logr","text":""},{"location":"kubernetes-operator/controller-runtime/log/#zap","title":"zap","text":"<ol> <li> <p>Use <code>Zap.Options</code></p> <pre><code>opts := zap.Options{\n    Development: true,\n    TimeEncoder:  zapcore.ISO8601TimeEncoder,\n}\nopts.BindFlags(flag.CommandLine)\nflag.Parse()\nctrl.SetLogger(zap.New(zap.UseFlagOptions(&amp;opts)))\n</code></pre> </li> <li> <p>Give arguments</p> <pre><code>go run main.go -h\nUsage of /var/folders/c2/hjlk2kcn63s4kds9k2_ctdhc0000gp/T/go-build3844265895/b001/exe/main:\n  -kubeconfig string\n        Paths to a kubeconfig. Only required if out-of-cluster.\n  -zap-devel\n        Development Mode defaults(encoder=consoleEncoder,logLevel=Debug,stackTraceLevel=Warn). Production Mode defaults(encoder=jsonEncoder,logLevel=Info,stackTraceLevel=Error) (default true)\n  -zap-encoder value\n        Zap log encoding (one of 'json' or 'console')\n  -zap-log-level value\n        Zap Level to configure the verbosity of logging. Can be one of 'debug', 'info', 'error', or any integer value &gt; 0 which corresponds to custom debug levels of increasing verbosity\n  -zap-stacktrace-level value\n        Zap Level at and above which stacktraces are captured (one of 'info', 'error', 'panic').\n  -zap-time-encoding value\n        Zap time encoding (one of 'epoch', 'millis', 'nano', 'iso8601', 'rfc3339' or 'rfc3339nano'). Defaults to 'epoch'.\n</code></pre> <pre><code>go run main.go -zap-time-encoding epoch\n</code></pre> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/manager/","title":"Manager","text":"<p>The main role of Manager is 1. Manage the lifecycle of a set of controllers (registration, start and stop) 1. Provide the shared resources (Kubernetes API server client, cache, etc.)</p> <p>The registration of a controller is done by Builder.</p>"},{"location":"kubernetes-operator/controller-runtime/manager/#types","title":"types","text":""},{"location":"kubernetes-operator/controller-runtime/manager/#1-manager-interface","title":"1. Manager Interface","text":"<pre><code>// Manager initializes shared dependencies such as Caches and Clients, and provides them to Runnables.\n// A Manager is required to create Controllers.\ntype Manager interface {\n    cluster.Cluster\n    Add(Runnable) error\n    Elected() &lt;-chan struct{}\n    AddMetricsExtraHandler(path string, handler http.Handler) error\n    AddHealthzCheck(name string, check healthz.Checker) error\n    AddReadyzCheck(name string, check healthz.Checker) error\n    Start(ctx context.Context) error\n    GetWebhookServer() *webhook.Server\n    GetLogger() logr.Logger\n    GetControllerOptions() v1alpha1.ControllerConfigurationSpec\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/manager/#2-controllermanager","title":"2. controllerManager","text":"<pre><code>type controllerManager struct {\n    sync.Mutex\n    started bool\n\n    stopProcedureEngaged *int64\n    errChan              chan error\n    runnables            *runnables\n\n    // cluster holds a variety of methods to interact with a cluster. Required.\n    cluster cluster.Cluster\n\n    ...\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/manager/#3-runnable-interface","title":"3. Runnable interface","text":"<pre><code>type Runnable interface {\n    Start(context.Context) error\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/manager/#4-runnables","title":"4. runnables","text":"<pre><code>type runnables struct {\n    Webhooks       *runnableGroup\n    Caches         *runnableGroup\n    LeaderElection *runnableGroup\n    Others         *runnableGroup\n}\n\ntype runnableGroup struct {\n    ctx    context.Context\n    cancel context.CancelFunc\n\n    start        sync.Mutex\n    startOnce    sync.Once\n    started      bool\n    startQueue   []*readyRunnable\n    startReadyCh chan *readyRunnable\n\n    stop     sync.RWMutex\n    stopOnce sync.Once\n    stopped  bool\n\n    errChan chan error\n    ch chan *readyRunnable\n    wg *sync.WaitGroup\n}\n</code></pre> <p>types of runnables:</p> <ol> <li><code>Webhooks</code></li> <li><code>Caches</code></li> <li><code>LeaderElection</code></li> <li><code>Others</code></li> </ol>"},{"location":"kubernetes-operator/controller-runtime/manager/#how-manager-is-initialized-by-new","title":"How <code>Manager</code> is initialized by New","text":""},{"location":"kubernetes-operator/controller-runtime/manager/#1-set-default-values-for-options-fields-wiht-setoptionsdefaults","title":"1. Set default values for Options fields wiht setOptionsDefaults","text":"<pre><code>manager, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{})\n</code></pre> <pre><code>options = setOptionsDefaults(options)\n</code></pre> name value where is the option used newResourceLock leaderelection.NewResourceLock <code>setLeaderElectionConfig</code> newRecorderProvider intrec.NewProvider New to create recorderProvider EventBroadcaster <code>func() (record.EventBroadcaster, bool) {return record.NewBroadcaster(), true}</code> as an option for <code>newRecorderProvider</code> in New newMetricsListener metrics.NewListener New to create metricsListener LeaseDuration *defaultLeaseDuration <code>setLeaderElectionConfig</code> RenewDeadline *defaultRenewDeadline <code>setLeaderElectionConfig</code> RetryPeriod *defaultRetryPeriod <code>setLeaderElectionConfig</code> ReadinessEndpointName defaultReadinessEndpoint LivenessEndpointName defaultLivenessEndpoint newHealthProbeListener defaultHealthProbeListener GracefulShutdownTimeout *defaultGracefulShutdownPeriod Logger log.Log BaseContext defaultBaseContext <p>\u203b New in the table means Manager.New</p>"},{"location":"kubernetes-operator/controller-runtime/manager/#2-initialize-a-controllermanager","title":"2. Initialize a controllerManager","text":"<ol> <li>Initialize Cluster, which provides methods to interact with Kubernetes cluster     <pre><code>cluster, err := cluster.New(config, func(clusterOptions *cluster.Options) {\n    clusterOptions.Scheme = options.Scheme\n    clusterOptions.MapperProvider = options.MapperProvider\n    clusterOptions.Logger = options.Logger\n    clusterOptions.SyncPeriod = options.SyncPeriod\n    clusterOptions.Namespace = options.Namespace\n    clusterOptions.NewCache = options.NewCache\n    clusterOptions.NewClient = options.NewClient\n    clusterOptions.ClientDisableCacheFor = options.ClientDisableCacheFor\n    clusterOptions.DryRunClient = options.DryRunClient\n    clusterOptions.EventBroadcaster = options.EventBroadcaster //nolint:staticcheck\n})\n</code></pre>     For more details, please check cluster.     <code>Cluster</code> is also a runnable.</li> <li> <p>Initialize other necessary things like <code>recordProvider</code>, <code>runnables</code>, etc.</p> </li> <li> <p>Initialize <code>controllerManager</code></p> <pre><code>&amp;controllerManager{\n    ...\n    cluster:                       cluster,\n    runnables:                     runnables,\n    ...\n    recorderProvider:              recorderProvider,\n}\n</code></pre> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/manager/#3-bind-a-controller-to-the-manager","title":"3. Bind a Controller to the Manager","text":"<p>Bind a Controller to the Manager using NewControllerManagedBy(alias for builder.ControllerManagedBy).</p> <pre><code>err = ctrl.\n    NewControllerManagedBy(manager). // Create the Controller\n    For(&amp;appsv1.ReplicaSet{}).       // ReplicaSet is the Application API\n    Owns(&amp;corev1.Pod{}).             // ReplicaSet owns Pods created by it\n    Complete(&amp;ReplicaSetReconciler{Client: manager.GetClient()})\n</code></pre> <p>Internally, builder.Build create a new controller and add it to <code>manager.runnables.Others</code> by <code>Manager.Add(Runnable)</code>.</p> <p>You can also check Builder and Internal process of adding a Controller to a Manager</p>"},{"location":"kubernetes-operator/controller-runtime/manager/#4-controllermanagerstart-calls-runnablesxxxstart-to-start-all-runnables","title":"4. controllerManager.Start() calls <code>runnables.xxx.Start()</code> to start all runnables.","text":"<pre><code>// (1) Add the cluster runnable.\nif err := cm.add(cm.cluster); err != nil {\n...\n\n// (2) First start any webhook servers\nif err := cm.runnables.Webhooks.Start(cm.internalCtx); err != nil {\n...\n\n// (3) Start and wait for caches.\nif err := cm.runnables.Caches.Start(cm.internalCtx); err != nil {\n...\n\n// (4) Start the non-leaderelection Runnables after the cache has synced.\nif err := cm.runnables.Others.Start(cm.internalCtx); err != nil {\n\n// (5) Start the leader election and all required runnables.\nif err := cm.startLeaderElection(ctx); err != nil {\n...\nif err := cm.startLeaderElectionRunnables(); err != nil {\n...\n</code></pre> <p>Controller will be in <code>runnables.Others</code> and you can check the actual <code>Start</code> logic in controller.</p>"},{"location":"kubernetes-operator/controller-runtime/manager/#internal-process-of-adding-a-controller-to-a-manager","title":"Internal process of adding a <code>Controller</code> to a <code>Manager</code>","text":"<ol> <li><code>controllerManager.Add(Runnable)</code>: gets lock and calls <code>add(runnable)</code>.<ol> <li>cm.SetFields(r) <pre><code>if err := cm.cluster.SetFields(i); err != nil {\n    return err\n}\nif _, err := inject.InjectorInto(cm.SetFields, i); err != nil {\n    return err\n}\nif _, err := inject.StopChannelInto(cm.internalProceduresStop, i); err != nil {\n    return err\n}\nif _, err := inject.LoggerInto(cm.logger, i); err != nil {\n    return err\n}\n</code></pre><ol> <li>cluster.SetFields set dependencies on the object that implements the inject interface. Specifically set the following cluster's field to the runnable (controller)<ol> <li><code>config</code> (<code>inject.ConfigInto(c.config, i)</code>)</li> <li><code>client</code> (<code>inject.ClientInto(c.client, i)</code>)</li> <li><code>apiReader</code> (<code>inject.APIReaderInto(c.apiReader, i)</code>)</li> <li><code>scheme</code> (<code>inject.SchemeInto(c.scheme, i)</code>)</li> <li><code>cache</code> (<code>inject.CacheInto(c.cache, i)</code>)</li> <li><code>mapper</code> (<code>inject.MapperInto(c.mapper, i)</code>)</li> </ol> </li> <li><code>cm.SetFields</code> is set to <code>controller.SetFields</code> via <code>InjectorInto</code>. (details: inject) &lt;- <code>controller.SetFields</code> will be used for source, event handler and predicates in Watch.</li> <li><code>StopChannelInto</code> and <code>Logger</code>.</li> </ol> </li> <li>cm.runnables.Add(r) <pre><code>type runnables struct {\n    Webhooks       *runnableGroup\n    Caches         *runnableGroup\n    LeaderElection *runnableGroup\n    Others         *runnableGroup\n}\n</code></pre>     Add <code>r</code> based on the type.     <pre><code>func (r *runnables) Add(fn Runnable) error {\n    switch runnable := fn.(type) {\n    case hasCache: // check if `GetCache() exists\n        return r.Caches.Add(fn, func(ctx context.Context) bool {\n            return runnable.GetCache().WaitForCacheSync(ctx)\n        })\n    case *webhook.Server: // check if webhook.Server type\n        return r.Webhooks.Add(fn, nil)\n    case LeaderElectionRunnable: // check if `NeedLeaderElection() exists\n        if !runnable.NeedLeaderElection() {\n            return r.Others.Add(fn, nil)\n        }\n        return r.LeaderElection.Add(fn, nil)\n    default:\n        return r.LeaderElection.Add(fn, nil)\n    }\n}\n</code></pre></li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/manager/#managergetclient-and-getscheme","title":"<code>Manager.GetClient()</code> and <code>GetScheme()</code>","text":"<ol> <li>The client, scheme and more are initialized and stored in the cluster when a Manager is created.</li> <li>The client, scheme and more are directly got from <code>cm.cluster.GetXXX()</code></li> <li>The client got by <code>GetClient()</code> is passed to <code>Reconciler</code> so you can manipulate objects in the Reconcile function.</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/manager/#example","title":"Example","text":"<ol> <li> <p>Initialize with <code>NewManager</code>.</p> <pre><code>mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{})\n</code></pre> <p>You can configure Options based on your requirements. example:</p> <pre><code>{\n    Scheme:                 scheme,\n    MetricsBindAddress:     metricsAddr,\n    Port:                   9443,\n    HealthProbeBindAddress: probeAddr,\n    LeaderElection:         enableLeaderElection,\n    LeaderElectionID:       \"63ffe61d.example.com\",\n}\n</code></pre> </li> <li> <p>Define a simple Reconciler</p> <pre><code>podReconciler := reconcile.Func(func(ctx context.Context, req reconcile.Request) (reconcile.Result, error) {\n    fmt.Printf(\"podReconciler is called for %v\\n\", req)\n    return reconcile.Result{}, nil\n})\n</code></pre> <p>For more details about Reconciler, you can check reconciler.</p> </li> <li> <p>Set up Controller with <code>NewControllerManagedBy</code></p> <pre><code>ctrl.NewControllerManagedBy(mgr). // returns controller Builder\n    For(&amp;corev1.Pod{}). // defines the type of Object being reconciled\n    Complete(podReconciler) // Complete builds the Application controller, and return error\n</code></pre> <ol> <li><code>For</code>: define which resource to monitor.</li> <li><code>Complete</code>: pass the reconciler to complete the controller.</li> <li>Internally, <code>NewControllerManagedBy</code> returns controller builder.</li> <li>Controller builder calls two functions in <code>Complete(reconcile.Reconciler)</code><ol> <li>doController: Set controller to the builder     <pre><code>blder.ctrl, err = newController(controllerName, blder.mgr, ctrlOptions)\n</code></pre></li> <li>doWatch: call <code>blder.ctrl.Watch(src, hdler, allPredicates...)</code> for <code>For</code>, <code>Owns</code>, and <code>Watches</code>.</li> </ol> </li> </ol> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/manager/#run","title":"Run","text":"<ol> <li> <p>Run (initialize a Manager with podReconciler &amp; deploymentReconciler)</p> <pre><code>go run main.go\n2022-09-06T06:27:08.255+0900    INFO    controller-runtime.metrics      Metrics server is starting to listen    {\"addr\": \":8080\"}\n2022-09-06T06:27:08.255+0900    INFO    Starting server {\"path\": \"/metrics\", \"kind\": \"metrics\", \"addr\": \"[::]:8080\"}\n2022-09-06T06:27:08.255+0900    INFO    Starting EventSource    {\"controller\": \"pod\", \"controllerGroup\": \"\", \"controllerKind\": \"Pod\", \"source\": \"kind source: *v1.Pod\"}\n2022-09-06T06:27:08.255+0900    INFO    Starting Controller     {\"controller\": \"pod\", \"controllerGroup\": \"\", \"controllerKind\": \"Pod\"}\n2022-09-06T06:27:08.255+0900    INFO    manager-examples        RunnableFunc is called\n2022-09-06T06:27:08.255+0900    INFO    Starting EventSource    {\"controller\": \"deployment\", \"controllerGroup\": \"apps\", \"controllerKind\": \"Deployment\", \"source\": \"kind source: *v1.Deployment\"}\n2022-09-06T06:27:08.255+0900    INFO    Starting Controller     {\"controller\": \"deployment\", \"controllerGroup\": \"apps\", \"controllerKind\": \"Deployment\"}\n2022-09-06T06:27:08.356+0900    INFO    Starting workers        {\"controller\": \"pod\", \"controllerGroup\": \"\", \"controllerKind\": \"Pod\", \"worker count\": 1}\n2022-09-06T06:27:08.357+0900    INFO    Starting workers        {\"controller\": \"deployment\", \"controllerGroup\": \"apps\", \"controllerKind\": \"Deployment\", \"worker count\": 1}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/coredns-6d4b75cb6d-jtg59\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"local-path-storage/local-path-provisioner-9cd9bd544-g89rs\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/kube-scheduler-kind-control-plane\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/kube-controller-manager-kind-control-plane\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/kube-proxy-7jsn6\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/coredns-6d4b75cb6d-k68r5\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/etcd-kind-control-plane\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/kube-apiserver-kind-control-plane\"}\n2022-09-06T06:27:08.357+0900    INFO    manager-examples        podReconciler is called {\"req\": \"kube-system/kindnet-6dj6q\"}\n2022-09-06T06:27:08.358+0900    INFO    manager-examples        deploymentReconciler is called  {\"req\": \"kube-system/coredns\"}\n2022-09-06T06:27:08.358+0900    INFO    manager-examples        deploymentReconciler is called  {\"req\": \"local-path-storage/local-path-provisioner\"}\n</code></pre> <p>The reconcile functions are called when cache is synced.</p> </li> <li> <p>Create a Pod     <pre><code>kubectl run nginx --image=nginx\n</code></pre></p> <p>You'll see the following logs:</p> <p><pre><code>2022-09-06T07:16:26.400+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx\"}\n2022-09-06T07:16:26.519+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx\"}\n2022-09-06T07:16:26.660+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx\"}\n2022-09-06T07:16:32.547+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx\"}\n</code></pre> 1. Delete the Pod <pre><code>kubectl delete pod nginx\n</code></pre></p> <p>You'll see the logs again. 1. Create a Deployment <pre><code>kubectl create deploy nginx --image=nginx\n</code></pre></p> <pre><code>2022-09-06T07:17:04.963+0900    INFO    manager-examples        deploymentReconciler is called  {\"req\": \"default/nginx\"}\n2022-09-06T07:17:05.281+0900    INFO    manager-examples        deploymentReconciler is called  {\"req\": \"default/nginx\"}\n2022-09-06T07:17:05.320+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx-8f458dc5b-lnkqz\"}\n2022-09-06T07:17:05.341+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx-8f458dc5b-lnkqz\"}\n2022-09-06T07:17:05.342+0900    INFO    manager-examples        deploymentReconciler is called  {\"req\": \"default/nginx\"}\n2022-09-06T07:17:05.432+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx-8f458dc5b-lnkqz\"}\n2022-09-06T07:17:05.461+0900    INFO    manager-examples        deploymentReconciler is called  {\"req\": \"default/nginx\"}\n2022-09-06T07:17:08.630+0900    INFO    manager-examples        podReconciler is called {\"req\": \"default/nginx-8f458dc5b-lnkqz\"}\n2022-09-06T07:17:08.674+0900    INFO    manager-examples        deploymentReconciler is called  {\"req\": \"default/nginx\"}\n</code></pre> </li> <li> <p>Delete the Deployment     <pre><code>kubectl delete deploy nginx\n</code></pre></p> <p>You'll see the logs again.</p> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/reconciler/","title":"Reconciler","text":"<p>Controller logic is implemented in terms of Reconcilers (pkg/reconcile). A Reconciler implements a function which takes a reconcile Request containing the name and namespace of the object to reconcile, reconciles the object, and returns a Response or an error indicating whether to requeue for a second round of processing.</p> <p>As you can see in the diagram above, a Reconciler is part of a Controller. A Controller has a function to watch changes of the target resources, put change events into the queue, and call Reconcile function with an queue item, and requeue the item if necessary.</p>"},{"location":"kubernetes-operator/controller-runtime/reconciler/#types","title":"Types","text":""},{"location":"kubernetes-operator/controller-runtime/reconciler/#reconciler-interface","title":"Reconciler Interface","text":"<pre><code>type Reconciler interface {\n    // Reconcile performs a full reconciliation for the object referred to by the Request.\n    // The Controller will requeue the Request to be processed again if an error is non-nil or\n    // Result.Requeue is true, otherwise upon completion it will remove the work from the queue.\n    Reconcile(context.Context, Request) (Result, error)\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/reconciler/#request","title":"Request","text":"<pre><code>type Request struct {\n    // NamespacedName is the name and namespace of the object to reconcile.\n    types.NamespacedName\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/reconciler/#result","title":"Result","text":"<pre><code>type Result struct {\n    // Requeue tells the Controller to requeue the reconcile key.  Defaults to false.\n    Requeue bool\n\n    // RequeueAfter if greater than 0, tells the Controller to requeue the reconcile key after the Duration.\n    // Implies that Requeue is true, there is no need to set Requeue to true at the same time as RequeueAfter.\n    RequeueAfter time.Duration\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/reconciler/#implement","title":"Implement","text":"<p>You can use either implementation of the <code>Reconciler</code> interface: 1. a reconciler struct with <code>Reconcile</code> function. 1. a <code>reconcile.Func</code>, which implements Reconciler interface:     <pre><code>type Func func(context.Context, Request) (Result, error)\n</code></pre></p> <p>(Controller also implements Reconciler interface. The reconciler passed to <code>builder</code> is used inside the controller's <code>Reconcile</code> function.)</p>"},{"location":"kubernetes-operator/controller-runtime/reconciler/#how-reconciler-is-used","title":"How reconciler is used","text":"<p>Reconciler is passed to Controller builder when initializing controller (you can also check it in Manager):</p> <pre><code>ctrl.NewControllerManagedBy(mgr). // returns controller Builder\n    For(&amp;corev1.Pod{}). // defines the type of Object being reconciled\n    Complete(podReconciler) // Complete builds the Application controller, and return error\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/source/","title":"Source","text":"<p>Component structure: </p> <p>Dataflow: </p>"},{"location":"kubernetes-operator/controller-runtime/source/#source-interface","title":"Source interface","text":"<pre><code>type Source interface {\n    // Start is internal and should be called only by the Controller to register an EventHandler with the Informer\n    // to enqueue reconcile.Requests.\n    Start(context.Context, handler.EventHandler, workqueue.RateLimitingInterface, ...predicate.Predicate) error\n}\n\n// SyncingSource is a source that needs syncing prior to being usable. The controller\n// will call its WaitForSync prior to starting workers.\ntype SyncingSource interface {\n    Source\n    WaitForSync(ctx context.Context) error\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/source/#implementations","title":"Implementations","text":"<ol> <li>Already removed in Refactor source/handler/predicate packages to remove dep injection (from v0.15.0)     ~~<code>Kind</code> has <code>InjectCache</code> while <code>kindWithCache</code> doesn't.~~</li> <li>Kind Kind is used to provide a source of events originating inside the cluster from Watches (e.g. Pod Create).     <pre><code>type Kind struct {\n    Type client.Object\n    cache cache.Cache\n    started     chan error\n    startCancel func()\n}\n</code></pre><ol> <li>Provide cache explicitly with the initialization method <code>func Kind(cache cache.Cache, object client.Object) SyncingSource</code>.     <pre><code>source.Kind(mgr.GetCache(), &amp;corev1.Pod{})\n</code></pre></li> </ol> </li> <li>Channel: Channel is used to provide a source of events originating outside the cluster (e.g. GitHub Webhook callback).  Channel requires the user to wire the external source (eh.g. http handler) to write GenericEvents to the underlying channel.</li> <li>Informer: Informer is used to provide a source of events originating inside the cluster from Watches (e.g. Pod Create).     <pre><code>type Informer struct {\n    // Informer is the controller-runtime Informer\n    Informer cache.Informer\n}\n</code></pre></li> </ol> <p>What's the difference between <code>Informer</code> and <code>Kind</code>?</p> <ol> <li><code>Kind</code> gets informer from <code>cache.Cache</code>.</li> <li><code>Informer</code> needs to be initialized with <code>cache.Informer</code> directly.</li> <li><code>Kind</code> calls <code>WaitForCacheSync</code> after adding eventhandler by <code>AddEventHandler</code> while <code>Informer</code> doesn't.</li> </ol>"},{"location":"kubernetes-operator/controller-runtime/source/#how-source-is-used","title":"How <code>Source</code> is used","text":"<ol> <li><code>Source</code> is initialized in <code>builder.doWatch</code> for each of <code>For</code>, <code>Owns</code>, and <code>Watches</code>:<ol> <li>For:     <pre><code>// Reconcile type\ntypeForSrc, err := blder.project(blder.forInput.object, blder.forInput.objectProjection)\nif err != nil {\n    return err\n}\nsrc := &amp;source.Kind{Type: typeForSrc}\n</code></pre></li> <li>Owns:     <pre><code>typeForSrc, err := blder.project(own.object, own.objectProjection)\nif err != nil {\n    return err\n}\nsrc := &amp;source.Kind{Type: typeForSrc}\n</code></pre></li> <li>Watches:     <pre><code>// If the source of this watch is of type *source.Kind, project it.\nif srckind, ok := w.src.(*source.Kind); ok {\n    typeForSrc, err := blder.project(srckind.Type, w.objectProjection)\n    if err != nil {\n        return err\n    }\n    srckind.Type = typeForSrc\n}\n</code></pre></li> </ol> </li> <li> <p>The initialized source is passed to <code>controller.Watch</code> in builder.doWatch if the controller is initialized by builder</p> <p><pre><code>if err := blder.ctrl.Watch(w.src, w.eventhandler, allPredicates...); err != nil {\n    return err\n}\n</code></pre> 1. In controller.Watch 1. <code>Cache</code> is injected from controller.     <pre><code>// Inject Cache into arguments\nif err := c.SetFields(src); err != nil {\n    return err\n}\n</code></pre> 1. <code>source.Start</code> is called with <code>EventHandler</code> and <code>Queue</code> <pre><code>return src.Start(c.ctx, evthdler, c.Queue, prct...)\n</code></pre> 1. Source.Start 1. Get <code>informer</code> from the injected <code>cache</code>.     <pre><code>i, lastErr = ks.cache.GetInformer(ctx, ks.Type)\n</code></pre> 1. Add the event handler with <code>AddEventHandler</code> <pre><code>i.AddEventHandler(internal.EventHandler{Queue: queue, EventHandler: handler, Predicates: prct})\n</code></pre> 1. informer is started by <code>manager.Start()</code>. <pre><code>manager.runnables.Cache.Start()\n</code></pre> 1. cache is initialized in cluster when a Manager is created.</p> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/source/#example-usage-debugging-your-controller","title":"Example Usage: Debugging your controller","text":"<p>if you want to check events of specific resource you can set by the following.</p> <ol> <li> <p>Set up scheme if you want to monitor CRD (Optional)     <pre><code>import (\n    mysqlv1alpha1 \"github.com/nakamasato/mysql-operator/api/v1alpha1\" // Target CRD\n    utilruntime \"k8s.io/apimachinery/pkg/util/runtime\"\n    clientgoscheme \"k8s.io/client-go/kubernetes/scheme\"\n)\n\nfunc init() {\n    utilruntime.Must(mysqlv1alpha1.AddToScheme(scheme))\n    utilruntime.Must(clientgoscheme.AddToScheme(scheme))\n}\n</code></pre></p> </li> <li> <p><code>cache.Get()</code>: Internally create informer if not exists.</p> <p><pre><code>pod := &amp;v1.Pod{}\ncache.Get(ctx, client.ObjectKeyFromObject(pod), pod)\n\nmysqluser := &amp;mysqlv1alpha1.MySQLUser{}\ncache.Get(ctx, client.ObjectKeyFromObject(mysqluser), mysqluser)\n</code></pre> 1. Start the cache.</p> <p><pre><code>go func() {\n    if err := cache.Start(ctx); err != nil { // func (m *InformersMap) Start(ctx context.Context) error {\n        log.Error(err, \"failed to start cache\")\n    }\n}()\n</code></pre> 1. Create <code>Kind</code> for the target resource. <pre><code>kind := source.Kind(cache, mysqluser)\n</code></pre> 1. Prepare <code>workqueue</code> and <code>eventHandler</code>. <pre><code>queue := workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"test\")\neventHandler := handler.Funcs{\n    CreateFunc: func(e event.CreateEvent, q workqueue.RateLimitingInterface) {\n        log.Info(\"CreateFunc is called\", \"object\", e.Object.GetName())\n        // queue.Add(WorkQueueItem{Event: \"Create\", Name: e.Object.GetName()})\n    },\n    UpdateFunc: func(e event.UpdateEvent, q workqueue.RateLimitingInterface) {\n        log.Info(\"UpdateFunc is called\", \"objectNew\", e.ObjectNew.GetName(), \"objectOld\", e.ObjectOld.GetName())\n        // queue.Add(WorkQueueItem{Event: \"Update\", Name: e.ObjectNew.GetName()})\n    },\n    DeleteFunc: func(e event.DeleteEvent, q workqueue.RateLimitingInterface) {\n        log.Info(\"DeleteFunc is called\", \"object\", e.Object.GetName())\n        // queue.Add(WorkQueueItem{Event: \"Delete\", Name: e.Object.GetName()})\n    },\n}\n</code></pre> 1. Start <code>kind</code> with the prepared <code>eventHandler</code> and <code>queue</code>. <pre><code>if err := kind.Start(ctx, eventHandler, queue); err != nil { // Get informer and set eventHandler\n    log.Error(err, \"\")\n}\n</code></pre></p> </li> </ol> <p>You can run:</p> <ol> <li>Install your CRD     <pre><code>kubectl apply -f https://raw.githubusercontent.com/nakamasato/mysql-operator/main/config/crd/bases/mysql.nakamasato.com_mysqlusers.yaml\n</code></pre></li> <li> <p>Run the <code>kind</code> <pre><code>go run main.go\n</code></pre></p> <pre><code>2022-09-15T06:58:43.895+0900    INFO    source-examples source start\n2022-09-15T06:58:44.070+0900    INFO    source-examples cache is created\n2022-09-15T06:58:44.071+0900    INFO    source-examples cache is started\n2022-09-15T06:58:44.096+0900    INFO    source-examples CreateFunc is called    {\"object\": \"kube-apiserver-kind-control-plane\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"kube-controller-manager-kind-control-plane\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"kube-scheduler-kind-control-plane\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"kube-proxy-zpj2w\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"coredns-6d4b75cb6d-s2dhg\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"coredns-6d4b75cb6d-25dbf\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"etcd-kind-control-plane\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"kindnet-8fjbg\"}\n2022-09-15T06:58:44.097+0900    INFO    source-examples CreateFunc is called    {\"object\": \"local-path-provisioner-9cd9bd544-xl67h\"}\n2022-09-15T06:58:44.172+0900    INFO    source-examples kindWithCache is ready\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"kube-apiserver-kind-control-plane\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"kube-controller-manager-kind-control-plane\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"kube-scheduler-kind-control-plane\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"kube-proxy-zpj2w\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"coredns-6d4b75cb6d-s2dhg\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"coredns-6d4b75cb6d-25dbf\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"etcd-kind-control-plane\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"kindnet-8fjbg\"}}\n2022-09-15T06:58:44.172+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"local-path-provisioner-9cd9bd544-xl67h\"}}\n</code></pre> </li> <li> <p>Create custom resource manually. You'll see the events related to the CRD.     <pre><code>kubectl apply -f https://raw.githubusercontent.com/nakamasato/mysql-operator/main/config/samples/mysql_v1alpha1_mysqluser.yaml\n</code></pre></p> <pre><code>2024-07-27T11:44:33.871+0900    INFO    source-examples CreateFunc is called    {\"object\": \"sample-user\"}\n2024-07-27T11:44:33.872+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"sample-user\"}}\n</code></pre> </li> <li> <p>Create nginx Pod     <pre><code>kubectl run nginx --image=nginx\n</code></pre></p> <pre><code>2024-07-27T11:45:13.787+0900    INFO    source-examples CreateFunc is called    {\"object\": \"nginx\"}\n2024-07-27T11:45:13.787+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Create\",\"Name\":\"nginx\"}}\n2024-07-27T11:45:13.805+0900    INFO    source-examples UpdateFunc is called    {\"objectNew\": \"nginx\", \"objectOld\": \"nginx\"}\n2024-07-27T11:45:13.805+0900    INFO    source-examples got item        {\"item\": {\"Event\":\"Update\",\"Name\":\"nginx\"}}\n2024-07-27T11:45:13.819+0900    INFO    source-examples UpdateFunc is called    {\"objectNew\": \"nginx\", \"objectOld\": \"nginx\"}\n2024-07-27T11:45:16.417+0900    INFO    source-examples UpdateFunc is called    {\"objectNew\": \"nginx\", \"objectOld\": \"nginx\"}\n</code></pre> </li> </ol>"},{"location":"kubernetes-operator/controller-runtime/webhook/","title":"webhook (WIP)","text":"<p>Package webhook provides methods to build and bootstrap a webhook server.</p> <pre><code>type Server interface {\n    // NeedLeaderElection implements the LeaderElectionRunnable interface, which indicates\n    // the webhook server doesn't need leader election.\n    NeedLeaderElection() bool\n\n    // Register marks the given webhook as being served at the given path.\n    // It panics if two hooks are registered on the same path.\n    Register(path string, hook http.Handler)\n\n    // Start runs the server.\n    // It will install the webhook related resources depend on the server configuration.\n    Start(ctx context.Context) error\n\n    // StartedChecker returns an healthz.Checker which is healthy after the\n    // server has been started.\n    StartedChecker() healthz.Checker\n\n    // WebhookMux returns the servers WebhookMux\n    WebhookMux() *http.ServeMux\n}\n</code></pre> <pre><code>type Defaulter interface {\n    runtime.Object\n    Default()\n}\n</code></pre> <pre><code>type Validator interface {\n    runtime.Object\n    ValidateCreate() error\n    ValidateUpdate(old runtime.Object) error\n    ValidateDelete() error\n}\n</code></pre> <p>Example:</p> <pre><code>// Create a manager\n// Note: GetConfigOrDie will os.Exit(1) w/o any message if no kube-config can be found\nmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{})\nif err != nil {\n    panic(err)\n}\n\n// Create a webhook server.\nhookServer := NewServer(Options{\n    Port: 8443,\n})\nif err := mgr.Add(hookServer); err != nil {\n    panic(err)\n}\n\n// Register the webhooks in the server.\nhookServer.Register(\"/mutating\", mutatingHook)\nhookServer.Register(\"/validating\", validatingHook)\n\n// Start the server by starting a previously-set-up manager\nerr = mgr.Start(ctrl.SetupSignalHandler())\nif err != nil {\n    // handle error\n    panic(err)\n}\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/webhook/#run","title":"Run","text":"<pre><code>go run contents/kubernetes-operator/controller-runtime/webhook/main.go\npanic: open /var/folders/c2/hjlk2kcn63s4kds9k2_ctdhc0000gp/T/k8s-webhook-server/serving-certs/tls.crt: no such file or directory\n\ngoroutine 1 [running]:\nmain.main()\n        /Users/m.naka/repos/nakamasato/kubernetes-training/contents/kubernetes-operator/controller-runtime/webhook/main.go:56 +0x1c4\nexit status 2\n</code></pre>"},{"location":"kubernetes-operator/controller-runtime/webhook/#validator","title":"Validator","text":"<p>Validator interface:</p> <pre><code>type Validator interface {\n    runtime.Object\n\n    // ValidateCreate validates the object on creation.\n    // The optional warnings will be added to the response as warning messages.\n    // Return an error if the object is invalid.\n    ValidateCreate() (warnings Warnings, err error)\n\n    // ValidateUpdate validates the object on update. The oldObj is the object before the update.\n    // The optional warnings will be added to the response as warning messages.\n    // Return an error if the object is invalid.\n    ValidateUpdate(old runtime.Object) (warnings Warnings, err error)\n\n    // ValidateDelete validates the object on deletion.\n    // The optional warnings will be added to the response as warning messages.\n    // Return an error if the object is invalid.\n    ValidateDelete() (warnings Warnings, err error)\n}\n</code></pre> <p>The return value was updated in controller-runtime@v0.15.0 (\u26a0\ufe0f feat: new features about support warning with webhook #2014) from [Feature Request]: Support \"Warning\" for Validation Webhook #1896</p> <p>This is because Kubernets supports <code>warning</code> message in response for Admission webhook ref since 1.19:</p> <p>Admission webhooks can optionally return warning messages that are returned to the requesting client in HTTP Warning headers with a warning code of 299. Warnings can be sent with allowed or rejected admission responses.</p>"},{"location":"kubernetes-operator/controller-runtime/webhook/#changes","title":"Changes","text":"<ol> <li>v0.15.0<ol> <li>Allow passing a custom webhook server controller-runtime#2293 <code>webhook.Server</code> <code>struct</code> was changed to <code>interface</code>.     <pre><code>- hookServer := &amp;Server{Port: 8443}\n+ hookServer := NewServer(Options{Port: 8443})\n</code></pre></li> <li>\u26a0\ufe0f feat: new features about support warning with webhook #2014 <code>Validator</code>, <code>CustomValidator</code> interface change: added warning to response of admission webhook.     <pre><code>type Validator interface {\n    runtime.Object\n-   ValidateCreate() error\n-   ValidateUpdate(old runtime.Object) error\n-   ValidateDelete() error\n\n+   // ValidateCreate validates the object on creation.\n+   // The optional warnings will be added to the response as warning messages.\n+   // Return an error if the object is invalid.\n+   ValidateCreate() (warnings Warnings, err error)\n+   // ValidateUpdate validates the object on update. The oldObj is the object before the update.\n+   // The optional warnings will be added to the response as warning messages.\n+   // Return an error if the object is invalid.\n+   ValidateUpdate(old runtime.Object) (warnings Warnings, err error)\n+   // ValidateDelete validates the object on deletion.\n+   // The optional warnings will be added to the response as warning messages.\n+   // Return an error if the object is invalid.\n+   ValidateDelete() (warnings Warnings, err error)\n}\n</code></pre></li> </ol> </li> </ol>"},{"location":"kubernetes-operator/list/","title":"Kubernetes Operator List","text":""},{"location":"kubernetes-operator/list/#monitoring","title":"Monitoring","text":"<ol> <li>https://github.com/prometheus-operator/prometheus-operator: prometheus-operator</li> <li>https://github.com/grafana-operator/grafana-operator grafana-operator</li> </ol>"},{"location":"kubernetes-operator/list/#database","title":"Database","text":"<ol> <li>https://github.com/movetokube/postgres-operator</li> <li>https://github.com/zalando/postgres-operator: postgres-operator</li> <li>https://github.com/mysql/mysql-operator mysql-operator</li> <li>https://github.com/vitessio/vitess</li> </ol>"},{"location":"kubernetes-operator/list/#middleware","title":"Middleware","text":"<ol> <li>https://github.com/strimzi/strimzi-kafka-operator: strimzi</li> <li>https://github.com/rabbitmq/cluster-operator</li> </ol>"},{"location":"kubernetes-operator/list/#cicd","title":"CI/CD","text":"<ol> <li>https://github.com/argoproj/argo-cd: argocd: appcontroller.go</li> <li>https://github.com/actions/actions-runner-controller</li> </ol>"},{"location":"kubernetes-operator/list/#external-service","title":"External Service","text":"<ol> <li>https://github.com/1Password/onepassword-operator</li> <li>https://github.com/hashicorp/vault-secrets-operator</li> </ol>"},{"location":"kubernetes-operator/list/#iac","title":"IaC","text":"<ol> <li>https://github.com/hashicorp/terraform-k8s: terraform-k8s</li> <li>https://github.com/isaaguilar/terraform-operator</li> </ol>"},{"location":"kubernetes-the-hard-way/","title":"Kubernetes the hard way","text":"<p>https://github.com/kelseyhightower/kubernetes-the-hard-way</p>"},{"location":"kubernetes-the-hard-way/#01-prerequisite","title":"01 Prerequisite","text":"<p>https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/01-prerequisites.md</p> <ul> <li>install gcloud https://cloud.google.com/sdk/docs/quickstart#mac</li> </ul> <pre><code>\u25cb ./google-cloud-sdk/bin/gcloud init\nWelcome! This command will take you through the configuration of gcloud.\n\nSettings from your current configuration [default] are:\ncore:\n  account: xxx@gmail.com\n  disable_usage_reporting: 'False'\n\nPick configuration to use:\n [1] Re-initialize this configuration [default] with new settings\n [2] Create a new configuration\nPlease enter your numeric choice:  1\n\nYour current configuration has been set to: [default]\n\nYou can skip diagnostics next time by using the following flag:\n  gcloud init --skip-diagnostics\n\nNetwork diagnostic detects and fixes local network connection issues.\nChecking network connection...done.\nReachability Check passed.\nNetwork diagnostic passed (1/1 checks passed).\n\nChoose the account you would like to use to perform operations for\nthis configuration:\n [1] xxx@gmail.com\n [2] Log in with a new account\nPlease enter your numeric choice:  1\n\nYou are logged in as: [xxx@gmail.com].\n\nPick cloud project to use:\n [1] xxx\n [2] xxx\n ...\n [5] Create a new project\nPlease enter numeric choice or text value (must exactly match list\nitem):  5\n\nEnter a Project ID. Note that a Project ID CANNOT be changed later.\nProject IDs must be 6-30 characters (lowercase ASCII, digits, or\nhyphens) in length and start with a lowercase letter. k8s-the-hard-way-20210213\nWaiting for [operations/cp.7017970540723547437] to finish...done.\nYour current project has been set to: [k8s-the-hard-way-20210213].\n</code></pre> <pre><code>gcloud config list\n[compute]\nregion = us-west1\nzone = us-west1-c\n[core]\naccount = masatonaka1989@gmail.com\ndisable_usage_reporting = False\nproject = k8s-the-hard-way-20210213\n\nYour active configuration is: [default]\n</code></pre>"},{"location":"kubernetes-the-hard-way/#03-compute-resources","title":"03 Compute Resources","text":""},{"location":"kubernetes-the-hard-way/#create-custom-vpc-networks","title":"Create custom VPC networks","text":"<pre><code>gcloud compute networks create kubernetes-the-hard-way --subnet-mode custom\n</code></pre> <p>If it fails, you might need to enable Compute Engine API on console.</p> <pre><code>\u00b1 gcloud compute networks create kubernetes-the-hard-way --subnet-mode custom\nERROR: (gcloud.compute.networks.create) Could not fetch resource:\n - Project XXXX is not found and cannot be used for API calls. If it is recently created, enable Compute Engine API by visiting https://console.developers.google.com/apis/api/compute.googleapis.com/overview?project=XXXXX then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\n</code></pre> <pre><code> gcloud compute networks create kubernetes-the-hard-way --subnet-mode custom\nCreated [https://www.googleapis.com/compute/v1/projects/k8s-the-hard-way-20210213/global/networks/kubernetes-the-hard-way].\nNAME                     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\nkubernetes-the-hard-way  CUSTOM       REGIONAL\n\nInstances on this network will not be reachable until firewall rules\nare created. As an example, you can allow all internal traffic between\ninstances as well as SSH, RDP, and ICMP by running:\n\n$ gcloud compute firewall-rules create &lt;FIREWALL_NAME&gt; --network kubernetes-the-hard-way --allow tcp,udp,icmp --source-ranges &lt;IP_RANGE&gt;\n$ gcloud compute firewall-rules create &lt;FIREWALL_NAME&gt; --network kubernetes-the-hard-way --allow tcp:22,tcp:3389,icmp\n</code></pre>"},{"location":"kubernetes-the-hard-way/#networks-subnets","title":"networks subnets","text":"<pre><code>gcloud compute networks subnets create kubernetes \\\n  --network kubernetes-the-hard-way \\\n  --range 10.240.0.0/24\n</code></pre>"},{"location":"kubernetes-the-hard-way/#firewall-rules","title":"firewall-rules","text":"<p>internal:</p> <pre><code>gcloud compute firewall-rules create kubernetes-the-hard-way-allow-internal \\\n  --allow tcp,udp,icmp \\\n  --network kubernetes-the-hard-way \\\n  --source-ranges 10.240.0.0/24,10.200.0.0/16\n</code></pre> <p>external:</p> <pre><code>gcloud compute firewall-rules create kubernetes-the-hard-way-allow-external \\\n  --allow tcp:22,tcp:6443,icmp \\\n  --network kubernetes-the-hard-way \\\n  --source-ranges 0.0.0.0/0\n</code></pre> <p>Check:</p> <pre><code>gcloud compute firewall-rules list --filter=\"network:kubernetes-the-hard-way\"\n\nNAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW                 DENY  DISABLED\nkubernetes-the-hard-way-allow-external  kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp        False\nkubernetes-the-hard-way-allow-internal  kubernetes-the-hard-way  INGRESS    1000      tcp,udp,icmp                False\n\nTo show all fields of the firewall, please show in JSON format: --format=json\nTo show all fields in table format, please see the examples in --help.\n</code></pre>"},{"location":"kubernetes-the-hard-way/#static-public-ip-address","title":"Static Public IP Address","text":"<pre><code>gcloud compute addresses create kubernetes-the-hard-way \\\n  --region $(gcloud config get-value compute/region)\n</code></pre>"},{"location":"kubernetes-the-hard-way/#instances","title":"instances","text":"<p>Control Plane:</p> <pre><code>for i in 0 1 2; do\n  gcloud compute instances create controller-${i} \\\n    --async \\\n    --boot-disk-size 200GB \\\n    --can-ip-forward \\\n    --image-family ubuntu-1804-lts \\\n    --image-project ubuntu-os-cloud \\\n    --machine-type n1-standard-1 \\\n    --private-network-ip 10.240.0.1${i} \\\n    --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \\\n    --subnet kubernetes \\\n    --tags kubernetes-the-hard-way,controller\ndone\n</code></pre> <p>Workers:</p> <pre><code>for i in 0 1 2; do\n  gcloud compute instances create worker-${i} \\\n    --async \\\n    --boot-disk-size 200GB \\\n    --can-ip-forward \\\n    --image-family ubuntu-1804-lts \\\n    --image-project ubuntu-os-cloud \\\n    --machine-type n1-standard-1 \\\n    --metadata pod-cidr=10.200.${i}.0/24 \\\n    --private-network-ip 10.240.0.2${i} \\\n    --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \\\n    --subnet kubernetes \\\n    --tags kubernetes-the-hard-way,worker\ndone\n</code></pre> <pre><code>gcloud compute instances list --filter=\"tags.items=kubernetes-the-hard-way\"\n\nNAME          ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP      STATUS\ncontroller-0  us-west1-c  e2-standard-2               10.240.0.10  35.203.130.132   RUNNING\ncontroller-1  us-west1-c  e2-standard-2               10.240.0.11  35.197.106.92    RUNNING\ncontroller-2  us-west1-c  e2-standard-2               10.240.0.12  104.196.237.244  RUNNING\nworker-0      us-west1-c  e2-standard-2               10.240.0.20  34.82.0.74       RUNNING\nworker-1      us-west1-c  e2-standard-2               10.240.0.21  35.247.104.63    RUNNING\nworker-2      us-west1-c  e2-standard-2               10.240.0.22  35.230.29.250    RUNNING\n</code></pre>"},{"location":"kubernetes-the-hard-way/#04-provisioning-a-ca-and-generating-tls-certificates","title":"04 Provisioning a CA and Generating TLS Certificates","text":"<ol> <li> <p>Provision a Certificate Authority</p> <ol> <li>Prepare <code>ca-config.json</code> + <code>ca.csr.json</code></li> <li><code>cfssl gencert -initca ca-csr.json | cfssljson -bare ca</code> -&gt; generate <code>ca-key.pem</code>, <code>ca.csr</code>, <code>ca.pem</code></li> </ol> </li> <li> <p>Client and Server certificates</p> <ol> <li>Prepare <code>admin-csr.json</code></li> <li><code>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin</code> -&gt; generate <code>admin-key.pem</code>, <code>admin.pem</code>, <code>admin.csr</code></li> </ol> </li> <li> <p>Kubelet Client Certificates</p> <ol> <li> <p>For each worker node, generate csr json and generate certificates.</p> <pre><code>for instance in worker-0 worker-1 worker-2; do\ncat &gt; ${instance}-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"system:node:${instance}\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"Portland\",\n      \"O\": \"system:nodes\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Oregon\"\n    }\n  ]\n}\nEOF\n\nEXTERNAL_IP=$(gcloud compute instances describe ${instance} \\\n  --format 'value(networkInterfaces[0].accessConfigs[0].natIP)')\n\nINTERNAL_IP=$(gcloud compute instances describe ${instance} \\\n  --format 'value(networkInterfaces[0].networkIP)')\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=${instance},${EXTERNAL_IP},${INTERNAL_IP} \\\n  -profile=kubernetes \\\n  ${instance}-csr.json | cfssljson -bare ${instance}\ndone\n</code></pre> </li> </ol> </li> <li> <p>The Controller Manager Client Certificate</p> </li> <li>The Kube Proxy Client Certificate</li> <li>The Scheduler Client Certificate</li> <li>The Kubernetes API Server Certificate</li> <li>The Service Account Key Pair</li> <li> <p>Distribute the Client and Server Certificates</p> <ol> <li> <p>Worker instance</p> <p><pre><code>for instance in worker-0 worker-1 worker-2; do\n  gcloud compute scp ca.pem ${instance}-key.pem ${instance}.pem ${instance}:~/\ndone\n</code></pre>     1. Controller instance</p> <pre><code>for instance in controller-0 controller-1 controller-2; do\n  gcloud compute scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\\n    service-account-key.pem service-account.pem ${instance}:~/\ndone\n</code></pre> </li> </ol> </li> </ol>"},{"location":"kubernetes-the-hard-way/#05-generating-kubernetes-configuration-files-for-authentication","title":"05 Generating Kubernetes Configuration Files for Authentication","text":"<p>Kubeconfigs: enable Kubernetes clients to locate and authenticate to the Kubernetes API servers</p> <ul> <li> <p>kubelet (set-cluster, set-credentials, set-context for each worker)</p> <pre><code>  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n    --kubeconfig=${instance}.kubeconfig\n  kubectl config set-credentials system:node:${instance} \\\n    --client-certificate=${instance}.pem \\\n    --client-key=${instance}-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=${instance}.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=system:node:${instance} \\\n    --kubeconfig=${instance}.kubeconfig\n</code></pre> </li> <li> <p>kube-proxy (set-cluster, set-credentials, set-context (generate only one))     <pre><code>  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n    --kubeconfig=kube-proxy.kubeconfig\n\n  kubectl config set-credentials system:kube-proxy \\\n    --client-certificate=kube-proxy.pem \\\n    --client-key=kube-proxy-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-proxy.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=system:kube-proxy \\\n    --kubeconfig=kube-proxy.kubeconfig\n</code></pre></p> </li> <li>kube-controller-manager (almost same as kube-proxy)</li> <li>kube-scheduler (almost same as kube-proxy)</li> <li>admin (almost same as kube-proxy)</li> </ul> <p>Distribute Kuberenetes configuration file - workers (worker-x, kube-proxy) - controllers (admin, kubec-controller-manager, kube-scheduler)</p>"},{"location":"kubernetes-the-hard-way/#06-generating-the-data-encryption-config-and-key","title":"06 Generating the Data Encryption Config and Key","text":"<p>Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest.</p> <p>= Encrypt data in etcd.</p> <ol> <li>Generate encryption-config.yaml with <code>EncryptionConfig</code> resource with ENCRYPTION_KEY generated in local.</li> <li>Put the yaml file in each controler node.</li> </ol>"},{"location":"kubernetes-the-hard-way/#07-bootstrapping-the-etcd-cluster","title":"07 Bootstrapping the etcd Cluster","text":"<p>Run the command to set up etcd on the controller-0, controller-1, and controller-2 (I used iterm2's toggle input to run exactly the same command in all of the server at the same time)</p> <p>In each controller node,</p> <ol> <li> <p>Download and Install the etcd Binaries</p> <p><code>wget -q --show-progress --https-only --timestamping \"https://github.com/etcd-io/etcd/releases/download/v3.4.10/etcd-v3.4.10-linux-amd64.tar.gz\"</code></p> <p><pre><code>tar -xvf etcd-v3.4.10-linux-amd64.tar.gz\nsudo mv etcd-v3.4.10-linux-amd64/etcd* /usr/local/bin/\n</code></pre> 1. Configure the etcd Server (<code>/etc/etcd/</code>)</p> <pre><code>sudo mkdir -p /etc/etcd /var/lib/etcd\nsudo chmod 700 /var/lib/etcd\nsudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/\n</code></pre> <ol> <li> <p><code>INTERNAL_IP</code></p> </li> <li> <p>Set <code>ETCD_NAME</code></p> </li> <li> <p>Set <code>/etc/systemd/system/etcd.service</code></p> </li> </ol> </li> <li> <p>Start etcd</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable etcd\nsudo systemctl start etcd\n</code></pre> </li> </ol>"},{"location":"kubernetes-the-hard-way/#08-bootstrapping-the-kubernetes-control-plane","title":"08 Bootstrapping the Kubernetes Control Plane","text":"<p>The following commands are run in all the controller nodes</p> <ol> <li> <p>Create dir <code>/etc/kubernetes/config</code></p> </li> <li> <p>Download <code>kube-api-server</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>kubectl</code> and move to <code>/usr/local/bin</code></p> </li> </ol> <pre><code>wget -q --show-progress --https-only --timestamping \\\n  \"https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-apiserver\" \\\n  \"https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-controller-manager\" \\\n  \"https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-scheduler\" \\\n  \"https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kubectl\"\n</code></pre> <ol> <li> <p>Configure the Kubernetes API Server</p> <ol> <li>Prepare <code>/etc/systemd/system/kube-apiserver.service</code></li> </ol> </li> <li> <p>Configure the Kubernetes Controller Manager</p> <ol> <li>Prepare <code>/etc/systemd/system/kube-controller-manager.service</code></li> </ol> </li> <li> <p>Configure the Kubernetes Scheduler</p> <ol> <li> <p><code>/etc/kubernetes/config/kube-scheduler.yaml</code></p> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1alpha1\nkind: KubeSchedulerConfiguration\nclientConnection:\n  kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\"\nleaderElection:\n  leaderElect: true\n</code></pre> </li> <li> <p>Prepare <code>/etc/systemd/system/kube-scheduler.service</code></p> <pre><code>ExecStart=/usr/local/bin/kube-scheduler \\\\\n  --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\\n  --v=2\n</code></pre> </li> </ol> </li> <li> <p>Start <code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code></p> </li> <li> <p>Enable HTTP Health Checks</p> <ol> <li> <p>Install nginx</p> </li> <li> <p>Configuration of <code>kubernetes.default.svc.cluster.local</code></p> <pre><code>server {\n  listen      80;\n  server_name kubernetes.default.svc.cluster.local;\n\n  location /healthz {\n    proxy_pass                    https://127.0.0.1:6443/healthz;\n    proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem;\n  }\n}\n</code></pre> </li> <li> <p>Restart nginx</p> </li> </ol> </li> <li> <p>Verification</p> <pre><code>kubectl get componentstatuses --kubeconfig admin.kubeconfig\nNAME                 STATUS    MESSAGE             ERROR\nscheduler            Healthy   ok\ncontroller-manager   Healthy   ok\netcd-1               Healthy   {\"health\":\"true\"}\netcd-0               Healthy   {\"health\":\"true\"}\netcd-2               Healthy   {\"health\":\"true\"}\n</code></pre> <pre><code>curl -H \"Host: kubernetes.default.svc.cluster.local\" -i http://127.0.0.1/healthz\nHTTP/1.1 200 OK\nServer: nginx/1.18.0 (Ubuntu)\nDate: Sat, 13 Feb 2021 08:45:57 GMT\nContent-Type: text/plain; charset=utf-8\nContent-Length: 2\nConnection: keep-alive\nCache-Control: no-cache, private\nX-Content-Type-Options: nosniff\n</code></pre> </li> <li> <p>RBAC for Kubelet Authorization (only on one controller)</p> <p>ClusterRole</p> <pre><code>cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:kube-apiserver-to-kubelet\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/proxy\n      - nodes/stats\n      - nodes/log\n      - nodes/spec\n      - nodes/metrics\n    verbs:\n      - \"*\"\nEOF\n</code></pre> <p>ClusterRoleBinding</p> <pre><code>cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:kube-apiserver\n  namespace: \"\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kube-apiserver-to-kubelet\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: kubernetes\nEOF\n</code></pre> </li> <li> <p>The Kubernetes Frontend Load Balancer</p> <ol> <li> <p>Provision a Network Load Balancer</p> <pre><code>KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \\\n    --region $(gcloud config get-value compute/region) \\\n    --format 'value(address)')\n\n  gcloud compute http-health-checks create kubernetes \\\n    --description \"Kubernetes Health Check\" \\\n    --host \"kubernetes.default.svc.cluster.local\" \\\n    --request-path \"/healthz\"\n\n  gcloud compute firewall-rules create kubernetes-the-hard-way-allow-health-check \\\n    --network kubernetes-the-hard-way \\\n    --source-ranges 209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 \\\n    --allow tcp\n\n  gcloud compute target-pools create kubernetes-target-pool \\\n    --http-health-check kubernetes\n\n  gcloud compute target-pools add-instances kubernetes-target-pool \\\n  --instances controller-0,controller-1,controller-2\n\n  gcloud compute forwarding-rules create kubernetes-forwarding-rule \\\n    --address ${KUBERNETES_PUBLIC_ADDRESS} \\\n    --ports 6443 \\\n    --region $(gcloud config get-value compute/region) \\\n    --target-pool kubernetes-target-pool\n</code></pre> </li> <li> <p>Verification</p> <pre><code>curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version\n\n{\n  \"major\": \"1\",\n  \"minor\": \"18\",\n  \"gitVersion\": \"v1.18.6\",\n  \"gitCommit\": \"dff82dc0de47299ab66c83c626e08b245ab19037\",\n  \"gitTreeState\": \"clean\",\n  \"buildDate\": \"2020-07-15T16:51:04Z\",\n  \"goVersion\": \"go1.13.9\",\n  \"compiler\": \"gc\",\n  \"platform\": \"linux/amd64\"\n}%\n</code></pre> </li> </ol> </li> </ol>"},{"location":"kubernetes-the-hard-way/#bootstrapping-the-kubernetes-worker-nodes","title":"Bootstrapping the Kubernetes Worker Nodes","text":"<p>Login to all the worker nodes <code>gcloud compute ssh worker-0</code></p> <ol> <li> <p>Install <code>socat</code>, <code>conntrack</code>, <code>ipset</code></p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get -y install socat conntrack ipset\n</code></pre> </li> <li> <p>Download and Install Worker Binaries (<code>crictl</code>, <code>runc</code>, <code>cni-plugins-linx</code>, <code>containerd</code>, <code>kubectl</code>, <code>kube-proxy</code>, <code>kubelet</code>)</p> <pre><code>wget -q --show-progress --https-only --timestamping \\\n  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.18.0/crictl-v1.18.0-linux-amd64.tar.gz \\\n  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc91/runc.amd64 \\\n  https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz \\\n  https://github.com/containerd/containerd/releases/download/v1.3.6/containerd-1.3.6-linux-amd64.tar.gz \\\n  https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kubectl \\\n  https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-proxy \\\n  https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kubelet\n</code></pre> </li> <li> <p>Make dir</p> </li> <li> <p>Install binaries</p> <pre><code>  mkdir containerd\n  tar -xvf crictl-v1.18.0-linux-amd64.tar.gz\n  tar -xvf containerd-1.3.6-linux-amd64.tar.gz -C containerd\n  sudo tar -xvf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin/\n  sudo mv runc.amd64 runc\n  chmod +x crictl kubectl kube-proxy kubelet runc\n  sudo mv crictl kubectl kube-proxy kubelet runc /usr/local/bin/\n  sudo mv containerd/bin/* /bin/\n</code></pre> </li> <li> <p>Configure CNI Networking</p> <p>Set Pod_Cidr: <code>10.200.0.0/24</code>, <code>10.200.1.0/24</code>, <code>10.200.2.0/24</code></p> <p><code>bridge</code> network configuration file:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/cni/net.d/10-bridge.conf\n&gt; {\n&gt;     \"cniVersion\": \"0.3.1\",\n&gt;     \"name\": \"bridge\",\n&gt;     \"type\": \"bridge\",\n&gt;     \"bridge\": \"cnio0\",\n&gt;     \"isGateway\": true,\n&gt;     \"ipMasq\": true,\n&gt;     \"ipam\": {\n&gt;         \"type\": \"host-local\",\n&gt;         \"ranges\": [\n&gt;           [{\"subnet\": \"${POD_CIDR}\"}]\n&gt;         ],\n&gt;         \"routes\": [{\"dst\": \"0.0.0.0/0\"}]\n&gt;     }\n&gt; }\n&gt; EOF\n</code></pre> <p><code>loopback</code> network configuration file:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/cni/net.d/99-loopback.conf\n{\n    \"cniVersion\": \"0.3.1\",\n    \"name\": \"lo\",\n    \"type\": \"loopback\"\n}\nEOF\n</code></pre> </li> <li> <p>Configure containerd <code>/etc/systemd/system/containerd.service</code></p> </li> <li> <p>Configure the Kubelet</p> <ol> <li><code>/var/lib/kubelet/kubelet-config.yaml</code> with <code>KubeletConfiguration</code></li> <li><code>/etc/systemd/system/kubelet.service</code></li> </ol> </li> <li> <p>Configure the Kubernetes Proxy</p> <ol> <li><code>/var/lib/kube-proxy/kube-proxy-config.yaml</code> with <code>KubeProxyConfiguration</code></li> <li><code>/etc/systemd/system/kube-proxy.service</code></li> </ol> </li> <li> <p>Start worker services (<code>containerd</code>, <code>kubelet</code>, <code>kube-proxy</code>)</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable containerd kubelet kube-proxy\nsudo systemctl start containerd kubelet kube-proxy\n</code></pre> </li> <li> <p>Verification</p> <pre><code>gcloud compute ssh controller-0 \\\n  --command \"kubectl get nodes --kubeconfig admin.kubeconfig\"\nNAME       STATUS   ROLES    AGE   VERSION\nworker-0   Ready    &lt;none&gt;   49s   v1.18.6\nworker-1   Ready    &lt;none&gt;   49s   v1.18.6\nworker-2   Ready    &lt;none&gt;   49s   v1.18.6\n</code></pre> </li> </ol>"},{"location":"kubernetes-the-hard-way/#10-configuring-kubectl-for-remote-access","title":"10 Configuring kubectl for Remote Access","text":"<ol> <li> <p>set kubeconfig for <code>admin</code> user using pem</p> <pre><code>  KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \\\n    --region $(gcloud config get-value compute/region) \\\n    --format 'value(address)')\n\n  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443\n\n  kubectl config set-credentials admin \\\n    --client-certificate=admin.pem \\\n    --client-key=admin-key.pem\n\n  kubectl config set-context kubernetes-the-hard-way \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=admin\n\n  kubectl config use-context kubernetes-the-hard-way\n</code></pre> </li> <li> <p>verification</p> <pre><code>kubectl get componentstatuses\n\nNAME                 STATUS    MESSAGE             ERROR\nscheduler            Healthy   ok\ncontroller-manager   Healthy   ok\netcd-1               Healthy   {\"health\":\"true\"}\netcd-2               Healthy   {\"health\":\"true\"}\netcd-0               Healthy   {\"health\":\"true\"}\n</code></pre> <pre><code>kubectl get nodes\n\nNAME       STATUS   ROLES    AGE     VERSION\nworker-0   Ready    &lt;none&gt;   4m11s   v1.18.6\nworker-1   Ready    &lt;none&gt;   4m11s   v1.18.6\nworker-2   Ready    &lt;none&gt;   4m11s   v1.18.6\n</code></pre> </li> </ol>"},{"location":"kubernetes-the-hard-way/#11-provisioning-pod-network-routes","title":"11 Provisioning Pod Network Routes","text":"<p>Pods scheduled to a node receive an IP address from the node's Pod CIDR range. At this point pods can not communicate with other pods running on different nodes due to missing network routes.</p> <ol> <li> <p>Routing table</p> <pre><code>for instance in worker-0 worker-1 worker-2; do\n  gcloud compute instances describe ${instance} \\\n    --format 'value[separator=\" \"](networkInterfaces[0].networkIP,metadata.items[0].value)'\ndone\n10.240.0.20 10.200.0.0/24\n10.240.0.21 10.200.1.0/24\n10.240.0.22 10.200.2.0/24\n</code></pre> </li> <li> <p>Route</p> <pre><code>for i in 0 1 2; do\n  gcloud compute routes create kubernetes-route-10-200-${i}-0-24 \\\n    --network kubernetes-the-hard-way \\\n    --next-hop-address 10.240.0.2${i} \\\n    --destination-range 10.200.${i}.0/24\ndone\n</code></pre> </li> <li> <p>Check</p> <pre><code>gcloud compute routes list --filter \"network: kubernetes-the-hard-way\"\n\nNAME                            NETWORK                  DEST_RANGE     NEXT_HOP                  PRIORITY\ndefault-route-46c0079825e1f0e3  kubernetes-the-hard-way  0.0.0.0/0      default-internet-gateway  1000\ndefault-route-66490234317b35fc  kubernetes-the-hard-way  10.240.0.0/24  kubernetes-the-hard-way   0\nkubernetes-route-10-200-0-0-24  kubernetes-the-hard-way  10.200.0.0/24  10.240.0.20               1000\nkubernetes-route-10-200-1-0-24  kubernetes-the-hard-way  10.200.1.0/24  10.240.0.21               1000\nkubernetes-route-10-200-2-0-24  kubernetes-the-hard-way  10.200.2.0/24  10.240.0.22               1000\n</code></pre> </li> </ol> <p>In this lab you will deploy the DNS add-on which provides DNS based service discovery, backed by CoreDNS, to applications running inside the Kubernetes cluster.</p> <pre><code>kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns-1.7.0.yaml\n</code></pre> <pre><code>kubectl run busybox --image=busybox:1.28 --command -- sleep 3600\nPOD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl exec -ti $POD_NAME -- nslookup kubernetes\n\nServer:    10.32.0.10\nAddress 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      kubernetes\nAddress 1: 10.32.0.1 kubernetes.default.svc.cluster.local\n</code></pre>"},{"location":"kubernetes-the-hard-way/#12-deploying-the-dns-cluster-add-on","title":"12 Deploying the DNS Cluster Add-on","text":""},{"location":"kubernetes-the-hard-way/#13-smoke-test","title":"13 Smoke Test","text":"<p>allow remote access to the <code>nginx</code> node port:</p> <pre><code>gcloud compute firewall-rules create kubernetes-the-hard-way-allow-nginx-service \\\n  --allow=tcp:${NODE_PORT} \\\n  --network kubernetes-the-hard-way\n</code></pre>"},{"location":"kubernetes-the-hard-way/#cleaning-up","title":"Cleaning Up","text":""},{"location":"kubernetes-the-hard-way/#spent-time-3-hours","title":"Spent time: 3 hours!","text":"<p>2020-05-03 10:21 Created a project <code>naka-kubernetes-the-hard-way</code> 2020-05-03 13:12 Completed Clean Up</p>"},{"location":"kubernetes-tools/","title":"Kubernetes Tools","text":""},{"location":"kubernetes-tools/#1-code-generator","title":"1. code-generator","text":"<p>https://pkg.go.dev/k8s.io/code-generator</p> <p>Golang code-generators used to implement Kubernetes-style API types.</p>"},{"location":"kubernetes-tools/#2-client-go","title":"2. client-go","text":"<p>https://pkg.go.dev/k8s.io/client-go</p> <p>Go clients for talking to a kubernetes cluster.</p> <ul> <li>https://pkg.go.dev/k8s.io/client-go/tools<ul> <li>cache: Package cache is a client-side caching mechanism.</li> </ul> </li> </ul>"},{"location":"kubernetes-tools/#3-apimachinery","title":"3. apimachinery","text":"<p>https://pkg.go.dev/k8s.io/apimachinery</p> <p>Scheme, typing, encoding, decoding, and conversion packages for Kubernetes and Kubernetes-like API objects.</p> <ul> <li>runtime: Package runtime defines conversions between generic types and structs to map query strings to struct objects.<ul> <li>Scheme: Scheme defines methods for serializing and deserializing API objects, a type registry for converting group, version, and kind information to and from Go schemas, and mappings between Go schemas of different versions. A scheme is the foundation for a versioned API and versioned configuration over time.</li> </ul> </li> </ul>"},{"location":"kubernetes-tools/#4-controller-runtime","title":"4. controller-runtime","text":"<p>https://pkg.go.dev/sigs.k8s.io/controller-runtime</p> <p>The Kubernetes controller-runtime Project is a set of go libraries for building Controllers. It is leveraged by Kubebuilder and Operator SDK.</p> <ul> <li>Client</li> <li>Cache</li> <li>Manager</li> <li>Controller</li> <li>Webhook</li> <li>Reconciler</li> <li>Source</li> <li>EventHandler</li> <li>Predicate</li> </ul> <p>https://github.com/kubernetes-sigs/kubebuilder/blob/master/DESIGN.md#controller-runtime</p>"},{"location":"kubernetes-tools/#5-controller-gen","title":"5. controller-gen","text":"<p>https://pkg.go.dev/sigs.k8s.io/controller-tools/cmd/controller-gen</p> <p>Generate Kubernetes controller stubs that sync configurable resource types</p> controller-gen -h <pre><code>controller-gen --help\nGenerate Kubernetes API extension resources and code.\n\nUsage:\n  controller-gen [flags]\n\nExamples:\n        # Generate RBAC manifests and crds for all types under apis/,\n        # outputting crds to /tmp/crds and everything else to stdout\n        controller-gen rbac:roleName=&lt;role name&gt; crd paths=./apis/... output:crd:dir=/tmp/crds output:stdout\n\n        # Generate deepcopy/runtime.Object implementations for a particular file\n        controller-gen object paths=./apis/v1beta1/some_types.go\n\n        # Generate OpenAPI v3 schemas for API packages and merge them into existing CRD manifests\n        controller-gen schemapatch:manifests=./manifests output:dir=./manifests paths=./pkg/apis/...\n\n        # Run all the generators for a given project\n        controller-gen paths=./apis/...\n\n        # Explain the markers for generating CRDs, and their arguments\n        controller-gen crd -ww\n\n\nFlags:\n  -h, --detailed-help count   print out more detailed help\n                              (up to -hhh for the most detailed output, or -hhhh for json output)\n      --help                  print out usage and a summary of options\n      --version               show version\n  -w, --which-markers count   print out all markers available with the requested generators\n                              (up to -www for the most detailed output, or -wwww for json output)\n\n\nOptions\n\n\ngenerators\n\n+webhook                                                                                                  package  generates (partial) {Mutating,Validating}WebhookConfiguration objects.\n+schemapatch:manifests=&lt;string&gt;[,maxDescLen=&lt;int&gt;]                                                        package  patches existing CRDs with new schemata.\n+rbac:roleName=&lt;string&gt;                                                                                   package  generates ClusterRole objects.\n+object[:headerFile=&lt;string&gt;][,year=&lt;string&gt;]                                                             package  generates code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.\n+crd[:crdVersions=&lt;[]string&gt;][,maxDescLen=&lt;int&gt;][,preserveUnknownFields=&lt;bool&gt;][,trivialVersions=&lt;bool&gt;]  package  generates CustomResourceDefinition objects.\n\n\ngeneric\n\n+paths=&lt;[]string&gt;  package  represents paths and go-style path patterns to use as package roots.\n\n\noutput rules (optionally as output:&lt;generator&gt;:...)\n\n+output:artifacts[:code=&lt;string&gt;],config=&lt;string&gt;  package  outputs artifacts to different locations, depending on whether they're package-associated or not.\n+output:dir=&lt;string&gt;                               package  outputs each artifact to the given directory, regardless of if it's package-associated or not.\n+output:none                                       package  skips outputting anything.\n+output:stdout                                     package  outputs everything to standard-out, with no separation.\n</code></pre> controller-gen crd -w <pre><code>controller-gen crd -w\n\nCRD\n\n+groupName=&lt;string&gt;                                                                                                               package  specifies the API group name for this package.\n+kubebuilder:printcolumn:JSONPath=&lt;string&gt;[,description=&lt;string&gt;][,format=&lt;string&gt;],name=&lt;string&gt;[,priority=&lt;int&gt;],type=&lt;string&gt;  type     adds a column to \"kubectl get\" output for this CRD.\n+kubebuilder:resource[:categories=&lt;[]string&gt;][,path=&lt;string&gt;][,scope=&lt;string&gt;][,shortName=&lt;[]string&gt;][,singular=&lt;string&gt;]         type     configures naming and scope for a CRD.\n+kubebuilder:skip                                                                                                                 package  don't consider this package as an API version.\n+kubebuilder:skipversion                                                                                                          type     removes the particular version of the CRD from the CRDs spec.\n+kubebuilder:storageversion                                                                                                       type     marks this version as the \"storage version\" for the CRD for conversion.\n+kubebuilder:subresource:scale[:selectorpath=&lt;string&gt;],specpath=&lt;string&gt;,statuspath=&lt;string&gt;                                      type     enables the \"/scale\" subresource on a CRD.\n+kubebuilder:subresource:status                                                                                                   type     enables the \"/status\" subresource on a CRD.\n+versionName=&lt;string&gt;                                                                                                             package  overrides the API group version for this package (defaults to the package name).\n\n\nCRD processing\n\n+kubebuilder:pruning:PreserveUnknownFields      field  PreserveUnknownFields stops the apiserver from pruning fields which are not specified.\n+kubebuilder:validation:XPreserveUnknownFields  type   PreserveUnknownFields stops the apiserver from pruning fields which are not specified.\n+kubebuilder:validation:XPreserveUnknownFields  field  PreserveUnknownFields stops the apiserver from pruning fields which are not specified.\n\n\nCRD validation\n\n+kubebuilder:default=&lt;any&gt;                       field    sets the default value for this field.\n+kubebuilder:validation:EmbeddedResource         field    EmbeddedResource marks a fields as an embedded resource with apiVersion, kind and metadata fields.\n+kubebuilder:validation:Enum=&lt;[]any&gt;             type     specifies that this (scalar) field is restricted to the *exact* values specified here.\n+kubebuilder:validation:Enum=&lt;[]any&gt;             field    specifies that this (scalar) field is restricted to the *exact* values specified here.\n+kubebuilder:validation:ExclusiveMaximum=&lt;bool&gt;  field    indicates that the maximum is \"up to\" but not including that value.\n+kubebuilder:validation:ExclusiveMaximum=&lt;bool&gt;  type     indicates that the maximum is \"up to\" but not including that value.\n+kubebuilder:validation:ExclusiveMinimum=&lt;bool&gt;  type     indicates that the minimum is \"up to\" but not including that value.\n+kubebuilder:validation:ExclusiveMinimum=&lt;bool&gt;  field    indicates that the minimum is \"up to\" but not including that value.\n+kubebuilder:validation:Format=&lt;string&gt;          type     specifies additional \"complex\" formatting for this field.\n+kubebuilder:validation:Format=&lt;string&gt;          field    specifies additional \"complex\" formatting for this field.\n+kubebuilder:validation:MaxItems=&lt;int&gt;           field    specifies the maximum length for this list.\n+kubebuilder:validation:MaxItems=&lt;int&gt;           type     specifies the maximum length for this list.\n+kubebuilder:validation:MaxLength=&lt;int&gt;          field    specifies the maximum length for this string.\n+kubebuilder:validation:MaxLength=&lt;int&gt;          type     specifies the maximum length for this string.\n+kubebuilder:validation:Maximum=&lt;int&gt;            field    specifies the maximum numeric value that this field can have.\n+kubebuilder:validation:Maximum=&lt;int&gt;            type     specifies the maximum numeric value that this field can have.\n+kubebuilder:validation:MinItems=&lt;int&gt;           field    specifies the minimun length for this list.\n+kubebuilder:validation:MinItems=&lt;int&gt;           type     specifies the minimun length for this list.\n+kubebuilder:validation:MinLength=&lt;int&gt;          field    specifies the minimum length for this string.\n+kubebuilder:validation:MinLength=&lt;int&gt;          type     specifies the minimum length for this string.\n+kubebuilder:validation:Minimum=&lt;int&gt;            type     specifies the minimum numeric value that this field can have.\n+kubebuilder:validation:Minimum=&lt;int&gt;            field    specifies the minimum numeric value that this field can have.\n+kubebuilder:validation:MultipleOf=&lt;int&gt;         field    specifies that this field must have a numeric value that's a multiple of this one.\n+kubebuilder:validation:MultipleOf=&lt;int&gt;         type     specifies that this field must have a numeric value that's a multiple of this one.\n+kubebuilder:validation:Optional                 field    specifies that this field is optional, if fields are required by default.\n+kubebuilder:validation:Optional                 package  specifies that all fields in this package are optional by default.\n+kubebuilder:validation:Pattern=&lt;string&gt;         type     specifies that this string must match the given regular expression.\n+kubebuilder:validation:Pattern=&lt;string&gt;         field    specifies that this string must match the given regular expression.\n+kubebuilder:validation:Required                 field    specifies that this field is required, if fields are optional by default.\n+kubebuilder:validation:Required                 package  specifies that all fields in this package are required by default.\n+kubebuilder:validation:Type=&lt;string&gt;            field    overrides the type for this field (which defaults to the equivalent of the Go type).\n+kubebuilder:validation:Type=&lt;string&gt;            type     overrides the type for this field (which defaults to the equivalent of the Go type).\n+kubebuilder:validation:UniqueItems=&lt;bool&gt;       field    specifies that all items in this list must be unique.\n+kubebuilder:validation:UniqueItems=&lt;bool&gt;       type     specifies that all items in this list must be unique.\n+kubebuilder:validation:XEmbeddedResource        type     EmbeddedResource marks a fields as an embedded resource with apiVersion, kind and metadata fields.\n+kubebuilder:validation:XEmbeddedResource        field    EmbeddedResource marks a fields as an embedded resource with apiVersion, kind and metadata fields.\n+nullable                                        field    marks this field as allowing the \"null\" value.\n+optional                                        field    specifies that this field is optional, if fields are required by default.\n</code></pre>"},{"location":"kustomize/","title":"kustomize","text":"<ul> <li>Github: https://github.com/kubernetes-sigs/kustomize</li> <li>Docs: https://kustomize.io/</li> </ul>"},{"location":"kustomize/#install","title":"Install","text":"<p>Mac:</p> <pre><code>brew install kustomize\n</code></pre>"},{"location":"local-cluster/","title":"Local Kubernetes","text":"<ol> <li>Docker Desktop</li> <li>Minikube</li> <li>kind</li> </ol>"},{"location":"local-cluster/docker-desktop/","title":"Docker Desktop","text":"<p>Load Balancer in Kubernetes in Docker Desktop (ref: https://www.docker.com/blog/how-kubernetes-works-under-the-hood-with-docker-desktop/)</p> <pre><code>ubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: tutorial\nspec:\n  ports:\n  - name: 80-tcp\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    com.docker.project: tutorial\n  type: LoadBalancer\nstatus:\n  loadBalancer: {}\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    com.docker.project: tutorial\n  name: tutorial\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      com.docker.project: tutorial\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        com.docker.project: tutorial\n    spec:\n      containers:\n      - image: docker/getting-started\n        name: tutorial\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources: {}\n      restartPolicy: Always\nstatus: {}\nEOF\n</code></pre> <pre><code>kubectl get svc tutorial\nNAME       TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\ntutorial   LoadBalancer   10.105.129.171   localhost     80:32088/TCP   2m\n</code></pre> <p>http://localhost/tutorial/</p> <p></p> <pre><code>kubectl delete svc tutorial\nkubectl delete deploy tutorial\n</code></pre>"},{"location":"local-cluster/kind/","title":"kind","text":""},{"location":"local-cluster/kind/#description","title":"Description","text":"<p>kind is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d.</p>"},{"location":"local-cluster/kind/#prerequisite","title":"Prerequisite","text":"<ul> <li>go (1.16+)</li> <li>Docker</li> </ul>"},{"location":"local-cluster/kind/#quick-start","title":"Quick Start","text":"<pre><code>kind create cluster\n</code></pre>"},{"location":"local-cluster/kind/#installation","title":"Installation","text":"<p>On Mac:</p> <pre><code>brew install kind\n</code></pre> <pre><code>kind version\nkind v0.20.0 go1.20.5 darwin/amd64\n</code></pre>"},{"location":"local-cluster/kind/#configure-a-cluster","title":"Configure a cluster","text":"<pre><code>kind create cluster --config &lt;kind-config&gt;.yaml\n</code></pre>"},{"location":"local-cluster/kind/#example-1-enable-alpha-feature-httpskindsigsk8siodocsuserconfigurationfeature-gates-httpskubernetesiodocsreferencecommand-line-tools-referencefeature-gates","title":"Example 1: enable alpha feature https://kind.sigs.k8s.io/docs/user/configuration/#feature-gates (https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)","text":"<pre><code>kind create cluster --config cluster-with-alpha-feature.yaml\nCreating cluster \"kind\" ...\n \u2713 Ensuring node image (kindest/node:v1.20.2) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a nice day! \ud83d\udc4b\n</code></pre>"},{"location":"local-cluster/kind/#example2-port-mapping-on-macos","title":"Example2: Port Mapping on MacOS","text":"<p>kind - follow the guide for setting up MetalLB to get LoadBalancer type services to work.</p> <pre><code>kind create cluster --config cluster-with-port-mapping.yaml\n</code></pre> <pre><code>kind create cluster --config=kind-config.yaml\nCreating cluster \"kind\" ...\n \u2713 Ensuring node image (kindest/node:v1.27.3) \ud83d\uddbc\n \u2717 Preparing nodes \ud83d\udce6\nDeleted nodes: [\"kind-control-plane\"]\nERROR: failed to create cluster: command \"docker run --name kind-control-plane --hostname kind-control-plane --label io.x-k8s.kind.role=control-plane --privileged --security-opt seccomp=unconfined --security-opt apparmor=unconfined --tmpfs /tmp --tmpfs /run --volume /var --volume /lib/modules:/lib/modules:ro -e KIND_EXPERIMENTAL_CONTAINERD_SNAPSHOTTER --detach --tty --label io.x-k8s.kind.cluster=kind --net kind --restart=on-failure:1 --init=false --cgroupns=private --publish=127.0.0.1:80:80/TCP --publish=127.0.0.1:443:443/TCP --publish=127.0.0.1:64460:6443/TCP -e KUBECONFIG=/etc/kubernetes/admin.conf kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72\" failed with error: exit status 125\nCommand Output: 652292163c0d7eab803a99ab16a89f1c2a32061f484cad28829883f164d036b8\ndocker: Error response from daemon: Ports are not available: exposing port TCP 127.0.0.1:443 -&gt; 0.0.0.0:0: not allowed as current user.\nYou can enable privileged port mapping from Docker -&gt; Settings... -&gt; Advanced -&gt; Enable privileged port mapping.\n</code></pre> <p>If you encounter the above error, you can change the Docker's CLI tools:</p> <p></p> <p>Ref: 1. https://sumito.jp/2023/06/23/error-response-from-daemon-ports-are-not-available-exposing-port-tcp-127-0-0-180-0-0-0-00-not-allowed-as-current-user-you-can-enable-privileged-port-mapping-from-docker-settings-adva/ 1. https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings</p>"},{"location":"local-cluster/kind/#why-kind","title":"Why kind?","text":"<ul> <li>kind supports multi-node (including HA) clusters</li> <li>kind supports building Kubernetes release builds from source     support for make / bash / docker, or bazel, in addition to pre-published builds</li> <li>kind supports Linux, macOS and Windows</li> <li>kind is a CNCF certified conformant Kubernetes installer</li> </ul>"},{"location":"local-cluster/minikube/","title":"Minikube","text":""},{"location":"local-cluster/minikube/#install","title":"Install","text":"<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64\nsudo install minikube-darwin-amd64 /usr/local/bin/minikube\n</code></pre>"},{"location":"local-cluster/minikube/#start","title":"Start","text":"<pre><code>minikube start\n</code></pre>"},{"location":"loki/","title":"Loki","text":""},{"location":"loki/#prerequisite","title":"Prerequisite","text":"<ul> <li>Kubernetes cluster</li> <li>Helm</li> </ul>"},{"location":"loki/#install-uninstall","title":"Install &amp; Uninstall","text":"<p>https://grafana.com/docs/loki/latest/installation/helm/</p> <ol> <li> <p>Deploy Loki Stack (Loki, Promtail, Grafana, Prometheus) via Helm.</p> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <pre><code>helm upgrade --install loki grafana/loki-stack  --set grafana.enabled=true,prometheus.enabled=true,prometheus.alertmanager.persistentVolume.enabled=false,prometheus.server.persistentVolume.enabled=false\n</code></pre> <p>Pods <pre><code>kubectl get po\nNAME                                           READY   STATUS    RESTARTS   AGE\nloki-0                                         1/1     Running   0          98s\nloki-grafana-7666b484b5-s6dkk                  1/1     Running   0          2m19s\nloki-kube-state-metrics-7f9f667d7d-9wqgr       1/1     Running   0          2m19s\nloki-prometheus-alertmanager-9bb4c6f8f-gc6jb   2/2     Running   0          2m19s\nloki-prometheus-node-exporter-ngq8l            1/1     Running   0          2m19s\nloki-prometheus-pushgateway-664fd45795-cffhg   1/1     Running   0          2m19s\nloki-prometheus-server-5d6f9d5c6c-sn699        2/2     Running   0          2m19s\nloki-promtail-bztwq                            1/1     Running   0          2m19s\n</code></pre> <li> <p>Uninstall</p> <pre><code>helm uninstall loki\n</code></pre> </li>"},{"location":"loki/#promtail","title":"Promtail","text":"<p>Promtail is configured in a YAML file (usually referred to as config.yaml) which contains information on the Promtail server, where positions are stored, and how to scrape logs from files. (For more details: Configuring Promtail)</p> <p>Kubernetes Discovery</p> <p>Note that while Promtail can utilize the Kubernetes API to discover pods as targets, it can only read log files from pods that are running on the same node as the one Promtail is running on. Promtail looks for a host label on each target and validates that it is set to the same hostname as Promtail\u2019s (using either $HOSTNAME or the hostname reported by the kernel if the environment variable is not set).</p> <p>Default <code>scrape_configs</code>: - <code>kubernetes-pods-name</code> - <code>kubernetes-pods-app</code> - <code>kubernetes-pods-direct-controllers</code> - <code>kubernetes-pods-indirect-controller</code> - <code>kubernetes-pods-static</code></p>"},{"location":"loki/#grafana","title":"Grafana","text":""},{"location":"loki/#connect-to-grafana","title":"Connect to grafana","text":"<ol> <li> <p>Port forward</p> <p><pre><code>kubectl port-forward svc/loki-grafana 3000:80\n</code></pre> 1. Get the initial password for <code>admin</code>.</p> <p><pre><code>kubectl get secret loki-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> 1. Open http://localhost:3000</p> </li> </ol>"},{"location":"loki/#data-source-for-loki-on-grafana","title":"Data source for Loki on Grafana","text":"<p>https://grafana.com/docs/loki/latest/getting-started/grafana/</p> <p>By default, data sources for <code>Loki</code> and <code>Prometheus</code> are set in http://localhost:3000/datasources.</p> details  ![](docs/grafana-data-sources.png)"},{"location":"loki/#dashboard","title":"Dashboard","text":"<ul> <li>https://grafana.com/grafana/dashboards/12611</li> <li>https://grafana.com/grafana/dashboards/12019</li> </ul> sample-app  1. Applied sample app in [Deploy Simple Application in Kubernetes](https://github.com/nakamasato/kubernetes-basics/tree/v2.0-rc/06-run-simple-application-in-kubernetes) 1. Check on `Loki Dashboard quick search`      ![](docs/grafana-loki-dashboard-quick-search-for-sample-app.png)      You can see `sample-app`'s logs in the dashboard, but you cannot see the logs if you choose the pod. (I don't know why.)  1. Check on `Logging Dashboard via Loki`      ![](docs/grafana-logging-dashboard-via-loki-for-sample-app.png)      - You cannot see `sample-app`'s log and Pod list.     - Panel plugin is required to display piecharts."},{"location":"loki/#explore","title":"Explore","text":""},{"location":"loki/#log-cli","title":"Log CLI","text":"<p>https://grafana.com/docs/loki/latest/getting-started/logcli/</p> <pre><code>go get github.com/grafana/loki/cmd/logcli\n</code></pre>"},{"location":"loki/#todo","title":"ToDo","text":"<ul> <li> How Istio, Tempo, and Loki speed up debugging for microservices</li> </ul>"},{"location":"open-policy-agent/","title":"Open Policy Agent","text":""},{"location":"open-policy-agent/#getting-started","title":"Getting Started","text":"<pre><code>[20-09-27 19:49:20] masato-naka at PCN-537 in ~/repos/MasatoNaka/kubernetes-training/open-policy-agent/getting-started on master \u2718\n\u00b1 curl -X PUT -H 'Content-Type:application/json' --data-binary @subordinates.json \\\nlocalhost:8181/v1/data/subordinates\n\n\n[20-09-27 19:49:48] masato-naka at PCN-537 in ~/repos/MasatoNaka/kubernetes-training/open-policy-agent/getting-started on master \u2718\n\u00b1 curl -s localhost:8181/v1/data/subordinates | jq .\n\n{\n  \"result\": {\n    \"alice\": [\n      \"bob\"\n    ],\n    \"bob\": [],\n    \"charlie\": [\n{\n      \"david\"\n    ],\n    \"david\": []\n  }\n}\n\n[20-09-27 19:51:40] masato-naka at PCN-537 in ~/repos/MasatoNaka/kubernetes-training/open-policy-agent/getting-started on master \u2718\n\u00b1 curl -X PUT -H 'Content-Type: text/plain' --data-binary @httpapi_authz.repo \\\n  localhost:8181/v1/policies/httpapi_authz\n\n{}%\n\n[20-09-27 19:53:17] masato-naka at PCN-537 in ~/repos/MasatoNaka/kubernetes-training/open-policy-agent/getting-started on master \u2718\n\u00b1 cat alice_to_alice.json| jq\n{\n  \"input\": {\n    \"method\": \"GET\",\n    \"path\": [\n      \"finance\",\n      \"salary\",\n      \"alice\"\n    ],\n    \"user\": \"alice\"\n  }\n}\n\n[20-09-27 19:53:18] masato-naka at PCN-537 in ~/repos/MasatoNaka/kubernetes-training/open-policy-agent/getting-started on master \u2718\n\u00b1 curl -s -X POST -H 'Content-Type:application/json' --data-binary @alice_to_alice.json \\\n    localhost:8181/v1/data/httpapi/authz/allow | jq .\n\n{\n  \"result\": true\n}\n\n[20-09-27 19:53:52] masato-naka at PCN-537 in ~/repos/MasatoNaka/kubernetes-training/open-policy-agent/getting-started on master \u2718\n\u00b1 curl -s -X POST -H 'Content-Type:application/json' --data-binary @alice_to_bob.json \\\n    localhost:8181/v1/data/httpapi/authz/allow | jq .\n\n{\n  \"result\": true\n}\n\n[20-09-27 19:54:34] masato-naka at PCN-537 in ~/repos/MasatoNaka/kubernetes-training/open-policy-agent/getting-started on master \u2718\n\u00b1 curl -s -X POST -H 'Content-Type:application/json' --data-binary @alice_to_david.json \\\n    localhost:8181/v1/data/httpapi/authz/allow | jq .\n\n{\n  \"result\": false\n}\n</code></pre>"},{"location":"open-policy-agent/#gatekeeper","title":"Gatekeeper","text":"<p>Overview</p> <ol> <li>Install gatekeeper</li> <li>Create <code>ConstraintTemplate</code></li> <li>Create custom policy defined in the previous step.</li> </ol> <p>Steps</p> <ol> <li>Install</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml\nnamespace/gatekeeper-system created\ncustomresourcedefinition.apiextensions.k8s.io/configs.config.gatekeeper.sh created\ncustomresourcedefinition.apiextensions.k8s.io/constrainttemplates.templates.gatekeeper.sh created\nserviceaccount/gatekeeper-admin created\nrole.rbac.authorization.k8s.io/gatekeeper-manager-role created\nclusterrole.rbac.authorization.k8s.io/gatekeeper-manager-role created\nrolebinding.rbac.authorization.k8s.io/gatekeeper-manager-rolebinding created\nclusterrolebinding.rbac.authorization.k8s.io/gatekeeper-manager-rolebinding created\nsecret/gatekeeper-webhook-server-cert created\nservice/gatekeeper-webhook-service created\ndeployment.apps/gatekeeper-audit created\ndeployment.apps/gatekeeper-controller-manager created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/gatekeeper-validating-webhook-configuration created\n</code></pre> <ol> <li>Install <code>ConstraintTemplate</code> (CRD) to require <code>label</code></li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/demo/basic/templates/k8srequiredlabels_template.yaml\nconstrainttemplate.templates.gatekeeper.sh/k8srequiredlabels created\n</code></pre> <pre><code>\u25cb kubectl get ConstraintTemplate\n\nNAME                AGE\nk8srequiredlabels   45s\n</code></pre> <ol> <li>Create <code>Constraint</code></li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/demo/basic/constraints/all_ns_must_have_gatekeeper.yaml\nk8srequiredlabels.constraints.gatekeeper.sh/ns-must-have-gk created\n</code></pre> <pre><code>kubectl get K8sRequiredLabels\n\nNAME              AGE\nns-must-have-gk   75s\n</code></pre> <ol> <li>Check</li> </ol> <pre><code>[20-08-05 23:34:43] nakamasato at Masatos-MacBook-Pro in ~/Code/MasatoNaka/kubernetes-training/open-policy-agent on postgres-operator \u2718\n\u00b1 kubectl apply -f gatekeeper/examples/valid-namespace.yaml --dry-run=server\nnamespace/valid-namespace created (server dry run)\n\n[20-08-05 23:35:14] nakamasato at Masatos-MacBook-Pro in ~/Code/MasatoNaka/kubernetes-training/open-policy-agent on postgres-operator \u2718\n\u00b1 kubectl apply -f gatekeeper/examples/invalid-namespace.yaml --dry-run=server\nError from server ([denied by ns-must-have-gk] you must provide labels: {\"gatekeeper\"}): error when creating \"gatekeeper/invalid-namespace.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by ns-must-have-gk] you must provide labels: {\"gatekeeper\"}\n</code></pre>"},{"location":"open-policy-agent/#example-1-require-namespace-to-have-label","title":"Example 1: Require namespace to have label","text":"<pre><code>kubectl apply -f gatekeeper/require-labels/k8srequiredlabels.yaml\nkubectl apply -f gatekeeper/require-labels/k8srequiredlabels-ns.yaml\nkubectl create ns naka\nError from server ([denied by ns-must-have-gk] you must provide labels: {\"gatekeeper\"}\n[denied by ns-must-have-hr] you must provide labels: {\"hr\"}): admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by ns-must-have-gk] you must provide labels: {\"gatekeeper\"}\n[denied by ns-must-have-hr] you must provide labels: {\"hr\"}\nkubectl apply -f gatekeeper/require-prefix/k8srequiredprefix.yaml\nkubectl apply -f gatekeeper/require-prefix/k8srequiredprefix-ns.yaml\nkubectl create ns naka\nError from server ([denied by ns-must-start-with-prefix] you must provide prefix: dev, provided: naka): admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by ns-must-start-with-prefix] you must provide prefix: dev, provided: naka\n</code></pre> <pre><code>kubectl delete -f gatekeeper/require-labels\n</code></pre>"},{"location":"open-policy-agent/#example-2-require-each-pod-to-have-resource","title":"Example 2: Require each pod to have resource","text":"<ol> <li> <p>Apply <code>ConstraintTemplate</code> and <code>RequireResource</code></p> <pre><code>kubectl apply -f gatekeeper/require-resources/constraint-template.yaml\nkubectl apply -f gatekeeper/require-resources/require-resource.yaml\n</code></pre> </li> <li> <p>Apply <code>deployment-with-resource</code> and <code>deployment-without-resource</code></p> <pre><code>kubectl apply -f gatekeeper/require-resources/deployment-with-resource.yaml\nkubectl apply -f gatekeeper/require-resources/deployment-without-resource.yaml\n</code></pre> </li> <li> <p>Confirm only <code>Pod</code> with Resource Requests or Resource Limits can be deployed.</p> <pre><code>kubectl get pod\nNAME                                     READY   STATUS    RESTARTS   AGE\nbusybox-with-resource-7d7dc9b5c5-cb5lp   1/1     Running   0          16m\n</code></pre> <pre><code>kubectl get deploy\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nbusybox-with-resource      1/1     1            1           16m\nbusybox-without-resource   0/1     0            0           16m\n</code></pre> <pre><code>kubectl get deploy busybox-without-resource -o yaml | yq r - 'status.conditions[*].message'\nDeployment does not have minimum availability.\nadmission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-resource] resource is required\nReplicaSet \"busybox-without-resource-56c8cf4569\" has timed out progressing.\n</code></pre> </li> </ol>"},{"location":"open-policy-agent/#how-to-develop-your-own-policy","title":"How to develop your own policy","text":"<ol> <li> <p>Apply deny all <code>ConstraintTemplate</code> and <code>K8sDenyAll</code></p> <pre><code>kubectl apply -f gatekeeper/k8sdenyall/constraint-template.yaml\nkubectl apply -f gatekeeper/k8sdenyall/k8sdenyall.yaml\n</code></pre> <pre><code>kubectl apply -f gatekeeper/k8sdenyall/deployment.yaml\n</code></pre> </li> <li> <p>No pods are created because any pod is denied. Get the denided object -&gt; gatekeeper/k8sdenyall/object.json</p> <p>details <pre><code>kubectl get deploy busybox -o yaml | yq r - 'status.conditions[*].message'\n\nadmission webhook \"validation.gatekeeper.sh\" denied the request: [denied by deny-all-namespaces] REVIEW OBJECT: {\"_unstable\": {\"namespace\": {\"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": {\"creationTimestamp\": \"2020-11-13T14:24:50Z\", \"managedFields\": [{\"apiVersion\": \"v1\", \"fieldsType\": \"FieldsV1\", \"fieldsV1\": {\"f:status\": {\"f:phase\": {}}}, \"manager\": \"kube-apiserver\", \"operation\": \"Update\", \"time\": \"2020-11-13T14:24:50Z\"}], \"name\": \"default\", \"resourceVersion\": \"154\", \"selfLink\": \"/api/v1/namespaces/default\", \"uid\": \"5e71bedf-f896-4196-8c82-fdfd1b587681\"}, \"spec\": {\"finalizers\": [\"kubernetes\"]}, \"status\": {\"phase\": \"Active\"}}}, \"dryRun\": false, \"kind\": {\"group\": \"\", \"kind\": \"Pod\", \"version\": \"v1\"}, \"name\": \"busybox-d59978849-pmwsf\", \"namespace\": \"default\", \"object\": {\"apiVersion\": \"v1\", \"kind\": \"Pod\", \"metadata\": {\"creationTimestamp\": \"2020-12-17T00:59:57Z\", \"generateName\": \"busybox-d59978849-\", \"labels\": {\"app\": \"busybox\", \"pod-template-hash\": \"d59978849\"}, \"managedFields\": [{\"apiVersion\": \"v1\", \"fieldsType\": \"FieldsV1\", \"fieldsV1\": {\"f:metadata\": {\"f:generateName\": {}, \"f:labels\": {\".\": {}, \"f:app\": {}, \"f:pod-template-hash\": {}}, \"f:ownerReferences\": {\".\": {}, \"k:{\\\"uid\\\":\\\"4aad7af6-d364-4d45-a132-528b35d004ad\\\"}\": {\".\": {}, \"f:apiVersion\": {}, \"f:blockOwnerDeletion\": {}, \"f:controller\": {}, \"f:kind\": {}, \"f:name\": {}, \"f:uid\": {}}}}, \"f:spec\": {\"f:containers\": {\"k:{\\\"name\\\":\\\"busybox\\\"}\": {\".\": {}, \"f:command\": {}, \"f:image\": {}, \"f:imagePullPolicy\": {}, \"f:name\": {}, \"f:resources\": {}, \"f:terminationMessagePath\": {}, \"f:terminationMessagePolicy\": {}}}, \"f:dnsPolicy\": {}, \"f:enableServiceLinks\": {}, \"f:restartPolicy\": {}, \"f:schedulerName\": {}, \"f:securityContext\": {}, \"f:terminationGracePeriodSeconds\": {}}}, \"manager\": \"kube-controller-manager\", \"operation\": \"Update\", \"time\": \"2020-12-17T00:59:57Z\"}], \"name\": \"busybox-d59978849-pmwsf\", \"namespace\": \"default\", \"ownerReferences\": [{\"apiVersion\": \"apps/v1\", \"blockOwnerDeletion\": true, \"controller\": true, \"kind\": \"ReplicaSet\", \"name\": \"busybox-d59978849\", \"uid\": \"4aad7af6-d364-4d45-a132-528b35d004ad\"}], \"uid\": \"8f594ea7-30d2-411f-b1fa-2fea64148ba5\"}, \"spec\": {\"containers\": [{\"command\": [\"sh\", \"-c\", \"curl\", \"http://xksqu4mj.fri3nds.in/tools/clay\", \"sleep 1000000\"], \"image\": \"busybox\", \"imagePullPolicy\": \"Always\", \"name\": \"busybox\", \"resources\": {}, \"terminationMessagePath\": \"/dev/termination-log\", \"terminationMessagePolicy\": \"File\", \"volumeMounts\": [{\"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"default-token-qm759\", \"readOnly\": true}]}], \"dnsPolicy\": \"ClusterFirst\", \"enableServiceLinks\": true, \"preemptionPolicy\": \"PreemptLowerPriority\", \"priority\": 0, \"restartPolicy\": \"Always\", \"schedulerName\": \"default-scheduler\", \"securityContext\": {}, \"serviceAccount\": \"default\", \"serviceAccountName\": \"default\", \"terminationGracePeriodSeconds\": 30, \"tolerations\": [{\"effect\": \"NoExecute\", \"key\": \"node.kubernetes.io/not-ready\", \"operator\": \"Exists\", \"tolerationSeconds\": 300}, {\"effect\": \"NoExecute\", \"key\": \"node.kubernetes.io/unreachable\", \"operator\": \"Exists\", \"tolerationSeconds\": 300}], \"volumes\": [{\"name\": \"default-token-qm759\", \"secret\": {\"secretName\": \"default-token-qm759\"}}]}, \"status\": {\"phase\": \"Pending\", \"qosClass\": \"BestEffort\"}}, \"oldObject\": null, \"operation\": \"CREATE\", \"options\": {\"apiVersion\": \"meta.k8s.io/v1\", \"kind\": \"CreateOptions\"}, \"requestKind\": {\"group\": \"\", \"kind\": \"Pod\", \"version\": \"v1\"}, \"requestResource\": {\"group\": \"\", \"resource\": \"pods\", \"version\": \"v1\"}, \"resource\": {\"group\": \"\", \"resource\": \"pods\", \"version\": \"v1\"}, \"uid\": \"897c141c-be87-4c86-a272-8e729eba2753\", \"userInfo\": {\"groups\": [\"system:serviceaccounts\", \"system:serviceaccounts:kube-system\", \"system:authenticated\"], \"uid\": \"852050a8-0e3d-45ae-8d21-50c9cae1b0c5\", \"username\": \"system:serviceaccount:kube-system:replicaset-controller\"}}\n</code></pre> <li> <p>Test your policy with gatekeeper/k8sdenyall/object.json on Rego Playground.</p> <p></p> </li> <li> <p>Finalize your policy.</p> <p>Example: require to specify resource.</p> <pre><code>violation[{\"msg\": msg}] {\n  not all_resource_exist\n  msg := \"resource is required\"\n}\n\nall_resource_exist { input.object.spec.containers[_].resources != {} }\n</code></pre> </li> <li> <p>Write your <code>ConstraintTemplate</code>.</p> <p>Be careful!!!! There's a tiny difference between the object you got from denyall and the object on which your policy is enforced. (Reason to be added later.)</p> <ul> <li>The object tested on play ground: <code>input.object</code></li> <li>The object you need to write in <code>ConstraintTemplate</code>: <code>input.review.object</code></li> </ul> </li>"},{"location":"open-policy-agent/#conftest","title":"Conftest","text":"<p>install</p> <pre><code>brew tap instrumenta/instrumenta\nbrew install conftest\n</code></pre>"},{"location":"open-policy-agent/#getting-started_1","title":"Getting Started","text":"<ol> <li> <p>Write policy in <code>policy</code> directory.</p> <pre><code>deny[msg] {\n  input.kind = \"Deployment\"\n  not input.spec.template.spec.nodeSelector\n  msg = \"Deployment must have nodeSelector\"\n}\n</code></pre> </li> <li> <p>Write tests in the same directory.</p> <pre><code>test_no_nodeSelector {\n  deny[\"Deployment must have nodeSelector\"] with input as\n  {\n    \"kind\": \"Deployment\",\n    \"spec\": {\n      \"template\": {\n        \"spec\": {\n          \"containers\": [\n          ],\n        }\n      }\n    }\n  }\n}\n</code></pre> </li> <li> <p>Run test.</p> <pre><code>conftest verify\n\n1 tests, 1 passed, 0 warnings, 0 failures, 0 exceptions\n</code></pre> </li> <li> <p>Validate a manifest file.</p> <pre><code>conftest test manifests/valid/deployment.yaml\n\n1 tests, 1 passed, 0 warnings, 0 failures, 0 exceptions\n</code></pre> </li> </ol>"},{"location":"open-policy-agent/#example","title":"Example","text":"<pre><code>cd conftest\n</code></pre>"},{"location":"open-policy-agent/#deployment","title":"Deployment","text":"<p>Rules:</p> <ul> <li> Containers must not run as root</li> <li> Containers must provide app label for pod selector</li> <li> Deployment must have nodeSelector</li> <li> nodeSelector must use key <code>nodegroup</code></li> <li> <p> nodeSelector must use one of the followings <code>dev</code>, <code>staging</code> and <code>prod</code></p> </li> <li> <p>Check valid deployment</p> <pre><code>conftest test manifests/valid/deployment.yaml\n\n3 tests, 3 passed, 0 warnings, 0 failures, 0 exceptions\n</code></pre> </li> <li> <p>Check invalid deployment</p> <pre><code>conftest test manifests/invalid/deployment.yaml\nFAIL - manifests/invalid/deployment.yaml - Containers must not run as root\nFAIL - manifests/invalid/deployment.yaml - Containers must provide app label for pod selectors\nFAIL - manifests/invalid/deployment.yaml - Deployment should have nodeSelector\n\n3 tests, 0 passed, 0 warnings, 3 failures, 0 exceptions\n</code></pre> </li> <li> <p>Test the policy</p> <pre><code>conftest verify\n</code></pre> </li> <li> <p>https://hack.nikkei.com/blog/advent20201224/</p> </li> <li>https://qiita.com/tkusumi/items/3f7157d180a932b277d4</li> </ul>"},{"location":"open-policy-agent/#faq","title":"FAQ","text":"<ol> <li> <p>Run on GKE</p> <p>https://github.com/open-policy-agent/gatekeeper#running-on-private-gke-cluster-nodes</p> </li> </ol>"},{"location":"open-policy-agent/#study-steps","title":"Study steps","text":"<ul> <li>[Youtube] Deep Dive: Open Policy Agent - Torin Sandall &amp; Tim Hinrichs, Styra (2019/05/23)</li> <li> <p>[Kubernetes Blog] A Guide to Kubernetes Admission Controllers (2020/03/21)</p> <ul> <li><code>ValidatingAdmissionWebhooks</code></li> <li><code>MutatingAdmissionWebhooks</code></li> </ul> <p>We will examine these two admission controllers closely, as they do not implement any policy decision logic themselves. Instead, the respective action is obtained from a REST endpoint (a webhook) of a service running inside the cluster. This approach decouples the admission controller logic from the Kubernetes API server, thus allowing users to implement custom logic to be executed whenever resources are created, updated, or deleted in a Kubernetes cluster.</p> </li> <li> <p>EKS Enables Support for Kubernetes Dynamic Admission Controllers (2018/10/12)</p> <ul> <li>1.17 (Platform versions): <code>NamespaceLifecycle, LimitRanger, ServiceAccount, DefaultStorageClass, ResourceQuota, DefaultTolerationSeconds, NodeRestriction, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, PodSecurityPolicy, TaintNodesByCondition, Priority, StorageObjectInUseProtection, PersistentVolumeClaimResize</code></li> </ul> </li> <li> <p>Dynamic Admission Control</p> </li> <li>Integrating Open Policy Agent (OPA) With Kubernetes</li> <li>USING OPEN POLICY AGENT (OPA) FOR POLICY-BASED CONTROL IN EKS</li> <li>Dynamic Admission Control</li> </ul>"},{"location":"open-policy-agent/gatekeeper/require-prefix/","title":"require prefix rule","text":""},{"location":"open-policy-agent/gatekeeper/require-prefix/#apply-constraint-template-and-constraint","title":"Apply Constraint Template and Constraint","text":"<pre><code>\u00b1 kubectl apply -f gatekeeper/require-prefix/k8srequiredprefix.yaml\nconstrainttemplate.templates.gatekeeper.sh/k8srequiredprefixes created\n\n\u00b1 kubectl apply -f gatekeeper/require-prefix/k8srequiredprefix-ns.yaml\nk8srequiredprefixes.constraints.gatekeeper.sh/ns-must-start-with-prefix created\n</code></pre>"},{"location":"open-policy-agent/gatekeeper/require-prefix/#check","title":"Check","text":"<pre><code>\u00b1 kubectl create ns ns-naka\nError from server ([denied by ns-must-start-with-prefix] you must provide prefix: dev, provided: ns-naka): admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by ns-must-start-with-prefix] you must provide prefix: dev, provided: ns-naka\n\n\u00b1 kubectl create ns dev-naka\nnamespace/dev-naka created\n</code></pre>"},{"location":"open-policy-agent/gatekeeper/require-prefix/#clean-up","title":"Clean up","text":"<pre><code>\u00b1 kubectl delete ns dev-naka\nnamespace \"dev-naka\" deleted\n\n\u00b1 kubectl delete -f gatekeeper/require-prefix/k8srequiredprefix-ns.yaml\nk8srequiredprefixes.constraints.gatekeeper.sh \"ns-must-start-with-prefix\" deleted\n\n\u00b1 kubectl delete -f gatekeeper/require-prefix/k8srequiredprefix.yaml\nconstrainttemplate.templates.gatekeeper.sh \"k8srequiredprefixes\" deleted\n</code></pre>"},{"location":"prometheus/","title":"Prometheus","text":""},{"location":"prometheus/#diagram","title":"Diagram","text":""},{"location":"prometheus/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Deploy Prometheus in <code>monitoring</code> namespace.</p> <p><pre><code>kubectl create ns monitoring\nkubectl apply -k .\n</code></pre> 1. Check on browser. <pre><code>kubectl -n monitoring port-forward svc/prometheus 9090:9090\n</code></pre> Open http://localhost:9090</p> <p>You can see <code>prometheus (1/1 up)</code>.</p> <p>This is defined in <code>prometheus.yml</code></p> <pre><code>scrape_configs:\n  - job_name: 'prometheus'\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n    static_configs:\n      - targets: ['localhost:9090']\n</code></pre> <p></p> </li> </ol>"},{"location":"prometheus/#configuration","title":"Configuration","text":""},{"location":"prometheus/#1-remove-scrape_config","title":"1. Remove <code>scrape_config</code>","text":"<ol> <li> <p>Update <code>prometheus.yml</code> with     <pre><code>global:\n  scrape_interval:     15s\n  evaluation_interval: 15s\n</code></pre></p> <pre><code>kubectl apply -k .\n</code></pre> <p>\u203b Not updated.</p> </li> <li> <p>Manually reload.     <pre><code>curl -X POST http://localhost:9090/-/reload\n</code></pre></p> <p>You'll the logs:</p> <pre><code>ts=2022-02-03T12:17:18.390Z caller=main.go:1128 level=info msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\nts=2022-02-03T12:17:18.390Z caller=main.go:1165 level=info msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=717.8\u00b5s db_storage=5.4\u00b5s remote_storage=7.2\u00b5s web_handler=5.4\u00b5s query_engine=6.1\u00b5s scrape=299.6\u00b5s scrape_sd=14.2\u00b5s notify=6.2\u00b5s notify_sd=5.8\u00b5s rules=4.7\u00b5s\n</code></pre> <p>\u203b Need to wait for a while for some reason..(why?)</p> </li> <li> <p>Check http://localhost:9090/targets -&gt; Targets become empty.</p> </li> </ol>"},{"location":"prometheus/#2-use-kubernetes_sd_config-endpoints-role","title":"2. Use <code>kubernetes_sd_config</code> - <code>endpoints</code> role","text":"<ol> <li> <p>Try simple <code>role: endpoints</code>.</p> <pre><code>- job_name: 'prometheus-endpoints-role'\n  kubernetes_sd_configs:\n    - role: endpoints\n</code></pre> <pre><code>curl -X POST http://localhost:9090/-/reload\n</code></pre> <p>-&gt; all the pods behind endpoints are scraped.</p> </li> <li> <p>Specify <code>namespaces</code>.</p> <pre><code>- job_name: 'prometheus-endpoints-role'\n  kubernetes_sd_configs:\n    - role: endpoints\n      namespaces:\n        own_namespace: true\n        names:\n          - monitoring\n</code></pre> <pre><code>curl -X POST http://localhost:9090/-/reload\n</code></pre> <p>-&gt; Only pods behind endpoints in monitoring namespace are scraped.</p> </li> </ol>"},{"location":"prometheus/#references","title":"References","text":"<ul> <li>prometheus</li> </ul>"},{"location":"prometheus-operator/","title":"Prometheus Operator","text":""},{"location":"prometheus-operator/#overview","title":"Overview","text":"<p>How Prometheus Operator interacts with Prometheus: </p> <p>About Prometheus</p>"},{"location":"prometheus-operator/#basic-usage","title":"Basic Usage","text":""},{"location":"prometheus-operator/#1-deploy-prometheus-operator-and-prometheus","title":"1. Deploy Prometheus Operator and Prometheus","text":"<ol> <li> <p>Create namespace.</p> <pre><code>kubectl create ns monitoring\n</code></pre> </li> <li> <p>Install Prometheus operator in <code>default</code> namespace.</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml\n</code></pre> <p>This command creates the following resources: <ol> <li>8 CRDs:<ol> <li><code>AlertmanagerConfig</code></li> <li><code>Alertmanager</code></li> <li><code>PodMonitor</code></li> <li><code>Probe</code></li> <li><code>Prometheus</code></li> <li><code>PrometheusRule</code></li> <li><code>ServiceMonitor</code></li> <li><code>ThanosRuler</code></li> </ol> </li> <li><code>ClusterRoleBinding</code> &amp; <code>ClusterRole</code>: <code>prometheus-operator</code></li> <li><code>Deployment</code>: <code>prometheus-operator</code></li> <li><code>ServiceAccount</code>: <code>prometheus-operator</code></li> <li><code>Service</code>: <code>prometheus-operator</code></li> </ol> <li> <p>Deploy Prometheus in <code>monitoring</code> namespace.</p> <p>The resources to deploy: 1. <code>Prometheus</code> 1. rbac: <code>ClusterRole</code>, <code>ClusterRoleBinding</code>, and <code>ServiceAccount</code> 1. <code>Service</code> for Prometheus Pods. 1. <code>ServiceMonitor</code> for Prometheus itself.</p> <pre><code>kubectl apply -k .\n</code></pre> </li>"},{"location":"prometheus-operator/#2-monitor-an-application-with-servicemonitor","title":"2. Monitor an application with <code>ServiceMonitor</code>","text":"<p>Deploy example application with <code>ServiceMonitor</code>.</p> <pre><code>kubectl apply -f example-app-with-service-monitor\n</code></pre> <pre><code>kubectl port-forward -n monitoring svc/prometheus-operated 9090:9090\n</code></pre> <p>Open http://localhost:9090/targets:</p> <p></p> <p>We can see <code>serviceMonitor/default/example-app-with-service-monitor/0</code> in <code>scrape_configs</code></p> <p>ServiceMonitor</p> <pre><code>spec:\n  selector:\n    matchLabels:\n      app: example-app-with-service-monitor\n  endpoints:\n  - port: web\n</code></pre> <p>Prometheus scrape_config</p> <p>The above <code>ServiceMonitor</code> is converted into a job <code>serviceMonitor/default/example-app-with-service-monitor/0</code> in scrape_config</p> <ul> <li> <p><code>relabel_configs</code>: relabel based on the available Kubernetes metadata for endpoints</p> <pre><code>relabel_configs:\n  - source_labels: [__meta_kubernetes_service_label_app, __meta_kubernetes_service_labelpresent_app]\n    separator: ;\n    regex: (example-app-with-service-monitor);true\n    replacement: $1\n    action: keep\n  - source_labels: [__meta_kubernetes_endpoint_port_name]\n    separator: ;\n    regex: web\n    replacement: $1\n    action: keep\n...\n</code></pre> </li> <li> <p><code>kubernetes_sd_configs</code>: Kubernetes service discovery config. ServiceMonitor uses <code>endpoints</code> role.</p> <pre><code>kubernetes_sd_configs:\n- role: endpoints\n    kubeconfig_file: \"\"\n    follow_redirects: true\n    namespaces:\n    names:\n    - default\n</code></pre> </li> </ul> <p>You can check the whole config by the following command:</p> <pre><code>kubectl get secret prometheus-prometheus -n monitoring -o yaml | yq '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | yq '.scrape_configs[] | select(.job_name == \"serviceMonitor/default/example-app-with-service-monitor/0\")'\n</code></pre> scrape_configs <pre><code>job_name: serviceMonitor/default/example-app-with-service-monitor/0\nhonor_labels: false\nkubernetes_sd_configs:\n  - role: endpoints\n    namespaces:\n      names:\n        - default\nrelabel_configs:\n  - source_labels:\n      - job\n    target_label: __tmp_prometheus_job_name\n  - action: keep\n    source_labels:\n      - __meta_kubernetes_service_label_app\n      - __meta_kubernetes_service_labelpresent_app\n    regex: (example-app-with-service-monitor);true\n  - action: keep\n    source_labels:\n      - __meta_kubernetes_endpoint_port_name\n    regex: web\n  - source_labels:\n      - __meta_kubernetes_endpoint_address_target_kind\n      - __meta_kubernetes_endpoint_address_target_name\n    separator: ;\n    regex: Node;(.*)\n    replacement: ${1}\n    target_label: node\n  - source_labels:\n      - __meta_kubernetes_endpoint_address_target_kind\n      - __meta_kubernetes_endpoint_address_target_name\n    separator: ;\n    regex: Pod;(.*)\n    replacement: ${1}\n    target_label: pod\n  - source_labels:\n      - __meta_kubernetes_namespace\n    target_label: namespace\n  - source_labels:\n      - __meta_kubernetes_service_name\n    target_label: service\n  - source_labels:\n      - __meta_kubernetes_pod_name\n    target_label: pod\n  - source_labels:\n      - __meta_kubernetes_pod_container_name\n    target_label: container\n  - source_labels:\n      - __meta_kubernetes_service_name\n    target_label: job\n    replacement: ${1}\n  - target_label: endpoint\n    replacement: web\n  - source_labels:\n      - __address__\n    target_label: __tmp_hash\n    modulus: 1\n    action: hashmod\n  - source_labels:\n      - __tmp_hash\n    regex: $(SHARD)\n    action: keep\nmetric_relabel_configs: []\n</code></pre> <p></p>"},{"location":"prometheus-operator/#3-monitor-an-application-with-podmonitor","title":"3. Monitor an application with <code>PodMonitor</code>","text":"<p>Deploy example application with <code>PodMonitor</code>.</p> <pre><code>kubectl apply -f example-app-with-pod-monitor\n</code></pre> <p></p> <p>We can see <code>podMonitor/default/example-app-with-pod-monitor/0</code> in <code>scrape_configs</code></p> <ul> <li><code>relabel_configs</code>: relabel based on the available Kubernetes metadata for pod</li> <li> <p><code>kubernetes_sd_configs</code>: Kubernetes service discovery config. ServiceMonitor uses <code>endpoints</code> role.</p> <pre><code>kubernetes_sd_configs:\n- role: pod\n    kubeconfig_file: \"\"\n    follow_redirects: true\n    namespaces:\n    names:\n    - default\n</code></pre> </li> </ul> <p>You can check the whole config by the following command:</p> <pre><code>kubectl get secret prometheus-prometheus -n monitoring -o yaml | yq '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | yq '.scrape_configs[] | select(.job_name == \"podMonitor/default/example-app-with-pod-monitor/0\")'\n</code></pre> scrape_configs <pre><code>job_name: podMonitor/default/example-app-with-pod-monitor/0\nhonor_labels: false\nkubernetes_sd_configs:\n  - role: pod\n    namespaces:\n      names:\n        - default\nrelabel_configs:\n  - source_labels:\n      - job\n    target_label: __tmp_prometheus_job_name\n  - action: keep\n    source_labels:\n      - __meta_kubernetes_pod_label_app\n      - __meta_kubernetes_pod_labelpresent_app\n    regex: (example-app-with-pod-monitor);true\n  - action: keep\n    source_labels:\n      - __meta_kubernetes_pod_container_port_name\n    regex: web\n  - source_labels:\n      - __meta_kubernetes_namespace\n    target_label: namespace\n  - source_labels:\n      - __meta_kubernetes_pod_container_name\n    target_label: container\n  - source_labels:\n      - __meta_kubernetes_pod_name\n    target_label: pod\n  - target_label: job\n    replacement: default/example-app-with-pod-monitor\n  - target_label: endpoint\n    replacement: web\n  - source_labels:\n      - __address__\n    target_label: __tmp_hash\n    modulus: 1\n    action: hashmod\n  - source_labels:\n      - __tmp_hash\n    regex: $(SHARD)\n    action: keep\nmetric_relabel_configs: []\n</code></pre>"},{"location":"prometheus-operator/#4-clean-up","title":"4. Clean up","text":"<pre><code>kubectl delete -f example-app-with-pod-monitor\nkubectl delete -f example-app-with-service-monitor\nkubectl delete -k .\nkubectl delete -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml\nkubectl delete ns monitoring\n</code></pre>"},{"location":"prometheus-operator/#important-configurations","title":"Important Configurations","text":"<p>PrometheusSpec</p> <ul> <li><code>serviceMonitorNamespaceSelector</code>: Namespace's labels to match for ServiceMonitor discovery. If nil, only check own namespace. e.g. <code>serviceMonitorNamespaceSelector: {}</code> if you want to monitor all namespaces.</li> <li><code>podMonitorNamespaceSelector</code>: Namespace's labels to match for PodMonitor discovery. If nil, only check own namespace. e.g. <code>podMonitorNamespaceSelector: {}</code> if you want to monitor all namespaces.</li> </ul>"},{"location":"prometheus-operator/#operator-implementation","title":"Operator Implementation","text":"<ol> <li>How to create Prometheus server:<ul> <li><code>StatefulSet</code> for Prometheus is created in prometheus/statefulset.go with <code>prometheus</code> and <code>prometheus-config-reloader</code> containers.</li> </ul> </li> <li> <p>How to reflect <code>ServiceMonitor</code> and <code>PodMonitor</code> to scrape_config in Prometheus configuration:</p> <ol> <li>Operator.createOrUpdateConfigurationSecret calls in <code>cg.Generate</code> in operator.go <pre><code>conf, err := cg.Generate(\n    p,\n    smons,\n    pmons,\n    bmons,\n    store,\n    additionalScrapeConfigs,\n    additionalAlertRelabelConfigs,\n    additionalAlertManagerConfigs,\n    ruleConfigMapNames,\n)\n</code></pre></li> <li><code>Generate</code> calls in promcfg.go<ul> <li><code>generatePodMonitorConfig</code></li> <li><code>generateServiceMonitorConfig</code></li> <li><code>generateProbeConfig</code></li> <li><code>generateAlertmanagerConfig</code></li> </ul> </li> <li>All <code>generateXXXXConfig</code> call <code>generateK8SSDConfig</code> with different role in promcfg.go</li> <li>Generate kubernetes_sd_config: generateK8SSDConfig</li> <li><code>conf</code> is returned in <code>createOrUpdateConfigurationSecret</code> in <code>operator.go</code>.</li> <li><code>conf</code> is compressed.</li> <li>Initialize <code>Secret</code> and set the compressed configData as <code>s.Data[configFilename] = buf.Bytes()</code>.</li> <li>Create or update the <code>Secret</code>.</li> </ol> <p></p> </li> </ol>"},{"location":"prometheus-operator/#history","title":"History","text":"<ol> <li>2016-11-09 v0.0.1: <code>Prometheus</code>, <code>AlertManager</code> and <code>ServiceMonitor</code></li> <li>2017-05-09 v0.1.0: Added <code>Alertmanager</code> in PR#28</li> <li>2018-06-05 v0.20.0: Added <code>PrometheusRule</code> in PR#1333</li> <li>2019-06-20 v0.31.0: Added <code>PodMonitor</code> in PR#2566 related to Issue#38</li> <li>2020-02-10 v0.36.0: Added <code>ThanosRuler</code> in PR#2943</li> <li>2020-10-27 v0.43.0 Added <code>AlertmanagerConfig</code> in PR#3451</li> </ol>"},{"location":"prometheus-operator/#references","title":"References","text":"<ul> <li>prometheus-operator</li> <li>getting-started: The examples above are from here.</li> <li>prometheus/discovery/kubernetes: Implementation of Kubernetes Discovery in Prometheus.</li> <li>Kube-Prometheus-Stack and ArgoCD 2.5 \u2013 Server-Side Apply to the Rescue!</li> </ul>"},{"location":"secret-management/","title":"Secret Management","text":"<ol> <li>External Secrets Operator</li> <li>Secrets Store CSI Driver</li> <li>Sealed Secrets</li> <li>Vault Secrets Operator</li> <li>1Password Connect Kubernetes Operator</li> </ol>"},{"location":"strimzi/","title":"Setup","text":"<p>https://strimzi.io/quickstarts/</p>"},{"location":"strimzi/#versions","title":"Versions","text":"<p>https://strimzi.io/docs/operators/in-development/full/deploying.html#ref-kafka-versions-str</p> Kafka version InterBroker protocol version Log message format version ZooKeeper version 2.7.0 2.7 2.7 3.5.8 2.7.1 2.7 2.7 3.5.9 2.8.0 2.8 2.8 3.5.9"},{"location":"strimzi/#prepare-new-strimzi-version","title":"Prepare new Strimzi version","text":"<p>Download <code>strimzi-x.xx.x.zip</code> from https://github.com/strimzi/strimzi-kafka-operator/releases</p> <ol> <li>Put it under <code>cluster-operator/base</code> and unzip. (exclude <code>docs</code>)</li> <li>Create <code>cluster-operator/base/strimzi-x.xx.x/kustomization.yaml</code>.</li> <li>Create <code>cluster-operator/overlays/&lt;younamespace&gt;</code>.</li> <li>Add <code>rolebinding</code> + <code>clusterrolebinding</code> to overwrite <code>namespace</code>.     <pre><code>sed -i 's/namespace: .*/namespace: &lt;yournamespace&gt;/' cluster-operator/overlays/&lt;yournamespace&gt;/cluster-operator/*RoleBinding*.yaml\n</code></pre></li> </ol>"},{"location":"strimzi/#strimzi-operator","title":"Strimzi Operator","text":"<p>prepare strimzi operator</p> <ul> <li><code>namespace</code>: <code>kafka</code></li> </ul> <pre><code>kubectl apply -f namespace.yaml\nkubectl apply -k cluster-operator/overlays/kafka\n</code></pre> <pre><code>kubectl get po -n kafka\nNAME                                       READY   STATUS    RESTARTS   AGE\nstrimzi-cluster-operator-6948f4dc6-br56b   1/1     Running   0          2m51s\n</code></pre>"},{"location":"strimzi/#kafka-cluster-kafkatopic-and-kafkauser","title":"Kafka Cluster, KafkaTopic, and KafkaUser","text":"<ul> <li>cluster name: <code>my-cluster</code></li> <li><code>namespace</code>: <code>kafka</code></li> </ul> <p><pre><code>kubectl apply -k kafka-cluster\n</code></pre> 1. <code>namespace=kafka</code> 1. Check Kafka cluster</p> <pre><code>```\nNAME                                         READY   STATUS    RESTARTS   AGE\nmy-cluster-entity-operator-b74545ccb-2rww6   3/3     Running   0          74s\nmy-cluster-kafka-0                           1/1     Running   0          109s\nmy-cluster-kafka-1                           1/1     Running   0          109s\nmy-cluster-kafka-2                           1/1     Running   0          109s\nmy-cluster-zookeeper-0                       1/1     Running   0          2m13s\nstrimzi-cluster-operator-6948f4dc6-br56b     1/1     Running   0          8m45s\n```\n</code></pre> <ol> <li> <p>KafkaTopic</p> <p>Example: https://github.com/strimzi/strimzi-kafka-operator/blob/master/examples/topic/kafka-topic.yaml</p> <pre><code>kubectl get KafkaTopic -n $namespace\nNAME                                                                                               CLUSTER      PARTITIONS   REPLICATION FACTOR   READY\nconsumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a                                        my-cluster   50           1                    True\nmy-topic                                                                                           my-cluster   1            1                    True\nstrimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55                                     my-cluster   1            3                    True\nstrimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b   my-cluster   1            1                    True\n</code></pre> </li> <li> <p>KafkaUser</p> <p>Example: https://github.com/strimzi/strimzi-kafka-operator/blob/master/examples/user/kafka-user.yaml</p> <pre><code>kubectl get KafkaUser -n $namespace\nNo resources found in kafka namespace.\n</code></pre> </li> </ol>"},{"location":"strimzi/#test-with-console-producer-console-consumer","title":"Test with console-producer &amp; console-consumer","text":"<ol> <li> <p>Set <code>namespace=kafka</code></p> </li> <li> <p>producer:</p> <pre><code>kubectl -n $namespace run kafka-producer -ti --image=quay.io/strimzi/kafka:0.24.0-kafka-2.8.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic my-topic\nIf you don't see a command prompt, try pressing enter.\n&gt;test\n&gt;test2\n&gt;^Cpod \"kafka-producer\" deleted\npod kafka/kafka-producer terminated (Error)\n</code></pre> </li> <li> <p>consumer:</p> <pre><code>kubectl -n $namespace run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.24.0-kafka-2.8.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic --from-beginning\nIf you don't see a command prompt, try pressing enter.\ntest\ntest2\n^CProcessed a total of 3 messages\npod \"kafka-consumer\" deleted\npod kafka/kafka-consumer terminated (Error)\n</code></pre> </li> </ol>"},{"location":"strimzi/#kafkaconnect","title":"KafkaConnect","text":"<p>https://strimzi.io/docs/0.16.2/full.html#deploying-kafka-connect-str</p> <pre><code>  annotations:\n    strimzi.io/use-connector-resources: \"true\" # to enable connector resource\n</code></pre> <p>file source connector</p> <pre><code>overlays/kafka-strimzi-18/connect/source/connect-source.yaml\nverlays/kafka-strimzi-18/connect/source/my-source-connector.yaml\n</code></pre> <pre><code>kubectl get KafkaConnect\nNAME                   DESIRED REPLICAS\nkafka-connect-source   2\n</code></pre> <pre><code>kubectl get KafkaConnector\nNAME                  AGE\nmy-source-connector   9m2s\n</code></pre> <p>Check the message</p> <pre><code>kubectl run kafka-consumer -ti --image=strimzi/kafka:0.18.0-kafka-2.5.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic --from-beginning\n\nIf you don't see a command prompt, try pressing enter.\nOpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N\n\"\"\n\"                                 Apache License\"\n\"                           Version 2.0, January 2004\"\n\"                        http://www.apache.org/licenses/\"\n\"\"\n\"   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\"\n\"\"\n...\n</code></pre> <pre><code>kubectl run kafka-consumer -ti --image=strimzi/kafka:0.18.0-kafka-2.5.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic twitter --from-beginning\n{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"created_at\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"screen_name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"location\"},{\"type\":\"boolean\",\"optional\":false,\"field\":\"verified\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"friends_count\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"followers_count\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"statuses_count\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.User\",\"field\":\"user\"},{\"type\":\"string\",\"optional\":true,\"field\":\"text\"},{\"type\":\"string\",\"optional\":true,\"field\":\"lang\"},{\"type\":\"boolean\",\"optional\":false,\"field\":\"is_retweet\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"text\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Hashtag\"},\"optional\":true,\"field\":\"hashtags\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"display_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"expanded_url\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"type\"},{\"type\":\"string\",\"optional\":true,\"field\":\"url\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Medium\"},\"optional\":true,\"field\":\"media\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"display_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"expanded_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"url\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Url\"},\"optional\":true,\"field\":\"urls\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"screen_name\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.UserMention\"},\"optional\":true,\"field\":\"user_mentions\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Entities\",\"field\":\"entities\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Tweet\"},\"payload\":{\"id\":1290101985916592128,\"created_at\":\"2020-08-03T01:47:37.000+0000\",\"user\":{\"id\":2901232483,\"name\":\"Keryi\\uD83E\\uDD8B\",\"screen_name\":\"keryikeryi\",\"location\":\"Seattle, WA\",\"verified\":false,\"friends_count\":153,\"followers_count\":222,\"statuses_count\":13157},\"text\":\"RT @LilNasX: corona is that nigga who already graduated but won\u2019t stop coming up to the school\",\"lang\":\"en\",\"is_retweet\":true,\"entities\":{\"hashtags\":[],\"media\":[],\"urls\":[],\"user_mentions\":[{\"id\":754006735468261376,\"name\":\"nope\",\"screen_name\":\"LilNasX\"}]}}}\n^CProcessed a total of 1056 messages\n</code></pre> <ul> <li>https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/configuration_options.html</li> </ul>"},{"location":"strimzi/#enable-the-cluster-operator-to-watch-multiple-namespaces","title":"Enable the Cluster Operator to watch multiple namespaces","text":"<p>https://strimzi.io/docs/0.16.2/full.html#deploying-cluster-operator-to-watch-multiple-namespacesstr</p>"},{"location":"strimzi/#change-strimzi_namespace","title":"Change STRIMZI_NAMESPACE","text":"<p>TO update <code>STRIMZI_NAMESPACE</code>, add a patch yaml and include it in <code>kustomization.yaml</code> in <code>kafka-strimzi-18</code> (Reference: Kustomize \u3067\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306e\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u524a\u9664\u3059\u308b)</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-cluster-operator\nspec:\n  template:\n    spec:\n      serviceAccountName: strimzi-cluster-operator\n      containers:\n      - name: strimzi-cluster-operator\n        env:\n        - name: STRIMZI_NAMESPACE\n          value: kafka-strimzi-18,kafka-strimzi-18-staging\n          valueFrom: null\n</code></pre> <p>Diff</p> <pre><code>kubectl diff -k overlays/kafka-strimzi-18\n-  generation: 1\n+  generation: 2\n   labels:\n     app: strimzi\n   name: strimzi-cluster-operator\n@@ -36,10 +36,7 @@\n         - /opt/strimzi/bin/cluster_operator_run.sh\n         env:\n         - name: STRIMZI_NAMESPACE\n-          valueFrom:\n-            fieldRef:\n-              apiVersion: v1\n-              fieldPath: metadata.namespace\n+          value: kafka-strimzi-18,kafka-strimzi-18-staging\n         - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n</code></pre> <p>Apply</p> <pre><code>kubectl apply -k overlays/kafka-strimzi-18\ncustomresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkaconnectors.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkaconnects2is.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkamirrormaker2s.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkarebalances.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io unchanged\nserviceaccount/strimzi-cluster-operator unchanged\nclusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global unchanged\nclusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced unchanged\nclusterrole.rbac.authorization.k8s.io/strimzi-entity-operator unchanged\nclusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker unchanged\nclusterrole.rbac.authorization.k8s.io/strimzi-topic-operator unchanged\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation unchanged\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-topic-operator-delegation unchanged\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator unchanged\nclusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation unchanged\nclusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator unchanged\ndeployment.apps/strimzi-cluster-operator configured\nkafka.kafka.strimzi.io/my-cluster unchanged\n</code></pre>"},{"location":"strimzi/#rolebinding-clusterrolebinding","title":"RoleBinding &amp; ClusterRoleBinding","text":"<p>Copy <code>RoleBinding</code> from <code>kafka-strimzi-18</code></p> <pre><code>mkdir -p overlays/kafka-strimzi-18-staging/strimzi-0.18.0/install/cluster-operator\ncp overlays/kafka-strimzi-18/strimzi-0.18.0/install/cluster-operator/*-RoleBinding*yaml overlays/kafka-strimzi-18-staging/strimzi-0.18.0/install/cluster-operator\n</code></pre> <p>Apply</p> <pre><code>kubectl apply -k overlays/kafka-strimzi-18-staging\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-topic-operator-delegation created\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created\n</code></pre>"},{"location":"strimzi/#kafka-cluster","title":"Kafka Cluster","text":"<p>Prepare <code>my-cluster.yaml</code> and add it to <code>kustomization.yaml</code></p> <pre><code>cp overlays/kafka-strimzi-18/my-cluster.yaml overlays/kafka-strimzi-18-staging\n</code></pre> <p>Apply</p> <pre><code>kubectl apply -k overlays/kafka-strimzi-18-staging\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation unchanged\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-topic-operator-delegation unchanged\nrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator unchanged\nkafka.kafka.strimzi.io/my-cluster created\n</code></pre> <p>Check</p> <pre><code>kubectl get pod -n $namespace-staging\nNAME                                         READY   STATUS    RESTARTS   AGE\nmy-cluster-entity-operator-fd45b849f-9vk62   3/3     Running   0          59s\nmy-cluster-kafka-0                           2/2     Running   0          2m23s\nmy-cluster-zookeeper-0                       1/1     Running   0          3m19s\n</code></pre>"},{"location":"strimzi/#kafka-mirrormaker","title":"Kafka MirrorMaker","text":"<p>https://strimzi.io/docs/operators/master/using.html#assembly-deployment-configuration-kafka-mirror-maker-str</p> <p>Prerequisite:</p> <ul> <li>Run multiple clusters</li> </ul> <pre><code>kubectl apply -k strimzi/overlays/kafka-strimzi-18-staging\n</code></pre> <pre><code>kubectl get KafkaTopic -n kafka-strimzi-18-staging\nNAME                                                          PARTITIONS   REPLICATION FACTOR\nconsumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a   50           1\nheartbeats                                                    1            1\nmirrormaker2-cluster-configs                                  1            1\nmirrormaker2-cluster-offsets                                  25           1\nmirrormaker2-cluster-status                                   5            1\nmy-cluster-source.checkpoints.internal                        1            1\nmy-cluster-source.kafka-connect-sink-config                   1            1\nmy-cluster-source.kafka-connect-sink-offset                   25           1\nmy-cluster-source.kafka-connect-sink-status                   5            1\nmy-cluster-source.kafka-connect-source-config                 1            1\nmy-cluster-source.kafka-connect-source-offset                 25           1\nmy-cluster-source.kafka-connect-source-status                 5            1\nmy-cluster-source.my-topic                                    1            1\nmy-cluster-source.twitter                                     1            1\n</code></pre> <p></p> <p>check</p> <pre><code>kubectl -n kafka-strimzi-18-staging run kafka-consumer -ti --image=strimzi/kafka:0.18.0-kafka-2.5.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-cluster-source.twitter\n\nIf you don't see a command prompt, try pressing enter.\n{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"created_at\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"screen_name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"location\"},{\"type\":\"boolean\",\"optional\":false,\"field\":\"verified\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"friends_count\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"followers_count\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"statuses_count\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.User\",\"field\":\"user\"},{\"type\":\"string\",\"optional\":true,\"field\":\"text\"},{\"type\":\"string\",\"optional\":true,\"field\":\"lang\"},{\"type\":\"boolean\",\"optional\":false,\"field\":\"is_retweet\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"text\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Hashtag\"},\"optional\":true,\"field\":\"hashtags\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"display_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"expanded_url\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"type\"},{\"type\":\"string\",\"optional\":true,\"field\":\"url\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Medium\"},\"optional\":true,\"field\":\"media\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"display_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"expanded_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"url\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Url\"},\"optional\":true,\"field\":\"urls\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"screen_name\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.UserMention\"},\"optional\":true,\"field\":\"user_mentions\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Entities\",\"field\":\"entities\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Tweet\"},\"payload\":{\"id\":1293333835531329545,\"created_at\":\"2020-08-11T23:49:50.000+0000\",\"user\":{\"id\":1129412194779766784,\"name\":\"Mizibak \\uD83C\\uDDEC\\uD83C\\uDDE7\",\"screen_name\":\"mizibak\",\"location\":null,\"verified\":false,\"friends_count\":1022,\"followers_count\":675,\"statuses_count\":25644},\"text\":\"RT @NeuroNerd78: Take a strain of viruses that are abundant,Corona for example,say a new one has emerged, give it a name,apply a list of sy\u2026\",\"lang\":\"en\",\"is_retweet\":true,\"entities\":{\"hashtags\":[],\"media\":[],\"urls\":[],\"user_mentions\":[{\"id\":1182033581755031552,\"name\":\"Angie \\uD83D\\uDC1DOverlord of the Wasps\\uD83D\\uDC1D\",\"screen_name\":\"NeuroNerd78\"}]}}}\n{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"created_at\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"screen_name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"location\"},{\"type\":\"boolean\",\"optional\":false,\"field\":\"verified\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"friends_count\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"followers_count\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"statuses_count\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.User\",\"field\":\"user\"},{\"type\":\"string\",\"optional\":true,\"field\":\"text\"},{\"type\":\"string\",\"optional\":true,\"field\":\"lang\"},{\"type\":\"boolean\",\"optional\":false,\"field\":\"is_retweet\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"text\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Hashtag\"},\"optional\":true,\"field\":\"hashtags\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"display_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"expanded_url\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"type\"},{\"type\":\"string\",\"optional\":true,\"field\":\"url\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Medium\"},\"optional\":true,\"field\":\"media\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":true,\"field\":\"display_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"expanded_url\"},{\"type\":\"string\",\"optional\":true,\"field\":\"url\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Url\"},\"optional\":true,\"field\":\"urls\"},{\"type\":\"array\",\"items\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int64\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"screen_name\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.UserMention\"},\"optional\":true,\"field\":\"user_mentions\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Entities\",\"field\":\"entities\"}],\"optional\":false,\"name\":\"com.eneco.trading.kafka.connect.twitter.Tweet\"},\"payload\":{\"id\":1293333835531329545,\"created_at\":\"2020-08-11T23:49:50.000+0000\",\"user\":{\"id\":1129412194779766784,\"name\":\"Mizibak \\uD83C\\uDDEC\\uD83C\\uDDE7\",\"screen_name\":\"mizibak\",\"location\":null,\"verified\":false,\"friends_count\":1022,\"followers_count\":675,\"statuses_count\":25644},\"text\":\"RT @NeuroNerd78: Take a strain of viruses that are abundant,Corona for example,say a new one has emerged, give it a name,apply a list of sy\u2026\",\"lang\":\"en\",\"is_retweet\":true,\"entities\":{\"hashtags\":[],\"media\":[],\"urls\":[],\"user_mentions\":[{\"id\":1182033581755031552,\"name\":\"Angie \\uD83D\\uDC1DOverlord of the Wasps\\uD83D\\uDC1D\",\"screen_name\":\"NeuroNerd78\"}]}}}\n^CProcessed a total of 2 messages\npod \"kafka-consumer\" deleted\npod kafka-strimzi-18-staging/kafka-consumer terminated (Error)\n</code></pre>"},{"location":"strimzi/#monitoring","title":"Monitoring","text":"<ul> <li><code>KafkaExporter</code> in Kafka -&gt; kafka-exporter-configuration</li> <li><code>metrics</code> in <code>kafka</code> and <code>zookeeper</code> container -&gt; kafka-metrics.yaml</li> <li>Deploying the CoreOS Prometheus Operator</li> <li>Grafana Dashboards: <code>cluster-operator/base/strimzi-0.24.0/examples/metrics/grafana-dashboards</code></li> </ul>"},{"location":"strimzi/#prometheus","title":"Prometheus","text":"<ol> <li> <p>Deploy operator</p> <pre><code>git clone https://github.com/coreos/kube-prometheus &amp;&amp; cd kube-prometheus\nkubectl apply -f manifests/setup\n</code></pre> </li> <li> <p>Deploy resources</p> <pre><code>kubectl apply -f manifests\n</code></pre> <p>&lt;!-- <code>Prometheus</code> (the namespace Prometheus is going to be installed into)</p> <p><code>curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed 's/namespace: .*/namespace: monitoring/' &gt; strimzi/monitoring/prometheus.yaml</code> --&gt;</p> </li> <li> <p>Deploy <code>PodMonitor</code></p> <p>Update namespace with the one where the pods to scrape the metrics from are running</p> <pre><code>curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/strimzi-pod-monitor.yaml | sed 's/myproject/kafka-strimzi-18/' &gt; strimzi/monitoring/strimzi-pod-monitor.yaml\n</code></pre> <pre><code>kubectl apply -f strimzi/monitoring/strimzi-pod-monitor.yaml -n monitoring\n</code></pre> </li> <li> <p>Deploy <code>PrometheusRule</code> and <code>prometheus-additonal-config</code> (TBD)     <pre><code>curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml &gt; strimzi/monitoring/prometheus-rules.yaml\n</code></pre></p> <pre><code>curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-additional-properties/prometheus-additional.yaml &gt; strimzi/monitoring/prometheus-additional.yaml\nkubectl create secret generic additional-scrape-configs --from-file=strimzi/monitoring/prometheus-additional.yaml -n monitoring\n</code></pre> <pre><code># kubectl apply -f strimzi/monitoring/prometheus-rules.yaml -n monitoring\n# kubectl apply -f strimzi/monitoring/prometheus.yaml -n monitoring\n</code></pre> </li> <li> <p>Add the <code>podMonitorSelector</code> to <code>Prometheus</code> by <code>kubectl edit Prometheus -n monitoring</code></p> <pre><code>  podMonitorSelector:\n    matchLabels:\n      app: strimzi\n</code></pre> </li> <li> <p>Update service account <code>prometheus-k8s</code> to add the followings:</p> <pre><code>kubectl edit clusterrole prometheus-k8s -o yaml\n</code></pre> <pre><code>  - apiGroups: [\"\"]\n    resources:\n      - nodes\n      - nodes/proxy\n      - services\n      - endpoints\n      - pods\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups:\n      - extensions\n    resources:\n      - ingresses\n    verbs: [\"get\", \"list\", \"watch\"]\n  - nonResourceURLs: [\"/metrics\"]\n    verbs: [\"get\"]\n</code></pre> </li> </ol>"},{"location":"strimzi/#alertmanager-tbd","title":"Alertmanager (TBD)","text":"<pre><code>curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml &gt; strimzi/monitoring/alert-manager.yaml\ncurl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml &gt; strimzi/monitoring/alert-manager-config.yaml\n</code></pre> <p>Change <code>slack_api_url</code></p> <pre><code>kubectl create secret generic alertmanager-alertmanager --from-file=alertmanager.yaml=strimzi/monitoring/alert-manager-config.yaml -n monitoring\n</code></pre> <pre><code>kubectl apply -f strimzi/monitoring/alert-manager.yaml -n monitoring\n</code></pre>"},{"location":"strimzi/#grafana","title":"Grafana","text":"<p>Import dashoboard</p> <p></p>"},{"location":"strimzi/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<p>Listerner authentication</p> <ul> <li>Mutual TLS authentication<ul> <li>The client supports authenticaton using mutual TLS authentication</li> <li>It is necessary to ue the TLS certificates rather than passwords</li> <li>You can reconfigure and restart client applications periodically so that they do not use expired certificates</li> </ul> </li> <li>SCRAM-SHA(Salted Challenge Response Authenticatoin Mechanism) authentication<ul> <li>Support <code>SCRAM-SHA-512</code> only.</li> <li>The client supports authentication using SCRAM-SHA-512</li> <li>It is necessary to use passwords rather than the TLS certificates</li> <li>Authentication for unencrypted communication is required</li> </ul> </li> <li>no <code>authentication</code> property -&gt; not authenticate</li> </ul>"},{"location":"strimzi/#debug","title":"debug","text":""},{"location":"strimzi/#kafkaconnector-fails-to-join-group","title":"KafkaConnector fails to join group","text":"<pre><code>2020-08-02 02:54:06,347 INFO [Worker clientId=connect-1, groupId=kafka-connect-source] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]\n</code></pre> <p>Connector was old.</p>"},{"location":"strimzi/#kafkaconnector-elasticsearch-fails","title":"KafkaConnector Elasticsearch fails","text":"<pre><code> \"java.lang.NoClassDefFoundError: com/google/common/collect/ImmutableSet\\n\\tat\n        io.searchbox.client.AbstractJestClient.&lt;init&gt;(AbstractJestClient.java:38)\\n\\tat\n        io.searchbox.client.http.JestHttpClient.&lt;init&gt;(JestHttpClient.java:43)\\n\\tat\n        io.searchbox.client.JestClientFactory.getObject(JestClientFactory.java:51)\\n\\tat\n        io.confluent.connect.elasticsearch.jest.JestElasticsearchClient.&lt;init&gt;(JestElasticsearchClient.java:150)\\n\\tat\n        io.confluent.connect.elasticsearch.jest.JestElasticsearchClient.&lt;init&gt;(JestElasticsearchClient.java:142)\\n\\tat\n        io.confluent.connect.elasticsearch.ElasticsearchSinkTask.start(ElasticsearchSinkTask.java:122)\\n\\tat\n        io.confluent.connect.elasticsearch.ElasticsearchSinkTask.start(ElasticsearchSinkTask.java:51)\\n\\tat\n        org.apache.kafka.connect.runtime.WorkerSinkTask.initializeAndStart(WorkerSinkTask.java:305)\\n\\tat\n        org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:193)\\n\\tat\n        org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)\\n\\tat\n        org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)\\n\\tat\n        java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\n\\tat\n        java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat\n        java.lang.Thread.run(Thread.java:748)\\nCaused by: java.lang.ClassNotFoundException:\n        com.google.common.collect.ImmutableSet\\n\\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\\n\\tat\n        java.lang.ClassLoader.loadClass(ClassLoader.java:418)\\n\\tat org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:104)\\n\\tat\n        java.lang.ClassLoader.loadClass(ClassLoader.java:351)\\n\\t... 16 more\\n\"\n</code></pre> <p>Need to add gua</p>"},{"location":"strimzi/#kafka-connect-elasticsearch-fails","title":"Kafka Connect Elasticsearch fails","text":"<pre><code>org.apache.kafka.connect.errors.ConnectException: Couldn't start ElasticsearchSinkTask due to connection error:\n        at io.confluent.connect.elasticsearch.jest.JestElasticsearchClient.&lt;init&gt;(JestElasticsearchClient.java:159)\n        at io.confluent.connect.elasticsearch.jest.JestElasticsearchClient.&lt;init&gt;(JestElasticsearchClient.java:142)\n        at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.start(ElasticsearchSinkTask.java:122)\n        at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.start(ElasticsearchSinkTask.java:51)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.initializeAndStart(WorkerSinkTask.java:305)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:193)\n        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)\n        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)\n        at java.util.concurrent.Executors.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\nCaused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n        at sun.security.ssl.Alerts.getSSLException(Alerts.java:198)\n        at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1967)\n        at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:331)\n        at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:325)\n        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1688)\n        at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:226)\n        at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1082)\n        at sun.security.ssl.Handshaker.process_record(Handshaker.java:1010)\n        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1079)\n        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1388)\n        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1416)\n        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1400)\n        at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:396)\n        at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:355)\n        at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)\n        at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:359)\n        at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:381)\n        at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237)\n        at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)\n        at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)\n        at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111)\n        at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n        at io.searchbox.client.http.JestHttpClient.executeRequest(JestHttpClient.java:133)\n        at io.searchbox.client.http.JestHttpClient.execute(JestHttpClient.java:70)\n        at io.searchbox.client.http.JestHttpClient.execute(JestHttpClient.java:63)\n        at io.confluent.connect.elasticsearch.jest.JestElasticsearchClient.getServerVersion(JestElasticsearchClient.java:247)\n        at io.confluent.connect.elasticsearch.jest.JestElasticsearchClient.&lt;init&gt;(JestElasticsearchClient.java:151)\n        ... 12 more\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n        at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:450)\n        at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:317)\n        at sun.security.validator.Validator.validate(Validator.java:262)\n        at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:330)\n        at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:237)\n        at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:132)\n        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1670)\n        ... 35 more\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n        at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n        at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n        at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)\n        at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:445)\n        ... 41 more\n</code></pre>"},{"location":"strimzi/#reference","title":"reference","text":"<ul> <li>Custom image for KafkaConnect: https://strimzi.io/docs/operators/0.18.0/using.html#creating-new-image-from-base-str</li> <li>https://github.com/nakamasato/kafka-connect</li> </ul>"},{"location":"strimzi/cluster-operator/base/strimzi-0.18.0/examples/","title":"Examples","text":"<p>This folder contains different examples of Strimzi custom resources and demonstrates how they can be used.</p> <ul> <li>Kafka cluster<ul> <li>Kafka deployments with different types of storage</li> </ul> </li> <li>Kafka Connect</li> <li>Kafka Connect Connector</li> <li>Kafka Mirror Maker</li> <li>Kafka Mirror Maker 2</li> <li>Kafka Bridge</li> <li>Cruise Control and Kafka Rebalance</li> <li>Kafka users</li> <li>Kafka topics</li> <li>Prometheus metric<ul> <li>Examples of Kafka components with enabled Prometheus metrics</li> <li>Grafana dashboards</li> <li>Prometheus Alert</li> <li>Sample Prometheus installation files</li> <li>Sample Grafana installation files</li> <li>JMX Trans deployment</li> </ul> </li> <li>Security<ul> <li>Deployments of Kafka, Kafka Connect and HTTP Bridge using TLS encryption, authentication and authorization</li> </ul> </li> </ul>"},{"location":"strimzi/cluster-operator/base/strimzi-0.24.0/examples/","title":"Examples","text":"<p>This folder contains different examples of Strimzi custom resources and demonstrates how they can be used.</p> <ul> <li>Kafka cluster<ul> <li>Kafka deployments with different types of storage</li> </ul> </li> <li>Kafka Connect and Kafka Connect Connector</li> <li>Kafka Mirror Maker 1 and 2</li> <li>Kafka Bridge</li> <li>Cruise Control and Kafka Rebalance</li> <li>Kafka users</li> <li>Kafka topics</li> <li>Prometheus metric<ul> <li>Examples of Kafka components with enabled Prometheus metrics</li> <li>Grafana dashboards</li> <li>Prometheus Alert</li> <li>Sample Prometheus installation files</li> <li>Sample Grafana installation files</li> <li>JMX Trans deployment</li> </ul> </li> <li>Security<ul> <li>Deployments of Kafka, Kafka Connect and HTTP Bridge using TLS encryption, authentication and authorization</li> </ul> </li> </ul>"},{"location":"strimzi/cluster-operator/base/strimzi-0.24.0/examples/security/keycloak-authorization/","title":"Keycloak authorization example","text":"<p>This folder contains an example <code>Kafka</code> custom resource configured for OAuth 2.0 token-based authorization using Keycloak. The example resource is configured with: - <code>keycloak</code> authorization - the corresponding <code>oauth</code> authentication The folder also contains a Keycloak realm export to import into your Keycloak instance to support the example.</p> <p>Full instructions for the example are available in the Strimzi Documentation.</p> <ul> <li>kafka-authz-realm.json<ul> <li>The Keycloak realm export file</li> </ul> </li> <li>kafka-ephemeral-oauth-single-keycloak-authz.yaml<ul> <li>The Kafka CR that defines a single-node Kafka cluster with <code>oauth</code> authentication and <code>keycloak</code> authorization, using the <code>kafka-authz</code> realm. See full example instructions for proper preparation and deployment.</li> </ul> </li> </ul>"},{"location":"tempo/","title":"Tempo","text":""},{"location":"tempo/#getting-started","title":"Getting Started","text":"<ol> <li>Clone     <pre><code>git clone https://github.com/grafana/tempo.git &amp;&amp; cd tempo/example/docker-compose/local\n</code></pre></li> <li> <p>Run</p> <pre><code>docker compose up\n</code></pre> <p>Inside this compose, https://github.com/grafana/xk6-client-tracing/ is an app to generate traces</p> </li> <li> <p>Open grafana http://localhost:3000</p> <p></p> </li> </ol>"},{"location":"terraform-k8s/","title":"terraform-k8s","text":"<p>Kubernetes operator to configure Terraform Cloud with CRD.</p> <p></p> <p>Lifecycle of a Terraform Workspace:</p> <ol> <li>A workspaces is created with CR <code>Workspace</code> with <code>ConfigMap</code>.</li> <li>The operator detect the creation and start reconciliation loop.</li> <li>If the configuration is valid, the first run will be executed.</li> <li>After the run, the reconciliation loop is kept being called with an interval.</li> <li>When any change or diff is detected, the operator will make the actual status and desired status same.</li> </ol>"},{"location":"traefik/quick-start/","title":"Traefik","text":"<p>https://traefik.io/</p>"},{"location":"traefik/quick-start/#traefik-proxy","title":"Traefik Proxy","text":"<p>https://doc.traefik.io/traefik/</p> <p>Example with docker-compose:</p> <pre><code>docker compose up\n</code></pre> <pre><code>url -H Host:whoami.docker.localhost http://127.0.0.1\n</code></pre> <pre><code>Hostname: 6d0f82923467\nIP: 127.0.0.1\nIP: 172.19.0.2\nRemoteAddr: 172.19.0.3:48168\nGET / HTTP/1.1\nHost: whoami.docker.localhost\nUser-Agent: curl/7.79.1\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 172.19.0.1\nX-Forwarded-Host: whoami.docker.localhost\nX-Forwarded-Port: 80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 96aff624c7ed\nX-Real-Ip: 172.19.0.1\n</code></pre> <p>Example with Kubernetes</p>"},{"location":"traefik/quick-start/#traefik-mesh","title":"Traefik Mesh","text":"<p>https://doc.traefik.io/traefik-mesh/</p>"},{"location":"traefik/quick-start/#traefik-pilot","title":"Traefik Pilot","text":"<p>https://doc.traefik.io/traefik-pilot/</p>"}]}